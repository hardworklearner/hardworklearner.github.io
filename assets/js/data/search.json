[ { "title": "Laravel React AdminLTE Dashboard Project and todo example", "url": "/posts/laravel-react-adminlte-dashboard-project-and-todo-example/", "categories": "", "tags": "", "date": "2024-10-07 00:00:00 +0700", "snippet": "Laravel React AdminLTE Project management example codeAdmin Dashboard using Laravel, AdminLTE and ReactJs to coded a website with purpose using in Project management, due to example code for tutori...", "content": "Laravel React AdminLTE Project management example codeAdmin Dashboard using Laravel, AdminLTE and ReactJs to coded a website with purpose using in Project management, due to example code for tutorial, this sample code cover case during code daily of a full stack developer. In this project, this was created from a long time project, so I using Laravel 5.x and update to React@lastest. I will update from Laravel 5.x to Laravel lastest in near future time and update in readme if possible.Prepare Working environment: Linux, MacOS, microsoft Windows Laravel 5.x ReactJs lastest PHP 7.4 Node 14 Install Install LaravelInstall Laravel via composer is more convienence than use laravel install due to maintain tool installer, certainly!, Composer is well-known package manager composer create-project --prefer-dist laravel/laravel laravel_react_adminlte_project_todo_management \"5.5.*\"cd laravel_react_adminlte_project_todo_managementcomposer install Install AdminLTEAdminLTE used in my repository is 2.x, which is stable for Laravel 5.x in 2018. For lastest support and maintain please use lastest version of adminLTE is 3.x in 2024. composer require jeroennoten/laravel-adminltephp artisan adminlte:install # if adminlte of lower version will not run this command `adminlte` not found Register AdminLTE in service provider to booter with laravel JeroenNoten\\LaravelAdminLte\\ServiceProvider::class, After install AdminLTE, publish vendor assests file to make css, and js file working. php artisan vendor:publish --provider=\"JeroenNoten\\LaravelAdminLte\\ServiceProvider\" --tag=assetphp artisan config:cachephp artisan view:clear Install Node packageThe version of Node is 14 and React is latest, and ReactDom is also lastest to avoid miss coparatitive with version and functions. nvm install 14nvm use 14npm install To build css and js file npm run dev To watch and synchonized change code automatically npm run watch Run conmand to generate keyLaravel is rich features, mean every code and feature can also have own config but before run a laravel app, laravel will look into the configure file and find out key generated which used for hash, ssl and algorithm functions. Laravel will not start without key generated. Make a .env file from .env.local php artisan key:generate Run command to migrateBefore run migrate, make sure to config database parameters in .env.There are multiplee databases support but in this repository, I use MYSQL. DB_CONNECTION=mysqlDB_HOST=127.0.0.1DB_PORT=3306DB_DATABASE=laravel_react_sample_codeDB_USERNAME=db_usernamDB_PASSWORD=db_pass Run migrate php artisan migrate Run command to make data for testMade some data for tests and available seed by commandphp artisan db:seedThe content is following to Seed are: ProjectTableSeeder.php and MemberTableSeeder.php&lt;?phpuse Illuminate\\Database\\Seeder;use App\\Project;class ProjectTableSeeder extends Seeder{ public function run() { DB::statement('SET FOREIGN_KEY_CHECKS=0;'); App\\Project::truncate(); DB::statement('SET FOREIGN_KEY_CHECKS=1;'); DB::table('project')-&gt;delete(); $name = 'Project A'; Project::create(array( 'name' =&gt; $name, 'information' =&gt; 'test1', 'deadline' =&gt; '2018-09-27', 'type' =&gt; 'lab', 'status' =&gt; 1, 'created_at' =&gt; '1990-09-27', 'updated_at' =&gt; '1990-09-27' )); $name = 'Project B'; Project::create(array( 'name' =&gt; $name, 'information' =&gt; 'test2', 'deadline' =&gt; '2018-09-27', 'type' =&gt; 'lab', 'status' =&gt; 1, 'created_at' =&gt; '1990-09-27', 'updated_at' =&gt; '1990-09-27' )); }}and MemberTableSeeder.php&lt;?phpuse Illuminate\\Database\\Seeder;use App\\Member;class MemberTableSeeder extends Seeder{ public function run() { DB::statement('SET FOREIGN_KEY_CHECKS=0;'); App\\Member::truncate(); DB::statement('SET FOREIGN_KEY_CHECKS=1;'); DB::table('member')-&gt;delete(); $name = 'Nguyen Kim'; Member::create(array( 'name' =&gt; $name, 'information' =&gt; 'test', 'date_of_birth' =&gt; '1992-09-25', 'position' =&gt; 'intern', 'phone' =&gt; '0965841492', 'gender' =&gt; 1, 'avatar' =&gt; 'storage/images/default.png' )); $name = 'Nguyen Phuc'; Member::create(array( 'name' =&gt; $name, 'information' =&gt; 'test', 'date_of_birth' =&gt; '1992-09-25', 'position' =&gt; 'intern', 'phone' =&gt; '0965841492', 'gender' =&gt; 1, 'avatar' =&gt; 'storage/images/default.png' )); }}File DatabaseSeeder.php will place code to call seeders&lt;?phpuse Illuminate\\Database\\Seeder;class DatabaseSeeder extends Seeder{ /** * Run the database seeds. * * @return void */ public function run() { $this-&gt;call(ProjectTableSeeder::class); $this-&gt;call(MemberTableSeeder::class); }} Check variables configureCheck on folder config and locate files: adminlte.php, MyGlobeSetting.php and app.php to checl whether conditions and variables are setted. These variables have own defaults.Please check config to add: App Name Database connection AdminLTE params: menu, plugins Fix any bugs appear and check bug math.divAfter install and run npm run dev, command is work properly but errors may be appear. Please aware to fix bugs appeard duting run at local site.The bug come from sass compiler is math.div not found so please go in detail error notice and make change from math.div to function symbols / Websites pictures Website pictures: Home page Project Page Create Project Member Update information Other Data is sample for test and demo Coding TutorialAdmin By LaravelCreate Admin Dashboard using AdminLTEAdmin Dashboard using ReactJSReference" }, { "title": "Virtual Private Cloud (VPC) part 2", "url": "/posts/virtual-private-cloud-vpc-part-2/", "categories": "Fullstack, Architect, AWS", "tags": "aws, vpc, subnet, DataSync, Disaster Recovery Strategies, PilotLight, Database Migration Service, Disaster Recovery, DMS", "date": "2024-02-08 00:00:00 +0700", "snippet": "Virtual Private Cloud (VPC)VPC Components DiagramUnderstanding CIDR - IPv4 Classless Inter-Domain Routing - a method for allocating IP addresses Used in Security Groups rules and AWS networking i...", "content": "Virtual Private Cloud (VPC)VPC Components DiagramUnderstanding CIDR - IPv4 Classless Inter-Domain Routing - a method for allocating IP addresses Used in Security Groups rules and AWS networking in general They help to define an IP address range: We’ve seen WW.XX.YY.ZZ/32 =&gt; one IP We’ve seen 0.0.0.0/0 =&gt; all IPs But we can define 192.168.0.0/26 =&gt; 192.168.0.0 - 192.168.0.63 (64 IP addresses) A CIDR consists of two components Base IP Represents an IP contained in the range (XX.XX.XX.XX) Example: 1.0.0.0 192.168.0.0 Subnet Mask Defines how many bits can change in the IP Example: /0, /24, /32 Can take two forms: /8 =&gt; 255.0.0.0 /16 =&gt; 255.255.0.0 /24 =&gt; 255.255.255.0 /32 =&gt; 255.255.255.255 Understanding CIDR - Subnet Mask The subnet mask basically allows part of the underlying IP to get additional next values from the base IP 192.168.0.0 / 32 allows for 1 IP -&gt; 192.168.0.0 192.168.0.0 / 31 allows for 2 IP -&gt; 192.168.0.1 192.168.0.0 / 30 allows for 4 IP -&gt; 192.168.0.0 -&gt; 192.168.0.3 192.168.0.0 / 29 allows for 8 IP -&gt; 192.168.0.7 192.168.0.0 / 28 allows for 16 IP -&gt; 192.168.0.15 … 192.168.0.0 / 16 allows for 65.536 IP -&gt; 192.168.255.255 192.168.0.0 / 0 allows for all IPs Understanding CIDR – Little Exercise 192.168.0.0 / 24 = … ? 192.168.0.0 – 192.168.0.255 (256 IPs) 192.168.0.0/16 = … ? 192.168.0.0 – 192.168.255.255 (65,536 IPs) 134.56.78.123/32 = … ? Just 134.56.78.123 0.0.0.0/0 all ips When in doubt, use this website: https://www.ipaddressguide.com/cidr Public vs Private IP (IPv4) The internet assigned number authority (IANA) established certain blocks of IPv4 addresses for the use of private (LAN) and public (internet) addresses Private IP can only allow certain values 10.0.0.0 - 10.255.255.255 (10.0.0.0/8) &lt;- in big networks 172.16.0.0 - 172.31.255.255 (172.16.0.0/12) &lt;- AWS default VPC in that range 192.168.0.0 - 192.168.255.255 (192.168.0.0/16) &lt;- e.g, home networks All the rest of the IP address on the Internet are Public VPC in AWS – IPv4 VPC = Virtual Private Cloud You can have multiple VPCs inn an Ắ region (mã 5 pẻ region - soft limit) Max. CIDR per VPC is 5, for each CIDR: Min. size is /28 (16 ip) Max size is /16 (65536 ip) Because VP is private, only the Private IPv4 ranges are allowed: 10.0.0.0 - 10.255.255.255 (10.0.0.0/8) 172.16.0.0 - 172.31.255.255 (172.16.0.0/12) 192.168.0.0 - 192.168.255.255 (192.168.0.0/16) Your VPC CIDR should NOT overlap with your other network VPC – Subnet (IPv4) AWS reserves 5 IP addresses (first 4 &amp; last) inn each subnet These 5 IP addresses are not available for use and can’t be assigned to an EC2 instance Example: If IDR block 10.0.0.0/24, then reserved IP addresses are: 10.0.0.0 Network Address 10.0.0.1 reserved by AWS for the VPC router 10.0.0.2 reserved by AWS for mapping to Amazon-provided DNS 10.0.0.3 reserved by AWS for future use 10.0.0.255 Network Broadcast address. AWS doesn’t support broadcast in a VPC, therefore the address is reserved Example Tip: If you need 29 IP addresses for EC2 instances: You can’t choose a subnet of size /27 (32 IP addresses, 32 - 5 = 27 &lt; 29) YOu need to choose a subnet of size /26 (64 IP address, 64 - 5 = 59 &gt; 29) Internet Gateway (IGW) Allows resources (e.g EC2 instances) in a VP connect to the Internet It scales horizontally and is highly available and redundant Must be created separately from a VPC One VPC can only be attached to one IGW adn vice versa Internet Gateway in their own do not allow internet access… Route tables must also be edited! VPC - Subnet (IPv4) AWS reserves 5 IP addresses (first 4 &amp; last) in each subnet These 5 IP addresses are not available for use adn can’t be assigned to an EC2 instance Example if CIDR block 10.0.0.0/24, then reserved IP addresses are: 10.0.0.0 Network Address 10.0.0.1 reserved by AWS for VPC router 10.0.0.2 reserved by AWS mapping Amazon-provided DNS 10.0.0.3 reserved by AWS for future use 10.0.0,255 Network Broadcast Address AWS does not support broadcast inn a VPC therefore the address is reserved Example Tip: if you need 29 IP address for EC2 instances: You can’t choose a subnet of size /27 (32 IP address, 32-5 = 27 &lt; 29) You need to choose a subnet of size /26 (64 - 5 = 59 &gt; 29) Bastion Hosts We can use a Bastion Host to SSH into our private EC2 instances The bastion is in the public subnet which is then connected to all other private subnet Bastion Host security group must be tightened Example Tip: Make sure the bastion host oly has port 22 traffic from the IP address you need, not from the security groups of your other EC2 instances NAT Instance(outdated but still at the exam) NAT = Network Address Transaction Allows EC2 instances in private subnet to connect to the internet Must be launched in a public subnet Must disable EC2 setting: Source / destination check Must have Elastic IP attached to it Route Tables must be configured to route traffic from private subnet to the NAT instance NAT Instance - Comments Pre-configured Amazon Linux AMI is available Reached the end of standard support on December 31, 2020 Not highly available / resilient setup [ out of the box You need to create ann ASG inn multi-AZ + resilient user-data script Internet traffic bandwidth depends on EC2 instance type YOu must manage Security Groups &amp; rules Inbound : Allow HTTP/HTTPS coming from Private Subnet Allow SSH from your home network (access is provided through Internet Gateway) Outbound: Allow HTTP/HTTPS traffic to the internet NAT Gateway AWS-managed NAT, higher bandwidth, high availability, no administration Pay per hour for usage and bandwidth NATGW is created in a specific Availability Zone, uses an Elastic IP Can’t be used by EC2 instance in the same subnet (only from other subnets) Requires an IGW (Private Subnet =&gt; NATGW =&gt; IGW) 5Gbs of bandwidth with automatic scaling up to 45Gbps No Security Groups to manage / required NAT Gateway with high availability NAT Gateway is resilient within a single Availability Zone Must create multiple NAT Gateways inn multiple AZs for fault tolerance There is no cross-AZ failover needed because if an AZ goes down it doesn’t need NAT NAT Gateway vs. NAT Instance DNS Resolution in VPC DNS Resolution in VPC(enableDnsSupport) Decides if DNS resolution from Route53 resolver server is supported for the VPC True (default): it queries the amazon provider DNS Server att 169.254.169.253 or the reserved IP address at the base of the VPC IPv4 network range plus two DNS Hostname (enableDnsHostnames) By default True =&gt; default VPC False =&gt; newly created VPCs Won’t do anything unless enableDnsSupport=true If True, assigns public hostname to EC2 instances if it has a public IPv4 If you use custom DNS domain names in a Private Hosted Zone in Route 53, you must set both these attributes (enableDnsSupport &amp; enableDnsHostname) to true Security Group vs NACLs Network Access control list (NACL) NACL are like a firewall which control traffic from and tto subnet One NACL per subnet, new subnets are assigned the Default NACL You define NACL rules: Rules have a number (1-32766), higher precedence with a lower number First rule match will drive the decision Example: If you define #100 Allow 10.0.0.10/32 and #200 DENY 10.0.0.10/32, the IP address will be allowed because 100 has a higher precedence over 200 The last rule is an asterisk(*) and denies a request in case of no rule match AWS recommends adding rules by increment of 100 Newly created NACLs will deny everything NACL are great way of blocking a specific IP address at the subnet level Default NACL Accepts everything inbound/outbound with the subnets it’s associated with Do NOT modify the default NACL, instead create custom NACLs Ephemeral Ports For any two endpoints to establish a connection, they must use ports Clients connect to a defined port and expect a response on an ephemeral port Different Operation Systems use different port ranges, examples: IANA &amp; MS Windows 10: 49152-65535 Many Linux kernels: 327668-60999 NACL with Ephemeral Ports Create NACL rules for each target subnets CIDR Security Group vs NACLs Security group Operates at the instance level Supports allow rules only Stateful: return traffic is automatically allowed, regardless of any rules All rules are evaluated before deciding whether to allow traffic Applies to an EC2 instance when specified by someone NACL Operate at the subnet level Support allow rules and deny rules Stateless: return traffic must be explicitly allowed by rules(thick of ephemeral ports) Rules are evaluated in order (lower to higher) when deciding whether to allow traffic, first match wins Automatically applies to all EC2 instances in the subnet that it’s associated with VPC - Reachability Analyzer A network diagnotics tool that troubleshoots network connectivity between two endpoints in yourVPC(s) It builds a model of the network configuration, then checks the reachability based on these configuration (it doesn’t send packets) When the destination is: Reachable: It produces hop by hop details of the virtual network path Not reachable: It identifies the blocking component(s) (e.g configuration issues in SG, NACLs, Route Tables,…) Use cases: troubleshoot connectivity issues, ensure network configuration is intended,… VPC Peering Privately connect two VPCs using AWS’s network Make them behave as if they were in the same network Must not have overlapping CIDRs VPC Peering connection is NOT transitive(Must be established for each VPC that need to communicate with one another) You must update route tables in each VPC’s subnet to ensure EC2 instances can communicate with each other VPC Peering – Good to know You can create VPC connection between VPCs in different AWS accounts/regions You can reference a security group inn a peered VPC (works cross accounts - same region) VPC Peering - VPC End Point VPC Endpoints (AWS PrivateLink) Every AWS service is publicly exposed (public URL) VPC Endpoint (powered by AWS PrivateLink) allows you to connect to AWS services using a private network instead of using the public internet They’re redundant and scale horizontally They remove the need of IGW, NATGW,… to access AWS Services In case of issues: Check DNS Setting Resolution in your VPC Check Route Tables Type of Endpoints Interface endpoints Provisions an ENI (private IP address) as an entry point (must attach a Security Group) Supports most AWS services Gateway Endpoints Provisions a gateway and must be used as a target in a route table Supports both S3 and DynamoDB VPC Flow Logs Capture information about IP traffic going into your interfaces: VPC Flow Logs Subnet Flow Logs Elastic network Interface (ENI) Flow Logs Helps to monitor &amp; troubleshoot connectivity issues Captures network information from AWS managed interface too: ELB, RDS, ElasticCache, Redshift, WorkSpaces,NATGW, Transit Gateway… VPC Flow Logs Syntax srcaddr &amp; dstaddr help identify problematic IP srcport &amp; dsttportt help identity problematic ports Action: Success or failures of request due to Security Group / NACL Can be used for analytics on usage patterns, or malicious behavior Query VPC flow logs using Athena on S3 or CloudWatch Logs Insights Flow Log examples: VPC Flow Logs – Troubleshoot SG &amp; NACL issues Look at the “ACTION” field Incoming Requests Inbound REJECT =&gt; NACL or SG Inbound ACCEPT, Outbound REJECT =&gt; NACL Outgoing request Outbound REJECT =&gt; NACL or SG Outbound ACCEPT, Inbound REJECT =&gt; NACL AWS Site-to-Site VPN Virtual Private Gateway (VGW) VPN Concentrator on the AWS side of the VPN connection VGW is created and attached to the VPC from which you want to create the site to site VP connection Possibility to customize the ASN (Autonomous System Number) Customer Gateway Software application or physical device onn customer side of the VPN connection https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html Site to Site VPN Connections Customer Gateway Device (On-premises) What IP address to use? Public Internet-routable IP address for your Customer Gateway device If it’s behind a NAT device that’s enabled for NAT traversal (NAT-T), use the public IP address of the NAT device Important step: enable Route Propagation for the Virtual Private Gateway inn route table that is associated with your subnets If you need yo ping your Ec2 instances from on-premises, make sure you add the ICMP protocol on the inbound of your security groups AWS VPN CloudHub Provide secure communication between multiple sites, if you have multiple VPN connections Low-cost hub and spoke model for primary or secondary network connectivity between different locations(VPN only) It’s a VPN connection so it goes over the public internet To set it up, connect multiple VPN connection on the same VGW, setup dynamic routing and configure route tables Direct Connect(DX) Provides a dedicated private connection from a remote network to your VPC Dedicated connection must be setup between your DC and AWS Direct Connect locations You need to setup a Virtual Private Gateway on Your VPC Access public resources (S3) and private(EC2) on same connection Use cases: Increase bandwidth throughput - working with large sets - lower cost More consistent network experience - application using real-time data feeds Hybrid Environments (on prem + cloud) Support both IPv4 ad Ipv6 Direct Connect Gateway If you want to setup a Direct Connect to one or more VPC in many different regions (same account), you must use a Direct Connect Gateway Direct Connect – Connection Types Dedicated Connections: 1Gbps to 10 Gbps capacity Physical ether port dedicated to a customer Request made to AWS first, then completed by AWS Direct Connect Partners Hosted Connections: 50Mbps, 500Mbps, to 10 Gbps Connection request are made via AWS Direct Connect Partners Capacity can be added or removed on demand 1,2,5,10, Gbps available at select AWS Direct Connect Partners Lead times are often longer than 1 month tto establish a new connection Direct Connect – Encryption Data inn transit is not encrypted but is private AWS Direct Connect + VPN provides an IPsec-encrypted private connection Good for a extra level of security, but slightly more complex to put in place Direct Connect - Resiliency High Resiliency Critical workloads (One connection at multiple locations) Maximum Resiliency for Critical Workloads(Maximum resilience is achieved by separate connectionsterminating on separate devices in more than one location.) Exposing services in yourVPC to otherVPC Option1: make it public Goes through the public www Tough to manage access Option2: VPC peering Must create many peering relations Open the whole AWS PrivateLink (VPC Endpoint Services) Most secure &amp; scalable way to expose a service to 1000s of VPC (ow or other accounts) Does not require VPC peering, internet gateway, NAT, route tables… Requires a network load balancer (service VPC) and ENI (CCustomer VPC) or GWLB If the NLB is inn multiple AZ, adn the ENIs inn multiple AZ, the solution is fault tolerant! AWS Private Link &amp; ECS EC2-Classic &amp; ClassicLink EC2-classic: instances run in a single network shared with other customers Amazon VPC: your instances run logically isolated to your AWS account Must associate a security group Enables communication using private IPv4 addresses Removes the need to make use of public IPv4 addresses or elastic IP address Likely to be distract att the exam EC2 Classic &amp; AWS ClassicLik Ec2-Classic: instance run in a single network shared with other customers Amazon VPC: your instances run logically isolated to your AWS account ClassicLink allows you to link EC2-Classic instances to a VPC i your account Must associate a security group Enables communication using private Ipv4 addresses Remove the need to make use of public Ipv4 addresses or Elastic IP addresses Likely to be distract at the exam Network topologies can become complicated Transit Gateway For having transitive peering between thousands of VPC and on-premises, hub and spoke (star) connection Regional resource, can work cross-region Share cross account using Resource Access Manager (RAM) YOu can peer Transit Gateway across regions Route Tables limit which VPC can talk with other VPC Works with Direct Connect Gateway, VPN Connections Supports IP Multicast (not supported any other AWS service) Transit Gateway: Site to Site VPN ECMP ECMP = Equal-cost multi-path routing Routing strategy to allow to forward a packet over multiple best path Use case: create multiple Site-to-Site VP connection to increase the bandwidth of your connection to AWS Transit Gateway: throughput with ECMPTransit Gateway: throughput with ECMP VPN to virtual private gateway VPN to transit gateway Transit Gateway: Share Direct Connect between multiple accounts VPC - Traffic Mirroring Allows you to capture and inspect network traffic in your VPC Route the traffic to security appliances that you manage Capture the traffic From (Source) - ENIs To (Targets) - an ENI or a Network Load Balancer Capture all packets of your interest (optionally, truncate packets) Source and Target can be in the same VPC or different VPCs (VPC Peering) Use cases: content inspection, threat monitoring, troubleshooting… What is IPvv6 Ipv4 design for 4.3 billion addresses (they’ll be exhausted soon) IPv6 is successor of IPv4 IPv6 is designed to provide 3.4^10 mu 38 unique IP addresses Every IPv6 address is public and internet-routable (no private range) Format è x.x.x.x.x.x.x.x (x is hexadecimal, range can be from 0000 to ffff) Examples: 2001:db8:3333:4444:5555:6666:7777:8888 2001:db8:3333:4444:cccc:dddd:eeee:ffff ::eall 8 segments are zero 2001:db8::ethe last 6 segments are zero ::1234:5678èthe first 6 segments are zero 2001:db8::1234:5678èthe middle 4 segments are zero IPv6 in VPC Ipv4 cannot be disabled for your VPC and subnets You can enable IPv6 (they’re public IP addresses) to operate in dual-stack mode Your EC2 instances will get at least a private internal IPv4 ad a public IPv6 They can communicate using either IPv4 or IPv66 to the internet through an Internet gateway IPv6 Troubleshooting IPv4 cannot be disabled for your VPC and subnets So, if you cannot launch an EC2 instance i your subnet it’s not because it cannot acquire ann IPv6 It’s because there are no available IPv4 in your subnet Solution: create a new IPv4 CIDR in your subnet Egress-only Internet Gateway Used for IPv66 only (similar to a NAT Gateway but for IPv6) Allows instances in your VPC outbound connections over IPv6 while preventing the internet to initiate a Ipv6 connection You must update the route tables IPv6 Routing Internet gateway Egress-only Internet Gateway Destination, Target Nat Gateway VPC Section Summary (1/3) CIDR - IP range VPC - Virtual Private Cloud =&gt; we define a list of IPv4 and IPv6 vCIDR Subnets - tied to an AZ, we define a CIDR Internet Gateway - at the VPC level, provide IPv4 &amp; IPv66 Internet Access Route Tables - Must be edited to add routes from subnets to the IGW, VPC Peering Connection, VPC Endpoints,… Bastion Host: Public EC2 instance to SSH into that has SSH connectivity to EC2 instances i private subnets NAT Instances: give internet access tto EC2 instances in private subnet. Old, must be setup in a public subnet, disable source / Destination check flag NAT Gateway: managed by AWS, provides scalable internet access to private EC2 instances, IPv4 only Private DS + Route 53 : enable DNS Resolution + DNS Hostnames (VPC) VPC Section Summary (2/3) NACL: stateless, subnet rules for inbound and outbound, do’t forget Ephemeral Ports Security Groups - Stateful, operate at the Ec2 instance level Reachability Analyzer - perform network connectivity testing between AWS resources VPC Peering - connect two VPCs with non overlapping CIDR, non-transitive VPC Endpoints - Provide private access to AWS services(S3, DynamoDB, cloudFormation, SSM) within a VPC VPC Flow Logs - can be setup at the VPC / Subnet / ENI Level, for ACCEPT and REJECT traffic, helps identifying attacks, analyze using Athena or CloudWatch Logs Insights Site-to-Site VPC - Set up a Customer Gateway on DC, a Virtual Private Gateway on VPC, and site-to-site VPN over public Internet AWS VPN CloudHub: hub and spoke VPN model to connect your sites VPC Section Summary (3/3) Direct Connect: Setup a Virtual Private Gateway on VPC, and establish a direct private connection to a AWS Direct Connect Location Direct Connect Gateway - Setup a Direct Connect to many VPCs in different AWS regions AWS PrivateLink / VPC Endpoint Services: Connect services privately from your service VPC to customer VPC Doesn’t need VPC Peering, public Internet, AT Gateway, Route Tables Must be used with Network Load Balancer &amp; ENI ClassicLink: Connect EC2 - classic EC2 instances privately to your VPC Transit Gateway - transitive peering connection for VPC, VPN, DX Traffic Mirroring - copy network traffic fromm ENIs for further analysis EEgress-only Internet Gateway - Like a NAT Gateway but for IPv6 Networking Costs in AWS per GB - Simplified Use Private IP instead of Public IP for good savings and better network performance Use same AZ for maximum savings (at the cost of high availability) Minimizing egress traffic network cost Egress traffic: outbound traffic (from AWS tto outside) Ingress traffic: inbound traffic - from outside too AWS Try to keep as much as internet traffic within AWS to minimize costs Direct Connect location that are co-located inn the same AWS Region result inn lower cost for egress network S3 Data Transfer Pricing – Analysis for USA S3ingress:free S3 to Internet: $0.09 per GB S3 Transfer Acceleration Faster transfer times(50 to 500% better) Additional cost on top of Data transfer Pricing: +$0.04 per GB S3 to CloudFront: $0.00 per GB CloudFront to Internet: $0.085 per GB(Slightly cheaper than S3) Caching capability (lower latency) Reduce costs associated with S3 RRequests pricing (7x cheaper with CloudFront) S3 Cross Region Replication: $0.02 per GB NAT Gateway vs Gateway VPC Endpoint Pricing:NAT Gateway vs Gateway VPC EndpointSubnet 1 route tableRegion (us-east-1)VPC (10.0.0.0/16)$0.045 NAT Gateway / hour$0.045 NAT Gateway data processed / GB $0.09 Data transfer out to S3 (cross-region) $0.00 Data transfer out to S3 (same-region) Disaster Recovery Overview Any event that has a negative impact on a company’s business continuity or finances is a disaster Disaster recovery (DR) is about preparing for and recovering from a disaster What kind of disaster recovery? On-premise =&gt; On-premise: traditional DR, and very expensive On-premise -&gt; AWS Cloud: hybrid recovery AWS Cloud Region A =&gt; AWS Cloud Region B Need to define two terms RPO: Recovery Point Objective RTO: Recovery Time Objective Disaster Recovery Strategies Backup ad restore Pilot Light Warm Standby Hot Site / Multi Site Approach Backup and Restore (High RPO) Disaster Recovery – Pilot Light A small version of the app is always running in the cloud Useful for the critical core (pilot light) Very similar tto Backup and Restore Faster than backup and restore as critical systems are already up Warm Standby Full system is up and running, but at minimum size Upon disaster, we can scale to production load Multi Site / Hot Site Approach Very low RTO (minutes or seconds) – very expensive Full Production Scale is running AWS and On Premise All AWS Multi Region Disaster RecoveryTips Backup EBS snapshot, RDS automated backups / Snapshots, etc … Regular pushes to S3/ S3 IA/ Glacier, Lifecycle Policy, Cross Region Replication From O-premise: Snowball or Storage Gateway High Availability Use Route53 to migrate DNS over from Region to Region RDS Multi-AZ, ElasticCache Multi-AZ , EFS, S3 Site to Site VPN as a recovery fromm Direct Connect Replication RDS Replication (Cross Region), AWS Aurora + Global Databases Database replication from on-promise to RDS Storage Gateway Automation CloudFormattion / Elastic Beantalk to re-create a whole new environment Recover / Reboot EC2 instances with CloudWatch iif alarm fail AWS Lambda function for customized automations Chaos Netflix has a “simian-army” random terminating EC2 DMS – Database Migration Service Quickly and securely migrate databases to AWS, resilient, self healing The source database remains available during the migration Supports: Homogeneous migration: ex Oracle to Oracle Heterogeneous migrations: ex Microsoft SQL Server too Aurora Continuous Data Replication using CDC You must create an EC2 instance too perform the replication tasks DMS Sources and Targets SOURCES: On-Premise and EC2 instances databaseTargets: DB AWS Schema ConversionTool (SCT) Convert your Database’s Schema from one engine to another Example OLTP: (SQL Server or Oracle) to MYSQL, PostgresSQL, Aurora Example OLAP: (Teradata or Oracle) to Amazon Redshift Prefer computer-intensive instances to optimize data conversions You do not need to use SCT if you are migrating the same DB engine Ex: On-premise PostgreSQL =&gt; RDS PostgreSQL The DB engine is still PostgreSQL (RDS is the platform) DMS - Continuous Replication On-Premise strategy with AWS Ability to download Amazon Linux 2 AMI as a VM (.iso format) • VMWare, KVM,VirtualBox (Oracle VM), Microsoft Hyper-V • VM Import / Export • Migrate existing applications into EC2 • Create a DR repository strategy for your on-premisesVMs Can export back the VMs from EC2 to on-premises AWS Application Discovery Service Gather information about your o-premise servers to plan a migration Server utilization and dependency mappings Track with AWS Migration Hub AWS Database Migration Service (DMS) replicate On-premise =&gt; AWS, AWS =&gt; AWS , AWS =&gt; O-premise Work with various database technologies (Oracle, Mysql, DynamoDB, etc…) AWS Server Migration Service (SMS) Incremental replication of on-premise live servers to AWS AWS DataSync Move large amount of data from on-premises to AWS Can synchronize to: Amazon S3 (any storage classes – including Glacier), Amazon EFS, Amazon FSx (Windows, Lustre…) Move data from your NAS or file system via NFS or SMB Replication tasks can be scheduled hourly, daily, weekly Leverage the DataSync agent to connect to your systems Can setup a bandwidth limit AWS Backup Fully managed service Centrally manage and automate across AWS services No need to create custom script and manual process • Supported services: EC2, EBS S3 RDS EFS, DocumentDB, Neptune FSx Storage Gateway • Supports cross-region backups • Supports cross-account backups AWS Backup • Supports PITR for supported services • On-Demand and Scheduled backups • Tag-based backup policies • You create backup policies known as Backup Plans Backup frequency (every 12 hours, daily, weekly, monthly, cron expression) Backup window Transition to Cold Storage (Never, Days, Weeks, Months, Years) Retention Period (Always, Days, Weeks, Months, Years) AWS Backup Vault Lock Enforce a WORM (Write Once Read Many) state for all the backups that you store in your AWS Backup Vault Additional layer of defense to protect your backups against: Inadvertent or malicious delete operations Updates that shorten or later retention periods Even the root user cannot delete backups when enable Transferring large amount of data into AWS Example: transfer 200TB of data in the cloud. We have a 100 Mbps internet connection. Over the internet / Site-to-Site VPN: Immediate to Setup Will take 200(TB) * 1000(GB) * 1000(MB) * 8(MB)/100 MBps = 16000000s = 185d Over direct connect 1Gbps: Log for the one time setup (over a month) Will take 200(TB) * 1000(GB) * 8(GB) / 1 Gbps = 18.5d Over Snowball: Will take 2 to 3 snowballs i parallel Takes about 1 week for the end-to-end transfer Ca be combined with DMS For ongoing replication / transfer: Site - to - Site VP or DX with DMS or DataSync" }, { "title": "AWS White Papers and Architectures", "url": "/posts/aws-white-papers-and-architectures/", "categories": "Fullstack, Architect, AWS", "tags": "aws, Architecture", "date": "2024-02-08 00:00:00 +0700", "snippet": "White Papers and ArchitecturesWell Architected Framework, Disaster Recovery, etc…Section Overview Well Architected Framework Whitepaper Well Architected Tool AWS Trusted Advisor Reference archi...", "content": "White Papers and ArchitecturesWell Architected Framework, Disaster Recovery, etc…Section Overview Well Architected Framework Whitepaper Well Architected Tool AWS Trusted Advisor Reference architectures resources (for real-world) Disaster Recovery on AWS Whitepaper Well Architected Framework General Guiding Principles https://aws.amazon.com/architecture/well-architected Stop guessing your capacity needs Test systems at production scale Automate to make architectural experimentation easier Allow for evolutionary architectures Design based on changing requirements Drive architectures using data Improve through game days Simulate applications for flash sale days Well Architected Framework 66 Pillars Operational Excellence Security Reliability Performance Efficiency Cost Optimization SustainabilityThey are not something to balance, or trade-offs, they’re a synergy AWS Well-architected Tool Free tool to review your architectures against the 6 pillars Well-Architected Framework and adopt architectural best practice How does it work? Select your workload and answer questions Review your answers against the 66 pillars Obtain advice: get videos ad documentations, generate a report, see the results iin a dashboard Let’s have a look: https://console.aws.amazon.com/wellarchitected Trusted Advisor No need to install anything - high level AWS account assessment Analyze your AWS accounts and provides recommendation: Core checks and recommendations - all customers Can enable weekly email notification from the console Full Trusted Advisor: Available for business &amp; Enterprise support plans Ability to set CloudWatch alarms when reaching limits Programmatic Access using AWS Support API Trusted Advisor Checks Examples Cost Optimization Low utilization EC2 instances, idle load balancers, under-utilized EBS volumes… Reserved instances &amp; savings plans optimizations Performance: High utilization EC2 instances, CloudFront CDN optimizations EC2 to EBS throughput optimizations, Alias records recommendations Security: MFA enabled on Root Account, IAM key rotation, exposed Access Keys S3 Bucket permissions for public access, security groups with unrestricted ports Fault Tolerance: EBS snapshots age, Availability Zone balance ASG Multi-AZ, RDS Multi-AZ, ELB configuration,,, Service Limits More Architecture Examples We explored the most important architectural patterns: Classic: EC2, ELB, RDS, ElastiCache, etc … Serverless: S3, Lambda, DynamoDB, CloudFront, API Gateway, etc… If you want to see more AWS architectures: " }, { "title": "AWS Virtual Private Cloud Overview (VPC) part 1", "url": "/posts/aws-virtual-private-cloud-overview-vpc-part-1/", "categories": "Fullstack, Blogging, AWS", "tags": "AWS, VPC, NACL, Transit Gateway, AWS Private Link, Direct Connect, VPC Flow Logs, Ephemeral Ports, VPC Peering, Internet Gateway, NAT, AWS VPN CloudHub, VPN", "date": "2024-02-07 00:00:00 +0700", "snippet": "Virtual Private Cloud (VPC)Default VPC Walkthrough All new AWS accounts have a default VPC New EC2 instances are launched into the default VPC of no subnet is specified Default VPC has Internet ...", "content": "Virtual Private Cloud (VPC)Default VPC Walkthrough All new AWS accounts have a default VPC New EC2 instances are launched into the default VPC of no subnet is specified Default VPC has Internet connectivity and all EC2 instances inside it have public IPv4 addresses We also get a public and a private IPv4 DNS names VPC in AWS - IPv4 VPC = Virtual Private Cloud You can have multiple VPCs in an AWS region (max. 5 per region - soft limit) Max. CIDR per VPC is 5, for each CIDR Min. size is /28 (16 addresses) Max. size is /16 (65536 IP addresses) Because VPC is private, only the Private IPv4 tanges are allowed 10.0.0.0 – 10.255.255.255 (10.0.0.0/8) 172.16.0.0 – 172.31.255.255 (172.16.0.0/12) 192.168.0.0 – 192.168.255.255 (192.168.0.0/16) Your VPC CIDR should NOT overlap with your other networks (e.g corporate) VPC – Subnet (IPv4) AWS reserves 5 IP addresses (first 4 &amp; last 1) in each subnet These 5 IP addresses are not available for use and can’t be assigned to an EC2 instance Example; if CIDR block 10.0.0.0/24 then reserved IP addresses are: 10.0.0.0 - Network Address 10.0.0.1 - reserved by AWS for the VPC router 10.0.0.2 - reserved by AWS for mapping to mazon-provided DNS 10.0.0.3 - reserved by AWS for future use 10.0.0.255 - Network Broadcast Addressm AWS does not support broadcast in a VPC therefore the address is reserved Exam tip: if you need 29 IP addresses for EC2 instances: You can’t choose a subnet of size /27 (32 IP addresses, 32-5 = 27 &lt; 29) you need to choose a subnet of size /26 (64 IP address, 64 - 5 = 59 &gt; 29) Internet Gateway (IGW) Allows resources (e.g EC2 instances) in a VPC connect to the Internet It scales horizontally and is highly available and redudant Must be created separately from a VPC One VPC can only be attached to one IGW and vice versa Internet Gateways on their own do not allow Internet access… Route tables must also be edited! State of Hands-on Adding Subnets Adding Internet Gateway Editing Route Tables Bastion Hosts We can use a Bastion Host to SSH into our private EC2 instances The bastion is in the public subnet which is then connected to all other private subnets Basion Host security group must be tighened Exam Tip: Makesure the bastion host only has port 22 traffic from the IP address you need not from the security groups of your other EC2 instances NAT Instance (outdated, but still at the exam) NAT = Network Address Translation Allows EC2 instances in private subnet to connect to the Internet Must be launched in a public subnet Must disable EC2 setting: Source / destination Check Must have Elastic IP attached to it Route Tables must be configured to route traffic from private subnets to the NAT instance NAT Instance NAT Instance – Comments Pre-configured Amazon Linux AMI is available Reached the end of standard support on December 31, 2020 Not highly available / resilient setup out of the box You need to create an ASG in multi-AZ + resilient user-data script Internet traffic bandwidth depends on EC2 instance type You must manage Security Groups &amp; Rules: Inbound: Allow HTTP / HTTPS traffic coming from Private Subnets Allow SSH from your home network (access is provided through Internet Gateway) Outbound: Allow HTTP / HTTPS traffic to the Internet NAT Gateway AWS-managed NAT, higher bandwidth, high availability, no administration Pay per hour for usage and bandwidth NATGW is created in a specific Availability Zone, uses an Elastic IP Can’t be used by EC2 instance in the same subnet (only from other subnets) Requires an IGW (Private Subnet =&gt; NATGW =&gt; IGW) 5 Gbps of bandwidth with automatic scaling up to 45 Gbps No Security Groups to manage / required NAT Gateway with High Availability NAT Gateway is resilient within a single Availability Zone Must create multiple NAT Gateways in multiple AZs for fault-teolerance There is no cross-AZ failover needed because if an AZ goes down it doesn’t need NAT NAT Gateway vs. NAT Instance DNS Resolution in VPC DNS Resolution (enableDnsSupport) Decides if DNS resolution from Route 53 Resolver server is supported for the VPC True (default): it queries the Amazon Provider DNS Server at 169.254.169.253 or the reserved IP address at the base of the VPC Ipv4 network range plus two DNS Resolution in VPC DNS Hostnames (enableDnsHostnames) By default True =&gt; default VPC False =&gt; newly created VPCs Won’t do anything unless enableDnsSupport=true If True, assigns public hostname to EC2 instance if it has a public IPv4 DNS Resolution in VPC If you use custom DNS domain names in a Private Hosted Zone in Route 53, you must set both these attributes (enableDnsSupport &amp; enableDnsHostname) to true Security Groups &amp; NACLs Incoming Request Outgoing Request Network Access Control List (NACL) NACL are like a firewall which control traffic from and to subnet One NACL per subnet, new subnets are assigned the Default NACL You define NACL Rules: Rules have a number (1-32766), higher precedence with a lower number First rule match will drive the decision Example: if you define #100 ALLOW 10.0.0.10/32 and #200 DENY 10.0.0.10/32, the IP address will be allowed because 100 has a higher precedence over 200 The last rule is an asterisk (*) and denies a request in case of no rule match AWS recommends adding rules by increment of 100 Newly created NACLs will deny everything NACL are a great way of blocking a specific IP address at the subnet level NACLs default NACLs Accepts everything inbound/outbound with the subnets it’s associated with Do NOT modify the Default NACL, instead create custom NACLs Ephemeral Ports For any two endpoints to establish a connection, they must use ports Clients connect to a defined port, and expect a response on an ephemeral port Different Operating Systems use different port ranges, examples: IANA &amp; MS Windows 10 -&gt; 49152 - 65535 Many Linux Kernels -&gt; 32768 - 60999 NACL with Ephemeral Ports Create NACL rules for each target subnets CIDR Security Group vs. NACLs VPC – Reachability Analyzer A network diagnostics tool that troubleshoots network connectivity between two endpoints in your VPC(s) It builds a model of the network configuration, then checks the reachability based on these configurations (it doesn’t send packets) When the destination is Reachable - It proceduces hop by hop details of the virtual network path Not reachable - It identifies the blocking components Use cases: troubleshoot connectivity issues, ensure network configuration is as intended… VPC Peering Privately connect two VPCs using AWS network Make them behave as if they were in the same network Must not have overlapping CIDRs VPC Peering connection is NOT transitive (must be established for each VPC that need to communicate with one another) You must update route tables in each VPC’s subnets to ensure EC2 instances can communicate with each other VPC Peering – Good to know You can create VPC Peering connection between VPCs in different AWS accounts/regions You can reference a security group in a peered VPC (works cross – same region) VPC Peering VPC Endpoints VPC Endpoints (AWS PrivateLink) Every AWS service is publicly exposed (public URL) VPC Endpoints (powered by AWS PrivateLink) allows you to connect to AWS services using a private network instead of using the public Internet They’re redudant and scale horizontally They remove the need of IGW, NATGW,… to access AWS Services In case of issues Check DNS Setting Resolution in your VPC Check Route Tables Types of Endpoints Interface Endpoints Provisions an ENI (private IP address) as an entry point (must attach a Security Group) Supports most AWS services Gateway endpoints Provisions a gateway and must be used as a target in a route table Supports both S3 and DynamoDB VPC Flow Logs Capture information about IP traffic going into your interfaces: VPC Flow Logs Subnet Flow Logs Elastic Network Interface (ENI) FLow Logs Helps To monitor &amp; troubleshoot connectivity issues Flow logs data can go to S3 / CLoudWatch Logs Captures network information from AWS managed interfaces too: ELB, RDS, ElastiCache, Redshift, Workspaces, NATGW, Transit Gateway… VPC Flow Logs VPC Flow Logs Syntax srcaddr &amp; dstaddr: help identify problematic IP srcport &amp; dstport: help identify problematic ports Action: success or failure of the request due to security group / NACL Can be used for analytics on usage patterns, or malicious behavior Query VPC flow logs using Athena on S3 or CloudWatch Logs Insights Flow Logs examples: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-records-examples.html VPC Flow Logs – Troubleshoot SG &amp; NACL issues Incoming Requests Inbound REJECT =&gt; NACL or SG Inbound ACCEPT, Outbound REJECT =&gt; NACL Outgoing Requests Outbound REJECT =&gt; NACL or SG Outbound ACCEPT, Inbound REJECT =&gt; NACL AWS Site-to-Site VPN AWS Site-to-Site VPN Virtual Private Gateway (VGW) VPN concentrator on the AWS side of the VPN connection VGW is created and attached to the VPC from which you want to create the Site-to-site VPN connection Possibility to customize the ASN (Autonomous System Number) Customer Gateway (CGW) Software application or physical device on customer side of the VPN connection https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html Site-to-site VPN connections Customer Gateway Device (On-premises) What IP address to use? Public Internet routable IP address for your customer gateway device If it’s behind a NAt device that’s enabled for NAT traversal (NAT-T), use the public IP address of the NAT device Important step: enable Route Propagation for the virtual private gateway in the route table that is associated with your subnets If you need to ping your EC2 instances from on-premise, make sure you ad the ICMP protocol on the inbound of your security groups AWS VPN CloudHub Provide secure communication between multiple sites, if you have multiple VPN connections Low cost hub and spoke model for primary or secondary network connectivity between diffrent locations (VPN only) It’s a VPN connection so it goes over the public internet To set it up, connect multiple VPN connections on the sam VGW, setup dynamic routing and configure route tables Direct Connect (DX) Provides a dedicated private connection from a remote network to your VPC Dedicated connection must be setup between your DC and AWS Direct Connect locations You need to setup a Virtual Private Gateway on your VPC Access Public resources (S3) and private (EC2) on same connection Use Cases: Increase bandwidth thoughtput - working with large data sets - lower cost More consistent network experience - application using real-time data feeds Hybrid environments (on-prem + cloud) Supports both IPv4 and IPv6 Direct Connect Diagram Direct Connect Gateway If you want to setup a Direct Connect to one or more VPC in many different regions (same account), you must use a Direct Connect Gateway Direct Connect – Connection Types Dedicated Connections: 1Gbps and 10 Gps capacity Physical ethernet port dedicated to a customer Request made to AWS first, then completed by AWS Direct Connect Partners Hosted Connection: 50Mbs, 500 Mbps, to 10 Gbps Connection requests are made via AWS Direct Connect Partners Capacity can be added or removed on demand 1, , 5, 10 Gbps available at select AWS Direct Connect Partners Lead times are often longer than 1 month to establish a new connection Direct Connect – Encryption Data in transit is not encrypted but is private AWS Direct Connect + VPN provides an IPsec-encrypted private connection Good for an extra level of security, but slightly more complex to put in place Direct Connect - Resiliency High Resiliency for Critical Workloads Maximum Resiliency for Critical Workloads Exposing services in your VPC to other VPC Option 1: make it public Goes through the public www Tough to manage access Option 2: VPC peering Must create many perring relations Open the whole network AWS PrivateLink (VPC Endpoint Services) Most secure &amp; scalable way to expose a service to 1000s of VPC (own or other accounts) Does not require VPC peering, internet gateway, NAT, route tables… Requires a network load balancer (Service VPC) and ENI (Customer VPC) or GWLB If the NLB is in multiple AZ, and the ENIs in multiple AZ, the solution is fault tolerant! AWS Private Link &amp; ECS EC2-Classic &amp; AWS ClassicLink (deprecated) EC2-Classic: instances run in a single network shared with other customers AmazonVPC: your instances run logically isolated to your AWS account ClassicLink allows you to link EC2-Classic instances to a VPC in your account Must associate a security group ENables communication using privte Upv3 addressess Removes the need to make use of public IPv4 addressess or Elastic IP addresses Likely to be distractors at the exam Network topologies can become complicated Transit Gateway For having transitive peering between thousands of VPC and on-premise, hub and spoke (star) connection Regional resource, can work cross region Share cross-account using Resource Access Manager (RAM) You can peer Transit Gateway across regions Route Tables: limit which VPC can talk with other VPC Works with ** Direct Connect Gateway, VPn connections** Supports Ip Multicast (not suppoerted by any other AWS service) Transit Gateway: Site-to-Site VPN ECMP ECMP = Equal - cost multi path routing Routing strategy to allow to forward a packet over multiple best path Use case: create multiple Site to site VPN connections to increase the bandwidth of your connection to AWS Transit Gateway: throughput with ECMP Transit Gateway – Share Direct Connect between multiple accounts " }, { "title": "AWS Router 53", "url": "/posts/aws-router-53/", "categories": "Fullstack, Blogging, AWS", "tags": "aws, router53, DNS, A, AA, CNAME, MX, TXT, TTL, NS, Hosted Zones, Geoproximity, Traffic flow, Routing Policies, Geolocation, Failover(Active - Passive), PrivateHosted Zones, Health Checks", "date": "2024-02-07 00:00:00 +0700", "snippet": "Route 53 SectionWhat is DNS Domain Name System which translates the human friendly hostnames into the machine IP address www.google.com =&gt; 172.2.17.18.36 DNS is the backbone of the internet ...", "content": "Route 53 SectionWhat is DNS Domain Name System which translates the human friendly hostnames into the machine IP address www.google.com =&gt; 172.2.17.18.36 DNS is the backbone of the internet DNS uses hierachical naming structure DNS Terminologies Domain Registrar: Amazon Route 53, GoDaddy, … DNS Records: A, AAAA, CNAME, NS, … Zone file: contains DNS records Name Server: resolves DNS queries (Authoritative or Non- Authoritative) Top Level Domain (TLD): .com, .us, .in, .gov, .org, … Second Level Domain (SLD): amazon.com, google.com, … How DNS works Web browser -&gt; Local DNS Server: Root DNS Server managed by ICANN (res: .com NS 1.2.3.4) TLD DNS Server managed by IANA (res: domain NS 5.6.7.8) SLD DNS Server managed by Domain Registrar (res: domain IP) Amazon Route 53 A highly available, scalable, fully managed and Authoritative DNS Authoritative = the customer (you) can update the DNS records Route 53 is also a Domain Registrar Ability to check the health of your resources The only AWS service which provide 100% availability SLA Why Route 53? 53 is a reference to the traditional DNS port Route53 - Records How you want to route traffic for a domain Each record contains: Domain/subdomain Name: example.com RecordType: e.g A or AAAA Value: 12.34.56.78 Routing Policy: How Route 53 responds to queries TTL: amount of time the record cached at DNS resolvers Route 53 supports the following DNS record types: A / AAAA / CNAME / NS CAA / DS / MX / NAPTR / PTR / SOA / TXT / SPF / SRV Route 53 - Record Types A: maps a hostname to Ipv4 AAAA: maps a hostname to IPv6 CNAME: maps a hostname to another hostname The target is a domain name which must have an A or AAAA record Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex) Exmaple: you can’t create for example.com but you create for www.example.com NS: name Server for the hosted zone Control how traffic is routed for a domain Route 53 - Hosted Zones A container for records that define how to route traffic to a domain and its subdomains Public Hosted Zone: contains records that specfy how to route traffic on the Internet (public domain names) Private Hosted Zones: contain records that specify how you route traffic within one or more VPCs (private domain names) application 1.company.internal You pay $0.50 per month per hosted zone Route 53: Public vs. Private Hosted Zone Public Hosted Zone: internet access Private Hosted Zone: unable to access from internet: EC2 instances, RDS instance Route 53 - Records TTL (Time To Live) High TTL - e.g 24 hr less traffic on Route 53 Possibily outdated records Low TTL - e.g 60 sec More traffics on Route 53 ($$) Records are outdated for less time Easy to change records Except for Alias records, TTL is mandatory for each DNS record CNAME vs Alias AWS Resources (Load Balancer, CloudFront…) expose an AWS hostname: b1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com CNAME: Points a hostname to any other hostname. (app.mydomain.com =&gt; bla.bla.anything.com) Only for non root domain (aka. something.mydomain.com) Alias: Points a hostname to an AWS Resource Works for Root domain and non root domain (aka mydomain.com) Free of charge Native health check Route 53 - Alias Records Maps a hostname to an AWS resource An extension to DNS functionality Automatically recognizes changes in the resource’s IP addresses Unlike CNAME, it can be used for the top node of a DNS namespace (zone Apex), e.g: example.com Alias Record is always of type A/AAAA for AWS resources (IPv4/IPv6) You can’t set the TTL Route 53 - Alias REcords target Elastic Load Balancers CloudFront Distributions API Gateway Elastic Beantalk environments S3 Websites VPC Interface Endpoints Global Accelerator accelerator Route 53 record in the same hosted zone You cannot set an ALIAS record for an EC2 DNS name Route 53 - Routing policies Define how Route 53 responds to DNS queries Don’t get confused by the word “Routing” It’s not the same as load balancer routing which routes the traffic DNS does not route any traffic, it only responds to the DNS queries Route 53 support the following Routing policies Simple Weighted Failover Latency based GeoLocation Multi-Value Answer Geoproximity (using Route 53 Traffic Flow feature) Routing Policies - Simple Typically route traffic to a single resource Can specify multiple values in the same record If multiple values are returned, a random one is chosen be the client When Alias enabled, specify only one AWS resource Can’t be associated with Health Checks Routing Policies – Weighted Control the % of the requests that go to each specific resource Assign each record a relative weight traffic(%) = Weight for a specific record / sum all the weights for all records Weights don’t need to sum up to 100 DNS records must have the same name and type Can be associated with Health Checks Use cases: Load balancing between regions, testing new application versions… Assign a weight of 0 to a record to stop sending traffic to a resource If all records have weight of 0, then all records will be returned equally Routing Policies - Latency based Redirected to the resource that has the least latency close to us Super helpful when latency for users is a priority Latency is based on traffic betwwen users and AWS Regions Germany users may be directed to be the US (if that’s the lowest latency) Can be associated with Health Checks (has a failover capability) Route 53 - Health Checks HTTP Health Checks are only for public resources Health Check =&gt; Automated DNS Failover: Health checks that monitor an endpoint (application, server, other AWS resource) Health checks that monitor other health checks (Calculated Health Checks) Health checks that monitor CloudWatch Alarms (full control !!) - e.g throttles of DynamoDB, alarms on RDS, custom metrics, … (helpful for private resources) Health Checks are integrated with CW metrics Health Checks – Monitor an Endpoint About 15 global health checkers will check the endpoint health Health/Unhealthy Threshold - 3 (default) Interval - 30 sec (can set to 10 sec - higher cost) Supported protocol: HTTP, HTTPS and TCP If &gt; 18% of health checkers report the endpoint is health, Route 53 considers it Health. Otherwise, it’s Unhealty Ability to choose which locations you want Route 53 to use Health Checks pass only when the endpoint respond with the 2xx and 3xx status codes Health Checks can be setup to pass / fail based on the text in the first 5120 bytes of the response Configure you router/firewall to allow incoming requests from Route 53 Health Checkers Route 53 - Calculated health checks Combine the results of multiple Health Checks into a single Health Check You can use Or, AND, or NOT Can monitor up to 256 Child Health Checks Specify how many of the health checks need to pass to make the parent pass Usage: perform maintenance to your website without causing all health check to fail Health Checks – Private Hosted Zones Route 53 health checkers are outside the VPC They can’t access private endpoints (private VPC or on-premises resource) You can create a CloudWatch Metric and associate a CloudWatch Alarm, then create a Health Check that checks the alarm itself Routing Policies - Failover (Active - Passive) Routing Policies Geolocation Different from latency-based! This routing is based on user location Specify location by COntinent, Country or by US State (if there’s overlaping, most precise location selected) Should create a “Default” record (in case there’s no match on location) Use cases: website localization, restrict content distribution, load balancing, … Can be associated with Health Checks Routing Policies – Geoproximity Route traffic to your resources based on the geographic location of users and resources Ability to shift more traffic to resources based on the defined bias To change the size of the geographic region, specify bias values To expand ( 1 to 99): more traffic to resource To shrink (-1 to -99): less traffic to the resource Resources can be: AWS resources (specify AWS region) Non-AWS resources (specify latitude and longitude) You must use Route 53 traffic flow to use this feature Routing Policies - Geoproximity Routing 53 - Traffic flow Simplify the process of creating and maintaining records in large and complex configurations Visual editor to manage complex routing decision trees Configurations can be saved as TRaffic Flow Policy Can be applied to different Route 53 Hosted Zones (different domain names) Supports versioning Routing Policies - Multi value Use when routing traffic to multiple resources Route 53 return multiple value/resources Can be associated with Health Checks (return only values for healthy resources) Up to 8 healthy records are returned for each Multi-Value query Multi-value is not a substitude for having an ELB Domain Registar vs DNS Service You buy or register you domain name with a Domain Registar typically by paying annual charges (e.g GoDaddy, Amazone Registar Inc, …) The Domain Registar ussually provides you with a DNS service to manage your DNS records But you can use another DNS service to manage your DNS records Example: purchase the domain from GoDaddy and use Route 53 manage your DNS records GoDaddy as Registrar &amp; Route 53 as DNS Service 3rd Party Registrar with Amazon Route 53 If you buy your domain on a 3rd party registrar, you can still use Route 53 as the DNS Service provider Create a Hosted Zone in Route 53 Update NS Records on 3rd party website to use Route 53 Name Servers Domain Registar != DNS Service But every Domain Registrar usually comes with some DNS features A record mapping to IPv4 AAAA record mapping to IPv6 CNAME records shortcuts to hostname CNAME ftp CNAME mail CNAME wwww google.com zone MX records how email work MX 10 mail MX 20 mail.other.domain TXT records — query txt, spam TTL 3600 Time to live , numeric value in seconds authoritative TTL 3600 Non-Authoritative TTL The time for which a DNS resolver caches a response is set by a value called the time to live (TTL) associated with every record. Amazon Route 53 does not have a default TTL for any record type. NS: how the root zone delegates control of .org to the .org registry" }, { "title": "Extra Solution Architecture and AWS Services discussions", "url": "/posts/extra-solution-architecture-and-aws-services-discussions/", "categories": "Fullstack, Architect, AWS", "tags": "FanOut, SNS, SQS, Lambda, EC2, SA, Solution, Pattern, CloudFront, WAF, ALB, Solution Architecture, HighPerformance Computing, Cache, NLB", "date": "2024-02-06 00:00:00 +0700", "snippet": "Extra Solution Architecture and AWS Services discussionsLambda, SNS &amp; SQS SQS + Lambda SQS FIFO + Lambda SNS asynchroous to Lambda + SQS Fan Out Pattern: deliver to multiple SQS SDK pu...", "content": "Extra Solution Architecture and AWS Services discussionsLambda, SNS &amp; SQS SQS + Lambda SQS FIFO + Lambda SNS asynchroous to Lambda + SQS Fan Out Pattern: deliver to multiple SQS SDK put SQSs SDK put SNS subcrible SQS(fan out) S3 Events S3: objectCreated, S3:ObjectRemove, S3:ObjectRestore, S3:Replication… Object name filtering possible (*.jpg) Use case: generate thumbnails of images upload to S3 Can create as many “S3 events” as desired S3 event notificatiions typically deliver events in secconds butt can sometimes take a minutesor longer If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent If you want to ensure tthat an event notification is sent for every successful write you can enable versioning on your bucket Caching Strategies Clientt -&gt; CloudFront -&gt; API gateway -&gt; App Logic Ec2 / Lambda -&gt; Redis Memcached -&gt; Database Cliennt -&gt; CloudFront(Edge) -&gt; S3 Caching, TTL, Network, Computation, Cost, Latency Blocking an IP address EC2 Instance Public IP Optional Firewall Software in EC2s NACL + Security groups Blocking an IP address – with an ALB Application Load Balancer Connection Termination EC2 Instance Private IP Blocking an IP address – with an NLB Network Load Balancer Traffic goes through No Security Group Blocking an IP address – ALB + WAF ALB + WAF(IP address filtering) Blocking an IP address – ALB, CloudFront WAF Client -&gt; CloudFront(GEO Restriction) -&gt; WAF IP address filtering -&gt; CloudFrontt Public IPs -&gt; Public ALB -&gt; EC2 Security group High Performance Computing (HPC) The cloud is perfect place to perform HPCC You can create a very high number of resources in no time You can speed up time to results by adding more resources You can pay only for the systems you have used Perform genomics, computation chemistry, financial risk modeling, weather predictionn, machine learning, deep learning, autonomous driving Which services help perform HPC? Data Management &amp; Transfer AWS Direct Connect Move GB/s of data to the cloud, over a private secure network Snowball &amp; Snowmobile Move PB of data to the cloud AWS DataSync Move large amount of datta between on-premises and S3, EFS, FSc for Windows Compute and Nettworking EC2 Instances: CPU optimized, GPU opttimized Spot Instances / Spot Fleets for cost savings + Auto Scaling EC2 Placement Groups: Cluster for good network performance Placement group cluster low latency 10Gbps network Compute and Networking EC2 Enhaced Networking (SR-IOV) Higher bandwidth, higher PPS (packet per second), lower latency Option1: Elastic Network Adapter (ENA) up to 100 Gbps Option2: Intel 82599 VF up to 10 Gbps - LEGACY Elastic Fabric Adapter (EFA) Improved ENA for HPC, only works for linux Greate for inter-node communication, tighly coupled workloads Leverages Message Passing Interface (MPI) standard Bypasses the underlying Linux OS to provide low-latenccy, reliable trannsport Storage Instance-attached storage EBS: scale up to 256000 IOPS with io2 Block Express Instance Store: Scale tto millions of IOPS, linked to EC2 instancee, low latenccy Network storage: Amazon S3: large blob, not a file system Amazon EFS: scale IOPS based on ttottal size, or use provisioned IOPS Amazon FSx for Lustre: HPC optimized distributed file system, milions of IOPS Backed by S3 Automation and Orchestration AWS Batch AWS BAtch supports multi-node parallel jobs, which enables you to run single jobs that span multtiple EC2 instances Easily schedule jobs and launch EC2 instances accordingly AWS ParallelCluster Open-source cluster management tool to deploy HPC on AWS Configure with text files Automate creation of VPC, subnet, clusterr type and instancce types Abiliitty to enable EFA on the cluster (improves nnetwork performance) Creating a highly available EC2 instance Creating a highly available EC2 instance With an Auto Scaling Group Creating a highly available EC2 instance width ASG + EBS Creating a highly available EC2 instance HA options for the bastion host Run2across2AZ Run 1 across 2 AZ with 1 ASG 1:1:1 Routing to the bastion host If 1 bastion host, use an elastic IP with ec2 user-data script to access it If 2 bastion hosts, use an Network Load Balancer (layer 4) deployed in multiple AZ If NLB, the bastion hosts can live in the private subnet directly Note: Can’t use ALB as ALB is layer 7(HTTP protocol)" }, { "title": "Exam Review & Tips", "url": "/posts/exam-review-tips/", "categories": "Fullstack, Architect, AWS", "tags": "aws", "date": "2024-02-06 00:00:00 +0700", "snippet": "Exam Review &amp; TipsState of learning checkpoint Let look how far we’ve gone on our learning Practice makes perfect If you’re new to AWS, take a bit of AWS practice thanks to this course ...", "content": "Exam Review &amp; TipsState of learning checkpoint Let look how far we’ve gone on our learning Practice makes perfect If you’re new to AWS, take a bit of AWS practice thanks to this course before rushing to the exam The exam recommends you to have one or more years of hands-on experiencee on AWS Practice makes perfect! If you feel overwhelmed by the amount of knowledge you just learned, just go through it one more time Proceed by elimination Most question are going to be scenario based For all the questions, rule out answers that you know for sure are wrong For the remaining answers, understand which one makes the most sense There are very few trick question Don’t over-think it If a solution seems feasible but highly complicated, it’s probably wrong Skim the AWS Whitepapers You can read about some AWS White Pappers here: Architecting for the Cloud: AWS Best Practicces AWS Well-architected Framework AWS Disaster Recovery Overall we’ve explored all the most important concepts in the course It’s never bad to have a look at the whitepappers you think are interesting! Read each service’s FAQ FAQ = Frequently asked questionsExample: FAQ cover a lot of the questions asked at the examThey help confirm your understannding of a service Get into the AWS Community Help out ad discuss with other people in the course Q&amp;A Review questions asked by other people in the Q&amp;A• Review questions asked by other people in the Q&amp;A Do the practice test in this section Read forums online Read online blogs Attend local meetups and discuss with other AWS engineers Watch re-invent videos on Youtube (AWS Conference) How will the exam work? You’ll have to register online at fee for the exam is 150 USD Provide two identity documents (ID, Credit Card, details are in emails sent to you) No notes are allowed, no pen is allowed, no speaking 65 question will be asked in 130 minutes At the end you can optionally review all the question / answers You will know right away if you passed / failed the exams You will not know which answers were right / wrong You will know the overall score a few days later (email nottification) To pass you need score of a least 720 out of 1000 If you fail, you can rettakke the exam again 14 days later" }, { "title": "Developing on AWS", "url": "/posts/developing-on-aws/", "categories": "Fullstack, Architect, AWS", "tags": "aws, SDK, AWS, AWS SDK", "date": "2024-02-06 00:00:00 +0700", "snippet": "Developing on AWSAWS EC2 Instance Metadata AWS EC2 Instance Medata is powerful but one of the least known features to developers It allows AWS EC2 instances to “learn about themselves” without us...", "content": "Developing on AWSAWS EC2 Instance Metadata AWS EC2 Instance Medata is powerful but one of the least known features to developers It allows AWS EC2 instances to “learn about themselves” without using an IAM role for that purpose THe URL is http://169.254.169.254/latest/meta-data You can retrieve the IAM Role name from the metadata, but you cannot retrieve the IAM policy. Metadata = Infor about the EC2 instance Userdata = launch script of the EC2 instance AWS SDK Overview What if you want to perform actions on AWS directly from your applications code? (without using the CLI) You can use an SDK (software development kit) ! Official SDKs are… Java .NET Node.js PHP Python (named boto3 / botocore) Go Ruby C++ AWS SDK Overview We have to use the AWS SDK when coding against AWS Services such as DynamoDB Fun fact… the AWS CLI uses the Python SDK (boto3) The exam experts you to know when you should use an SDK We’ll pratice the AWS SDK when we get to the Lamba functions Good to know: if you don’t specify or configure a default region, then us-east-1 will be chosen by default" }, { "title": "AWS Security & Encryption", "url": "/posts/aws-security-encryption/", "categories": "Fullstack, Architect, AWS", "tags": "AWS, SSL, KMS, AWS KMS, AWS Shared Responsibility Model, Amazon Macie, Amazon Inspector, AWS Secrets Manager, AWS Shield, CloudHSM, AWS WAF, AWS Firewall Manager, Amazon GuardDuty, Inspector, SSM", "date": "2024-02-06 00:00:00 +0700", "snippet": "AWS Security &amp; EncryptionKMS, Encryption SDK, SSM Parameter StoreWhy encryption? Encryptio in flight (SSL) Data is encrypted before sennding and decrypted after receiving SSL certtificatts he...", "content": "AWS Security &amp; EncryptionKMS, Encryption SDK, SSM Parameter StoreWhy encryption? Encryptio in flight (SSL) Data is encrypted before sennding and decrypted after receiving SSL certtificatts help with enccryptionnn (HTTPS) Encryption in flight ensure no MITM (ma in tthe middle attack) can happen Why enryption? Server side encryption at rest Data is encrytion after being received by the server Data is decryted before beinng sent It is stored in an encryted form thanks to a key (usually a data key) The enncryption / decryption keys must be managed somewhere and the server must have access to it Why encryption? Client side encryption Data is encrypted by the client and never decrypted by server Data will be decrypted by a receivinng client The server should not be able to decrypt the data Cloud leverage Envelope Encryption AWS KMS (Key Management Service) Anytime you hhear “enncryption” for an AWS service, it’s most likely KMS Easy way to control access to your data, AWS manages keys for us Fully integrated with IAM for authorizationn Seamlessly integrated into Amazon EBS: encrypt volumes Amazon S3: Server side ecryption of objects Amazon Redshift: enryption of data Amazon RDS: encryption of data Amazon SSM: Parameter store Etc… But you can also use CLI/SDK KMS - Customer master Key (CMK) Types Symmetric (AES-256 keys) First offering of KMS, single encryption key that is used to encrypt and Decrypt AWS Services that are integrated with KMS use Symetric CMKs Necessary for envelope encryption You never get acccess to the Key unenncrypted (must call KMS API to use) Asymmetricc (RSA &amp; ECCC kkey pairs) Publicc (Ecrypt) and Private key (Decrypt) pair Used for Ennccrypt/DEryptt, Or Sign ? Verify operattions The publicc key is downloadable, but you cann’t access tthe Private Key unencrypted Use case: enncryption outside of AWS bu users who can’t call the KMS API AWS KMS (Key Mmanagement Service) Able to fully manage the keys &amp; policies Create Rotation policies Disable Enable Able to audit key usage (using CloudTrail) Three types of customer master keys (CMK): AWS managed servicce default CMK: free User keys created in KMS: $1 / month User keys imported (must be 256 bit sysmetric key): $1 / month pay for API call to KMS ($0.03 / 1000 calls) AWS KMS 101 Anytime you need to share sensitive information… use KMS Database passwords Credenttials to external service Private key of SSL certificattes The value in KMS is that the CMK can be rotated for extra security Never ever store your secrets in plaintext, especially in your code! Encrypted secrets can be stored in the code / environment variables KMS can only help inn ecrypting up to 4KB of data per call if data &gt; 4KB, use envelope enncryption To give access to KMS to someone: Make sure the Key policy allows the user Make sure the IAM Policy allows the API calls Copy Snapshots across regions Create a snapshott, encrypted with your ownn CMK Attach a KMS Key policy tto authorize cross-account access Share the ecrypted snapshott (in target) Create a copy of the snapshot, encrypt itt withh a KMS Key in your account Create a volume from the snapshot { \"Sid\": \"Allow use of the key with destination account\", \"Effectt\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam:TARGET_GROUP_ID:role/ROLENAME\" }, \"Action\": [ \"kms:Decrypted\", \"kms:CreateGrant\" ], \"Resource\": \"*\", \"Conditio\": { \"StringEquals\": { \"kms:ViaService\": \"ec2.REGION.amazonaws.com\", \"kms:CallerAccount\": \"TARGET-ACCOUNT-ID\" } }} KMS Automatic Key Rotation For customer-managed CMK (not AWS managed CMK) If enabled: automatic key rotation happens every 1 year Previous key is kept active so you cann descrypt old data new key has the same CMK ID (only the backinng key is changed) KMS Manual Key Rotation When you want to rotate key every 90 days, 180 days, etc… New Key has a different CMK ID Keep the previous kkey active so you can decrypt old data Better to use aliases in this case (to hide the change of key for the application) Good soluttion to rotate CMK that are not eligible for automatic rotation (like asymmetric CMK) KMS Alias Updating Better to use aliases in this case (to hide the change of key for the application) SSM Parameter Store Secure storage for configuration and secrets Optional seamless encryption using KMS Serverless, scalable, durable, easy SDK Version tracking of configurations / secrets Configuration management using path &amp; IAM Notifications with CloudWatch Events Integrattion with CloudFormattion SSM Parameter Store Hierachy Standard and advanced parameter tiers Parameters Policies (for advanncced parameters) Allow to assign a TTL to a parameter (expiration date) to force updating or deleting sensitive data such as passwords Can assign multiple policies at a time AWS Secrets Manager Newer service, meant for storing secrets Capabilitty to force rottation of secrets every X days Automate generation of secrets on rotation (use Lambda) Integration with Amazon RDS (MYSQL, PostgreSQL, Aurora) Secrets and encrypted using KMS Mostly meant for RDS integration AWS Shield AWS Shield Standard Free Service That is activated for every AWS customer Provides protection from attacks such as SYN/UDP Floods, Reflecttion attackks and other layer 3/layer 4 attacks AWS Shield Advanced Opttional DDOS mitigation service ($3000 per month per organization) Protec against more sophisticated attack on Amazon Ec2, Elastic Load Balanncing (ELB), Amazon CloudFrontt, AWS Global Accelerator, and Route 53 24/7 access to AWS DDoS response team (DRP) Protect against higher fees during usage spikes due to DDoS CloudHSM KMS =&gt; AWS manages the software for encryption CloudHSM =&gt; AWS provisions encryption hardware Dedicated Hardware (HSM = Hardware Security Module) You manage your ownn encryption keys entirely (not AWS) Supports both symmetric and asymmetric encryptionn (SSL/TLS keys) No free tier available Must use the CloudHSM Client Software Redshift supports CloudHSM for database encryption and key managementt Good option to use with SSE-C encryption CloudHSM Diagram IAM permissions CRUD an HSM Cluster CloudHSM Software Manage the keys Manage the users CloudHSM - High Availability CloudHSM clustters are spread accrooss multi AZ (HA) Greate for availability and durability AWS WAF – Web Application Firewall Protects your web applications fromm common web exploits (layer 7) Layer 7 is HTTTP (vs Layer 4 is TCP) Deploy on Application Load Balancer, API Gateway, CloudFront Define Web ACL (Web Access Control List): Rules can include: IP addresses, HTTP headers, HTTP body, or URI strings Protects from common attack - SQL injection and cross-site scripting (XSS) Size constraints, geo-match (block countries) Rate-based rules (to count occurrences of events) - for DDos protection AWS Firewall Manager Manage rules in all accounts of an AWS Organization Common set of security rules WAF rules (Application Load Balancer, API Gateways, CLoudFront) AWS Shield Advanced (ALB, CLB, Elastic IP, CloudFront) Securrity Groups for EC2 and ENI resources in VPC Sample Referennces Arcchitecture for DDos Protection Amazon GuardDuty Intelligent Threat discovery to Protect AWS Account Uses Machine Learning algorithhms, anomaly detection, 3rd party data One click to enable (30 days trials), no need to install software Input data includes: CloudTrail Events Logs - unusual API calls, unauthorized deployments CloudTrail Management Events - create VPC subnet, create trail,… CloudTrail S3 Data events - get object, list object, delete object, … VPC Flow logs - unusual internal traffic, unusual IP address DNS Logs - conpromised EC2 instaces sending encoded data within DNS queries Kubernetes Audit Logs - suspicious activities and potential EKS cluster compromises Can setup CloudWatch Event rules to be notified in case of findings CloudWatch events rules can target AWS Lambda or SNS Can protect against CryptoCurrency attacks (has a dedicated “finding” for it) Amazon Inspector Automated Security Assessments For EC2 instances Leveraging the AWS Systtem Manager (SSM) agent Analyze againstt unintended network accessibility Analyze the running OS against know vulnerabilities For Containers push to Amazon ECR Assessment of containers as they are pushed Reporting &amp; integration with AWS Security Hub Send finding to Amazon Event Bridge What does AWS Inspector evaluate? Remember: only for EC2 instances and container infrastructure Continuous scanning of the infrastructure, only when needed Package vulnerabilities (EC2 &amp; ECR) - database of CVE Network reachabilitty (EC2) A risk score is associated with all vulnnerabilities for prioritization Amazon Macie Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII) AWS Shared Responsibility Model AWS responsibility - security of the cloud protecting infrastructure (hardware, software, facibilities, and networking), that runs all the AWS services Managed service like S3, DynamoDB, RDS, etc Customer responsibilities - security in the cloud For EC2 instance, customerr is reponsible for managememnt of the guest OS (including security patches and updates), firewall &amp; network configuration, IAM Shared controls Patch Management, Configurationn Management, Awareness &amp; Training Example, for RDS AWS responsibility Manage the underlying EC2 instance, disable SSH access Automated DB patching Automated OS patching After the underlying instance and disks &amp; guarantee it functions Your responsibility Check the ports / IP / security group inbound rules in DB’s SG In-database user creation and permissions Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connectionns Database encryptionn setting Example, for S3 AWS responsibility Guarantee you get unnlimited storage Guarantee you get encryption Ensure separation of the data between different customers Ensure AWS employees can’t access your data Your responsibility Bucket configuration Bucket policy / public setting IAM user and roles Enabling encryption Example, for RDS AWS responsibility Manage the underlying EC2 instance, disable SSH access Automated DB patching Automated OS patching Audit the underlying instance anndn disk &amp; guaranntee it functionn Your responsibility Check the ports / IP / security group innbound rules in DB’s SG In-database user creation and permission Creating a database with or without public access Ensure parameter groups or DB is configured to allow SSL connections Database encryption setting AWS Shared Responsibility Model AWS responsibility - Security of the cloud Protect infrastructure (hardware, software, facibilities, and networking) that runs all the AWS services Managed services like S3, DynamoDB, RDS, etc. Customer responsibility - Security in the Cloud For Ec2 instance, customer is responsibe for management of the guest OS (including security patched and updates), firewall &amp; protect network configurationn, IAM Shared controls Patch Management, Configuration Management, Awareness &amp; Training " }, { "title": "AWS Question and Answer Practice", "url": "/posts/aws-question-and-answer-practice/", "categories": "Fullstack, Architect, AWS", "tags": "aws, FAQ", "date": "2024-02-06 00:00:00 +0700", "snippet": " What is ACM? The AWS certtificates manager is a service which allow the creation, managerment and renewable of certificates. It allow deployment certificcatte onto support AWS service su...", "content": " What is ACM? The AWS certtificates manager is a service which allow the creation, managerment and renewable of certificates. It allow deployment certificcatte onto support AWS service such as CloudFront and ALB. What is CloudFronnt Security OAI and CCustom origin OAI is features where virtual identities can be created, associatted with a CCloudFront Distributionn and deployed to edge locations.Access to ann S3 buckket cann be connttrolled by using these OAI’s allowinng access from an OAI, and usinnng an implicit DENY for everythinng else. They are generally used to esure o direct access tto S3 objecct is allowed when usinng preivate CF Distributions What is Lambda@edge Lambda@edge allows cloudfront to run lambda functionn at CCloudFront edge loccations to modify traffic between the viewer and edge locations and origins What is Global Accelerator? AWS Global Acccelerator is designed to improve global network performance by offerinng entry point on to thhe global AWS transit network as close to ccustomers as possible using Anycast IP addresses Which type of S3 encryption shows as AES256? SSE-S3 Which S3 Storage class is suitable for data which is easily replaced (choose the most cost effective)? S3 One Zone-IA Which Object class in S3 is ideal for uncertain access and low admin overhead? S3 inteligent tiering What is the cheapest S3 storage class for important data which need to be retained for long periods and is rarely accessed? S3 Glacier Which steps are required to allow an S3 bucket to operate as a website (choose all which apply) ? Upload Web file Set index and error documents Enable static web hosting Disable block public access setting Add a bucket policy What S3 feature allows objects storage classes to be changed and objects deleted automatically? S3 Lifecycle policy What is the default limit of the number of S3 buckets in an AWS account? 100 How large can an object in S3 be ? and what (if any) limits are there on the number of objects in a bucket? 5TB, no limit object bucket What S3 feature needs to be enabled to allow Cross-Region Replication (CRR)? Versioning What S3 feature can be used to grant external accounts access to an S3 bucket? Resource Policy Which type of encryption allows for role separation where an S3 Full Admin might not be able to decrypt objects? SSE-KMS Which type of encryption is where AWS perform encryption operations but DON’T hold any keys? SSE-C What type of encryption means AWS perform the encryption operations and handle key creation &amp; management? SSE-S3 What feature is required to allow CRR to function? Versioning What happens when an object is deleted in a bucket with versioning enabled? A delete maker is added What is Network access control list(ACL) ? NACL: is optional layer of security for your VPC that acts as a firewall for controlling traffic in and out on one or more subnet. You might setup network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.(stateless) What is Seccurity Group(stateful)? Are another seccurity feature of AWS VPC … only unlike NACL they are attached to AWS Resources, nnot VPC subnetsSG offer a few advagesvs NACLs in that they can recognize AWS resources and filter based on them, they can reference other SGs and also themselvesBut SG are not cappable of explicy traffic - often rquire assistance NACL " }, { "title": "AWS Monitoring, Audit, and Performance", "url": "/posts/aws-monitoring-audit-and-performance/", "categories": "Fullstack, Architect, AWS", "tags": "aws, cloud Watch, SSO, AWS Resource Access Manager, SCP, IAM, Could Trailt, EventBridge", "date": "2024-02-06 00:00:00 +0700", "snippet": "AWS Monitoring, Audit and PerformanceAWS CloudWatch Metrics Cloudwatch provides metrics for every services in AWS Metric is a variable to monitor (CPUUtilization, NetworkIn…) Metrics belong to n...", "content": "AWS Monitoring, Audit and PerformanceAWS CloudWatch Metrics Cloudwatch provides metrics for every services in AWS Metric is a variable to monitor (CPUUtilization, NetworkIn…) Metrics belong to namespaces Dimension is an attribute of a metric (instance id, environment, etc…) Up to 10 dimensions per metric Metrics have timestamps Can create CloudWatch dashboards of metrics EC2 Detailed monitoring EC2 instance metrics have metrics “every 5 minutes” With detailed monitoring (for a cost), you get data “every 1 minute” Use detailed monitoring if you want to scale faster for you ASG! The AWS tier allows us to have 10 detailed monitoring metrics Note: EC2 Memory usage is by default not pushed (must be pushed from inside the instance as a custom metric) CloudWatch Custom Metrics Possibility to define and send your own custom metrics to CloudWatch Example: memory (RAM) usage, disk space, number of logged in users … Use API call PutMetricData Ability to use dimensions (attributes) to segment metrics Instance.id Environment.name Metric resolution (StorageResolution API parameter - two possible value): Standard: 1 minute (60 seconds) High Resolution: 1/5/10/30 second(s) - Highr cost Important: Accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC@ instance time corectly) CloudWatch Dashboards Great way to setup custom dashboards for quick access to key metrics and alarms Dashboards are global Dashboards can include graphs from different AWS accounts and regions You can change the time zone &amp; time range of dashboards You can setup automatic refresh (10s, 1m, 2m, 5m, 15m) Dashboards can be shared with people who don’t have an AWS account (public, email address, 3rd party SSO provider through Amazon Cognito) Pricing 3 dashboards (up to 50metrics) for free $3/dashboard/month afterwards CloudWatch Logs Log groups: arbitrary name, usually representing an application Log stream: instances within application / log files / containers Can define log expiration policies (never expire, 30 days, etc..) CloudWatch Logs can send logs to: Amazon S3 (exports) Kinesis Data Streams Kinesis Data Firehose AWS Lambda ElasticSearch CloudWatch Logs - Sources SDK, CloudWatch Logs Agent, CloudWatch Unified Agent Elastic Beanstalk: collection of logs from application ECS: collection from containers AWS Lambda: collection from function logs VPC Flow Logs: VPC specific logs API Gateway CloudTrail based on filter Route53: Log DNS queries CloudWatch Logs Metric Filter &amp; Insights CloudWatch Logs can use filter expressions For example, find a specific IP inside of a log Or count occurrences of “ERROR” in your logs Metric filters can be used to triggr CloudWatch alarms Cloudwatch Logs Insights can be used to query logs and add queries to CloudWatch Dashboards CloudWatch Logs – S3 Export Log data can take up to 12 hours to become available for export The API call is CreateExportTask Not near-real time or real-time… use Logs Subscriptions instead CloudWatch Logs Subscriptions CloudWatch Logs Aggregation Multi-Account &amp; Multi Region CloudWatch Logs for EC2 By default, no logs from your EC2 machine will go to CloudWatch You need to run a CloudWatch agent on EC2 to push the log files you want Make sure IAM permissions are correct The CloudWatch log agent can be setup on premises too CloudWatch Logs Agent &amp; Unified Agent For virtual servers (EC2 instances, on-premises servers…) CloudWatch Logs Agent Old version of the agent Can only send to CloudWatch Logs CloudWatch Unified Agent Collect additional system-level metrics such as RAM, processes, etc… Collect logs to send to CloudWatch Logs Centralized configuration using SSM Parameter Store CloudWatch Unified Agent – Metrics Collected directly on your Linux server / EC2 instance CPU (active, guest, idle, system, user, steal) Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops) RAM (free, inactive, used, total, cached) Netstat (number of TCP and UDP connections, net packets, bytes) Processes (total, dead, bloqued, idle, running, sleep) Swap Space (free, used, used %) Reminder: out-of-the box metrics for EC2 – disk, CPU, network (high level) CloudWatch Alarms Alarms are used to trigger notifications for any metric Various options (sampling, %, max, min, etc…) Alarm States: OK INSUFFICIENT_DATA ALARM Period: Length of time in seconds to evaluate the metric High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec CloudWatch Alarm Targets Stop, Terminate, Reboot, or Recover an EC2 Instance Trigger Auto Scaling Action Send notification to SNS (from which you can do pretty much anything) EC2 Instance Recovery Status Check: Instance status = check the EC2 VM System status = check the underlying hardware Recovery: Same Private, Public, Elastic IP, metadata, placement group CloudWatch Events Event pattern: Interceot events from AWS services (Sources) Example sources: EC2 instance start, CodeBuild Failre, S3, Trusted Advisor Can intercept any API call with CloudTrai integration Schedule or Cron (example: create an event every 4 hours) A JSON payload is created from the event and passed to a target… Compute: Lambda, Batch, ECS task Integration: SQS, SNS, Kinesis Data Streams, Kinesis Data Firehose Orchestration: Step functions, CodePipeline, CodeBuild Maintenance: SSM, EC2 Actions Amazon EventBridge EventBridge is the next evolution of CloudWatch Events Default Event Bus - generated by AWS services (CloudWatch Events) Partner Event Bus: receive events from SaaS service or applications (Zendesk, DataDog, Segment, Auth0…) Custom Event Buses: for your own applications Event Buses can be accessed by other AWS accounts You can archive events (all/filter) sent to an event bus (indefinitely or set period) Ability to replay archived events Rules: how to process the events (like couldwatch events) Amazon EvntBridge - Schema Registry EventBridge can analyze the events in your bus and infer the schema The Schema Registry allows you to generate code for your application, that will know in advance how data is structured in the event bus Schema can be versioned Amazon EventBridge – Resource-based Policy Manage permissions for a specific Event Bus Example: allow/deny events from another AWS account or AWS region Use case: aggregate all events from your AWS Organization in a single AWS account or AWS region Amazon EventBridge vs CloudWatch Events Amazon EventBridge builds upon and extends CloudWatch Events. It uses the same service API and endpoint, and the same undrlying service infrastructure EventBridge allows extension to add event buses for your custom applications and your third-party SaaS apps. Event Bridge has the Schema registry capability EventBridge has a different name to mark the new capabilities Over time, the CloudWatch Events name will be replaced with EventBridge AWS CloudTrail Provides governance, compliance and audit for your AWS Account CloudTrail is enabled by default! Get an history of events / API calls made within your AWS Account by: Console SDK CLI AWS Services Can put logs from CloudTrail into Cloud A trail can be applied to All Regions (default) or a single Region If a resource is deleted in AWS, investigate CloudTrail first! CloudTrail Diagram CloudTrail Events Management Events: Operations that are performed on resources in your AWS account Examples: Configuring security (IAM AttachRolePolicy) Configuring rules for routing data (Amazon EC2 CreateSubnet) Setting up logging (AWS CloudTrail CreateTrail) By default, trails are configured to log management events. Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources) Data Events: By default, data events are not logged (because high volume operations) Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject ): can separate Read and Write Events AWS Lambda function execution activity (the Invoke API) CloudTrail Insights Events See next slide CloudTrail Insights Enable CloudTrail Insights to detect unusual activity in your account: inaccurate resource provisioning hitting service limits Bursts of AWS IAM actions Gaps in periodic maintenance activity CloudTrail Insights analyzes normal management events to create a baseline And then continuously analyzes write events to detect unusual patterns Anomalies appear in the CloudTrail console Event is sent to Amazon S3 An EventBridge event is generated (for automation needs) CloudTrail Events Retention Events are stored for 90 days in CloudTrail To keep events beyond this period, log them to S3 and use Athena AWS Config Helps with auditing and recording compliance of your AWS resources Helps record configurations and changes over time Questions that can be solved by AWS Config: Is there unrestricted SSH access to my security groups? Do my buckets have any public access? How has my ALB configuration changed over time? You can receive alerts (SNS notifications) for any changes AWS Config is a per-region service Can be aggregated across regions and accounts Possibility of storing the configuration data into S3 (analyzed by Athena) Config Rules Can use AWS managed config rules (over 75) Can make custom config rules (must be defined in AWS Lambda) Ex: evaluate if each EBS disk is of type gp2 Ex: evaluate if each EC2 instance is t2.micro Rules can be evaluated / triggered: For each config change And / or: at regular time intervals AWS Config Rules does not prevent actions from happening (no deny) Pricing: no free tier, $0.003 per configuration item recorded per region,$0.001 per config rule evaluation per region AWS Config Resource View compliance of a resource over time View configuration of a resource over time View CloudTrail API calls of a resource over time Config Rules – Remediations Automate remediation of non-compliant resources using SSM AutomationDocuments Use AWS-Managed Automation Documents or create custom AutomationDocuments Tip: you can create custom Automation Documents that invokes Lambda function You can set Remediation Retries if the resource is still non-compliant after auto-remediation Config Rules – Notifications Use EventBridge to trigger notifications when AWS resources are non-compliant Ability to send configuration changes and compliance state notificationsto SNS (all events – use SNS Filtering or filter at client-side) CloudWatch vs CloudTrail vs Config CloudWatch Performance monitoring (metrics, CPU, network, etc…) &amp; dashboards Events &amp; Alerting Log Aggregation &amp; Analysis CloudTrail Record API calls made within your Account by everyone Can define trails for specific resources Global Service Config Record configuration changes Evaluate resources against compliance rules Get timeline of changes and compliance For an Elastic Load Balancer CloudWatch: Monitoring Incoming connections metric Visualize error codes as % over time Make a dashboard to get an idea of your load balancer performance Config: Track security group rules for the Load Balancer Track configuration changes for the Load Balancer Ensure an SSL certificate is always assigned to the Load Balancer (compliance) CloudTrail: Track who made any changes to the Load Balancer with API calls AWS STS – Security Token Service Allows to grant limited and temporary access to AWS resources. Token is valid for up to one hour (must be refreshed) AssumeRole Within your own account: for enhanced security Cross Account Access: assume role in target account to perform actions there AssumeRoleWithSAML return credentials for users logged with SAML AssumeRoleWithWebIdentity return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible…) AWS recommends against using this, and using Cognito instead GetSessionToken for MFA, from a user or AWS account root user Using STS to Assume a Role Define an IAM Role within your account or cross-account Define which principals can access this IAM Role Use AWS STS (Security Token Service) to retrieve credentials and impersonate the IAM Role you have access to (AssumeRole API) Temporary credentials can be valid between 15 minutes to 1 hour Cross account access with STS Identity Federation in AWS Federation lets users outside of AWS to assume temporary role for accessing AWS resources. These users assume identity provided access role. Federations can have many flavors: SAML 2.0 Custom Identity Broker Web Identity Federation with Amazon Cognito Web Identity Federation without Amazon Cognito Single Sign On Non-SAML with AWS Microsoft AD Using federation, you don’t need to create IAM users (user management is outside of AWS) SAML 2.0 Federation To integrate Active Directory / ADFS with AWS (or any SAML 2.0) Provides access to AWS Console or CLI (through temporary creds) No need to create an IAM user for each of your employees SAML 2.0 Federation – Active Directory FS Same process as with any SAML 2.0 compatible IdP SAML 2.0 Federation Needs to setup a trust between AWS IAM and SAML (both ways) SAML 2.0 enables web-based, cross domain SSO Uses the STS API: AssumeRoleWithSAML Note federation through SAML is the “old way” of doing things Amazon Single Sign On (SSO) Federation is the new managed andsimpler way Read more here: https://aws.amazon.com/blogs/security/enabling-federation-to-aws-using-windows-active-directory-adfs-and-saml-2-0/ Custom Identity Broker Application Use only if identity provider is not compatible with SAML 2.0 The identity broker must determine the appropriate IAM policy Uses the STS API: AssumeRole or GetFederationToken Web Identity Federation – AssumeRoleWithWebIdentity Not recommended by AWS – use Cognito instead (allows for anonymous users, data synchronization, MFA) AWS Cognito Goal: Provide direct access to AWS Resources from the Client Side (mobile, web app) Example: provide (temporary) access to write to S3 bucket using Facebook Login Problem: We don’t want to create IAM users for our app users How: Log in to federated identity provider – or remain anonymous Get temporary AWS credentials back from the Federated Identity Pool These credentials come with a pre-defined IAM policy stating their permissions What is Microsoft Active Directory (AD)? Found on any Windows Server with AD Domain Services Database of objects: User Accounts, Computers, Printers, File Shares, Security Groups Centralized security management, create account, assign permissions Objects are organized in trees A group of trees is a forest AWS Directory Services AWS Managed Microsoft AD Create your own AD in AWS, manage users locally, supports MFA Establish “trust” connections with your on-premises AD AD Connector Directory Gateway (proxy) to redirect to on-premises AD, supports MFA Users are managed on the on-premises AD Simple AD AD-compatible managed directory on AWS Cannot be joined with on-premises AD AWS Organizations Global service Allows to manage multiple AWS accounts The main account is the master account – you can’t change it Other accounts are member accounts Member accounts can only be part of one organiztion Consolidated Billing across all accounts - single payment method Pricing benefits from aggregated usage (volume discount for EC2, S3…) API is available to automate AWS account creation Multi Account Strategies Create accounts per department, per cost center, per dev / test / prod, based on regulatory restrictions (using SCP), for better resource isolation (ex: VPC), to have separate per-account service limit, isolated account for logging Multi Account vs One Account Multi VPC Use tagging standards for billing purposes Enable CloudTrail on all accounts, send logs to central S3 account Send CloudWatch logs to central logging account Establish cross account roles for admin purposes Organizational Units (OU) - Examples AWS Organization Service Control Policies (SCP) Whitelist or blacklist IAM actions Applied at the OU or Account level Does not apply to the Master Account SCP is applied to all the Users and Roles of the Account, including Root user The SCP does not affect service-linked roles Service-linked roles enable other AWS services to integrate with AWS Organizations and can’t be restricted by SCPs. SCP must have an explicit Allow (does not allow anything by default) Use cases: Restrict access to certain services (for example: can’t use EMR) Enforce PCI compliance by explicitly disabling services SCP Hierarchy AWS Organization – Moving Accounts To migrate accounts from one organization to another Remove the member account from the old organization Send an invite to the new organiztion Accept the invite to the new organization from the member account If you want the master account of the old organization to also join the new organization, do the following: Remove the member accounts from the organizations using procedure above Delete the old organization Repeat the process above to invite the old master account to the new org IAM Conditions IAM for S3 ListBucket permission applies to arn:aws:s3:::test =&gt; bucket level permission GetObject, PutObject, DeleteObject applies to arn:awn:s3:::test/* =&gt; object level permission IAM Roles vs Resource Based Policies Attach a policy to a resource (example: S3 bucket policy) versus attaching of a using a role as a proxy IAM Roles vs Resource Based Policies When you assume a role (user, application or service), you give up youroriginal permissions and take the permissions assigned to the role When using a resource based policy, the principal doesn’t have to giveup his permissions Example: User in account A needs to scan a DynamoDB table in Account A and dump it in an S3 bucket in Account B Supported by: Amazon S3 buckets, SNS topics, SQS queues, etc… IAM Permission Boundaries IAM Permission Boundaries are supported for users and roles (not groups) Advanced feature to use a managed policy to set the maximum permissionsan IAM entity can get. IAM Permission Boundaries Can be used in combinations of AWS Organizations SCP Use cases Delegate responsibilities to non administrators within their permission Allow developers to self-assign policies and manage their own permissions, while making sure they can’t escalate their privileges (= make themselves admin) Useful to restrict one specific user (instead of a whole account using Organizations &amp; SCP) IAM Policy Evaluation Logic Example IAM Policy Can you perform sqs:CreateQueue? Can you perform sqs:DeleteQueue? Can you perform ec2:DescribeInstances? AWS Resource Access Manager (RAM) Share AWS resources that you own with other AWS accounts Share with any account or within your Organization Avoid resource duplication! VPC Subnets: allow to have all the resources launched in the same subnets must be from the same AWS Organizations. Cannot share security groups and default VPC Participants can manage their own resources in there Participants can’t view, modify, delete resources that belong to other participants or the owner AWS Transit Gateway Route53 Resolver Rules License Manager Configurations Resource Access Manager – VPC example Each account… is responsible for its own resources cannot view, modify or delete other resources in other accounts Network is shared so… Anything deployed in the VPC can talk to other resources in the VPC Applications are accessed easily across accounts, using private IP! Security groups from other accounts can be referenced for maximum security AWS Single Sign-On (SSO) Centrally manage Single Sign-On to access multiple accounts and 3rd -party business applications. Integrated with AWS Organizations Supports SAML 2.0 markup Integration with on-premises Active Directory Centralized permission management Centralized a auditing with CloudTrail AWS Single Sign-On (SSO) – Setup with AD SSO – vs AssumeRoleWithSAML " }, { "title": "AWS Machine Learning", "url": "/posts/aws-machine-learning/", "categories": "Fullstack, Architect, AWS", "tags": "aws, Rekognition, Transcribe, Polly, Translate, Lex& Connect, Comprehend, Sage Maker, Forecast, Kendra, Personalize, Textract", "date": "2024-02-06 00:00:00 +0700", "snippet": "Machine LearningAmazon Rekognition Find objects, people, text, scenes in images and videos using ML Facial analysis and facial search to do user verification, people counting Create a database o...", "content": "Machine LearningAmazon Rekognition Find objects, people, text, scenes in images and videos using ML Facial analysis and facial search to do user verification, people counting Create a database of “familiar faces” or compare against celebrities Use case: Labeling Content Moderation Text Detection Face Detection and Analysis (gender, age, range, emotions…) Face Search and Verification Celebrity Recognition Pathing (ex: for sports game analysis) Amazon Transcribe Automatic convert speech to text Uses a deep learning process called automatic speech recognition(ASR) to convert speech to text quickly and accurately Use cases: Transcribe customer service calls automate closed caption and subtitling generate metadata for media assets to create a fully searchable archive Amazon Polly Turn text into lifelike speech using deep learning Allowing you to create application that talk Amazon Translate Nature and accurate language translation Amazon Translate allows you to localize content - such as website and applications - for internation users, and to easily translate large volume of text efficiently Amazon Lex &amp; Connect Amazon Lex: (Same technology that powers Alexa) Automatic Speech Recognition (ASR) to convert speech to text Natural Language Understanding to recognize the intent of text, callers Helps build chatbots, call center bots Amazon Connect: Receive calls, create contact flows, cloud-based virtual contact center Can integrate with other CRM systems or AWS No upfront payment, 80% cheaper than traditional contact center solutions Amazon Comprehend For Nature language Processing - NLP Fully managed and serverless service Uses machine learning to find insight and relationship in text Language of the text Extracts key phrases, places, people, brands, or events Understanding how positive or negative the text is Analysis text using tokenization and parts of speech Automatically organizes a collection of text files by topic Sample use cases: analyze customer interaction (email) to find what leads to a positive or negative experience Create and groups articles by topics that Comprehend will uncover Amazon SageMaker Fully managed service for developers / data scientists to build ML models Typically, difficult to do all the processes in one place + provision servers Machine learning process (simplified): predicting your exam score Amazon Forecast Fully managed service that uses ML to deliver highly accurate forecasts Example: predict the future sales of a raincoat 50% more accurate than looking at the data itself Reduce forecasting time from months to hours Use cases: Product Demand Planning, Financial Planning, Resource Planning,… Amazon Kendra Fully managed document search service powered by Machine Learning Extract answer from within a document (text, pdf, HTML, MS Word, …) Natural language search capabilities Learn from user interactions/feedback tto promote preferred results (Incremental learning) Ability to manually fine-tune search results (importance of data, freshness, custom, …) Amazon Personalize Fully managed ML-service to build apps with real-time personalized recommendations Example: personalized product recommendations/re-ranking, customized direct marketing Example: User bought gardening tools, provide recommendations on the next one tto buy Same technology used by Amazon.com Integrates into existing website, applications, SMS, email marketing systems,… Implement in days, ot months (you don’t need to build, train, and deploy ML solutions) Use cases retail stores, media, and entertainment… Amazon Textract Automatically extract text, handwriting, and data from ay scanned document using AI ad ML Extract data from forms and tables Read and process any type of documents (PDFs, images, …) Use cases: Financial Service (e.g invoice, financial reports) Healthcare (e.g medical records, insurance claims) Public Sector (e.g tax forms, ID documents, passports) AWS Machine Learning - Summary Rekognition: face detection, labeling, celebrity recognition Transcribe: audio to text Polly: text to audio Translate: translation Lex: build conversation bots - chatbots Connect: cloud contact center Comprehend: natural language processing SageMaker: machine learning for every developer and data scientist Forecast: build highly accurate forecast Kendra: ML-powered search engine Personalize: real-time personalized recommendation Textract: detect text and data in documents" }, { "title": "AWS Integration & Messaginng", "url": "/posts/aws-integration-messaginng/", "categories": "Fullstack, Blogging, AWS", "tags": "sqs, SQS, Queue, Long Polling, decouple", "date": "2024-02-06 00:00:00 +0700", "snippet": "AWS Integration &amp; MessaginngSQS, SNS &amp; Kinesis When we start deploying multiple applications, they will inevitably need to communicate with one another There are two patterns of applicati...", "content": "AWS Integration &amp; MessaginngSQS, SNS &amp; Kinesis When we start deploying multiple applications, they will inevitably need to communicate with one another There are two patterns of application communication Synchromous communication (app to app) Asynchronous / Evenbased (app to queue to app) Introdution Synchronnous between applications can be problematic if there are sudden spikes of traffic What if you need to suddenly encode 1000 videos but usually it’s 10? in that case, it better to decouple your applications, SQS: queue model SNS: pub/sub model Kinesis: real-time streaming model These services can scale independently from our application! Amazon SQS, What’s a queue? SQS - Standard queue Oldest offering (over 10 years old) Fully managed service, used to decouple applications Attributes: unlimited throughput, unlimited number of messages in queue Default retention of message: 4 days, maxximum of 14 days Low latenccy (&lt; 10ms on publish and receive) Limitation of 256kb per message sent Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering) SQS - product messages Produced to SQS using the SDK (SendMessage API) The message is persited in SQS until consumer deletes it Message retention: default 4 days, upto 14days Example: order id customer id any attributes you want SQS standard: unlimited throughput SQS - Consuming messages Consumes (running on EC2 instances, Servers, or AWS lambda)… Poll SQS for messages (receive up to 10 messages at a time) Process the messages (example: insert the message inyo an RDS database) Delete the messages using thhe deleteMessage API SQS - Mutiple EC2 Innstances Consumers Consumers receive and process messages in parallel At least once delivery Best effort message ordering Consumers delete messages after processig them We can scale consumers horzontally to improve throughput of processing SQS with auto scaling group (ASG) SQS to decouple between appliccation tiers Amazon SQS - Security Encryption In-flight encryption using HTTPS API At rest encryption using KMS keys Client side encrytion if the client wants to perform encryptioon/decrytion itself Access Controls: IAM policies too regulate access to SQS API SQS Access Policies (similiar to S3 bucket policies) Useful for cross account access to SQS queues Useful for allowinng other services (SNS, S3…) to write to an SQS queue SQS Queue Access Policy Cross Acccount Access Publish S3 Event Notifications to SQS Queue SQS Message Visiblity Timeout After a message is polled by a consumer, it becomes invisible to other consumers By default, the “message visibility timeout” is 30 seconds That means the message has 30 seconnds to be processed After the message visibility timeout is over, the message is “visible” in SQS If a message is not processed within the visibility timeout, it will be processed twice A consumer could call the ChangelMessageVisibility API tto get more time If visibility timeout is high (hours), and consumer crashes, re-processing will take ttime If visiblity ttimeouut is too low (seconds), we may get duplicates Amazon SQS - Dead Letter Queue If a consumer fails to process a message within the visibility timeout… the message goes back to the queue! We can set a threshold of how manny times a message can go back to the queue After tthe mmaximumreceivves tthreshold is exceeded, the message goes intto a dead lettter queue (DLQ) Userful for debugginng! Make sure to process the messages in the DLQ before they expire: Good to sett a rettenttion oof 14days in the DLQ SQS DLQ - Redrive to Source Feature to help consume messages in the DLQ to understand what is wrong with them. When our code is fixed, we can redrive the messages from the DLQ backk into the source queue (or any otther queue) inn batches without writing custtom code Amazon SQS - Delay Queue Delay a message (consummers don’t see it immediately) up to 15 minuttes Defaultt is 0 secconnds (message is available rightt away) Can set a defaultt at queue level Can override tthe defaultt o sennd using the DelaySecccond parameters Amazon SQS - Long Polling When a consumer requests messages from the queue, it can optioally “wait” for messages to arrive if tthere are nonne in the queue This is called long pollinng LongPolling dereases the nuumber of API calls made tto SQS while increasign tthe efficiency and reduinng lattenccy of your applicationn The wait time can be between 1 se tto 20 sec (20 sec preferable) Lonng polling is preferable to short polling long polling level using waittimeseconnds" }, { "title": "AWS DNS", "url": "/posts/aws-dns/", "categories": "Fullstack, Architect, AWS", "tags": "aws, dns, route53, aws zone", "date": "2024-02-06 00:00:00 +0700", "snippet": "AWS DNS DNS is a discovery service Translate machine into human and vice versa wwww.amazon.com =&gt; 104.98.34.131 it’s huge and has to be distributed 4294967296 IPv4 DNS Zone Webserver ...", "content": "AWS DNS DNS is a discovery service Translate machine into human and vice versa wwww.amazon.com =&gt; 104.98.34.131 it’s huge and has to be distributed 4294967296 IPv4 DNS Zone Webserver resolve DNS client -&gt; your laptop Resolver =&gt; software on your device, or a server which queries DNS on your behalf Zone =&gt; A part of the DNS database Zonefile =&gt; physical database for a zone Nameserver =&gt; where zonfiles are hosted DNS ROOT &amp; ROOT ZONE DNS Root IANA: The global coordination of the DNS Root, IP addressing, and other Internet protocol resources is performed as the Internet Assigned Numbers Authority (IANA) functions. Root Zone Database: The Root Zone Database represents the delegation details of top-level domains, including gTLDs such as .com, and country-code TLDs such as .uk. As the manager of the DNS root zone, we are responsible for coordinating these delegations in accordance with our policies and procedures. root zone , root IANA DNS resolution : Walking the tree Root Hints =&gt; config points at the root servers IPs and addresses Root Server =&gt; hosts the DNS root zone Root Zone =&gt; points at TLD authoritative servers gTLD =&gt; generic top level domain (.com .org) ccTLD =&gt; country code top level domain (.uk .eu) Route53 Register Domain Host Zones.. managed nameservers Global servicce … single database Globally resilient create zonefile also allocated service Hosted on four managed name servers can be public or private .. linked to VPC(s) …stores records (recordsets)" }, { "title": "AWS Database: RDS, Aurora, ElastiCache, DynamoDB, S3, Athena, Redshift, AWS Glue, AWS Neptune, ElasticSearch", "url": "/posts/aws-database-rds-aurora-elasticache-dynamodb-s3-athena-redshift-aws-glue-aws-neptune-elasticsearch/", "categories": "Fullstack, Architect, AWS", "tags": "aws, athena, RedShift, dynamodb, RDS, glue, neptune, Redshift, s3, ElastiCache, ElasticSearch, Database", "date": "2024-02-06 00:00:00 +0700", "snippet": "DatabasesChoosing the Right Database Read-heavy, write heavy, or database workload? Throughput needs? Will it change, does it need to scale or fluctuate during the day? How much data to store and...", "content": "DatabasesChoosing the Right Database Read-heavy, write heavy, or database workload? Throughput needs? Will it change, does it need to scale or fluctuate during the day? How much data to store and for how long? Will it grow? average object size? How are they accessed? Data durability? Source of truth for the data ? Latency requirements? Concurent users? Data model? How will you query the data? Joins? Structured? Semi-Structured? Strong schema? More flexibility? Reporting? Search? RDBMS? NoSQL? License costs? Switch to Cloud Native DB such as Aurora? Database Types RDBMS (= SQL / OLTP): RDS, Aurora – great for joins NoSQL database: DynamoDB (~JSON), ElastiCache (key / value pairs), Neptune (graphs) – no joins, no SQL Object Store: S3 (for big objects) / Glacier (for backups / archives) Data Warehouse (= SQL Analytics / BI): Redshift (OLAP), Athena Search: ElasticSearch (JSON) – free text, unstructured searches Graphs: Neptune – displays relationships between data RDS Overview Managed PostgreSQL / MySQL / Oracle / SQL Server Must provision an EC2 instance &amp; EBS volume type and size Support for Read Replicas and Multi AZ Security through IAM, Security Groups, KMS, SSL in transit Backup / Snapshot / Point in time restore feature Managed and Scheduled maintenance Monitoring through CloudWatch Use case: Store relational datasets (RDBMS / OLTP), perform SQL queries, transactional inserts / update / delete is available RDS for solutions architect Operations: small downtime when failover happens, when maintenance happens, scaling in read replicas / ec2 instance / restore EBS implies manual intervention, application changes Security: AWS responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, using SSL Reliability: Multi AZ feature, failover in case of failures performance: depends on EC2 instance type, EBS volume type, ability to add Read Replicas. Storage auto-scaling &amp; manual scaling of instances Cost: Pay per hour based on provisioned EC2 and EBS Aurora Overview Compatible API for PostgreSQL / MySQL Data is held in 6 replicas, across 3 AZ Auto healing capability Multi AZ, Auto Scaling Read Replicas Read Replicas can be Global Aurora databae can be global for DR or latency purpose AUto scaling of storage from 10GB to 128 TB Define EC2 instance type for aurora instances Same security / monitoring / maintenance features as RDS Aurora Serveless - for unpredictable / intermittent workloads Aurora Multi-Master - for continous writes failover use case: same as RDS, but with less maintenance / more flexibility / more performance Aurora for Solutions Architect Operations: less operations, auto scaling storage Security: AWS responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, using SSL Reliability: Multi AZ, high available, possibly more than RDS, Aurora Serverless option, Aurora Multi-Master option Performance: 5x performance (according to AWS) due to architectural optimations. Upto 15 read replica (only 5 for RDS) Cost: Pay per hour based on EC2 and storage usage. Possibly lower costs compared to Enterprise grade databases such as Oracle ElastiCache Overview Managed Redis / Memcached (similar offering as RDS, but for caches) In-memory data store, sub-milisecond latency Must provision an EC@ instance type Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding) Security through IAM, Security Groups, KMS, Redis Auth Backup / Snapshot / Point in time restore feature Managed and Scheduled maintenance Monitoring through CloudWatch Use case: Key/Value store, frequent reads, less writes, cache results for DB queries, store session data for websits, cannot use SQL ElastiCache for Solutions Architect Operation: Same as RDS Security: AWS responsible for OS security, we are responsible for setting up KMS&lt; security groups, IAM policies, users (Redis Auth), using SSL Reliability: Clustering, Multi AZ Performance: Sub-milisecond performance, in memory, read replicas for sharding, very polular cache option Cost: Pay per hour based on EC2 and storage usage DynamoDB Overview AWS proprietary technology, managed NOSQL database Serverless, provisioned capacity, auto scaling, on demand capacity (Nov 2018) Can replace ElastiCache as a key/value store (strong sessiong data for example) Highly Available, Multi AZ by default, Read and Writes are decoupled, AX for read cache Reads can be eventually consistent or strongly consistent Security, authentication and authorization is done through IAM DynamoDB streams to integrate with AWS Lambda Backup / Restore feature, GlobalTable feature Monitoring through CloudWatch Can only query on primary key, sort key or indexes use Case: Servrless applications development (small documents 100s KB), distributed serverless cache, doesn’t have SQL query language available, has transactions capability from Nov 2018 DynamoDB for Solutions Architect Operations: no operations needed, auto scaling capability, serverless Security: full security through IAM policies Reliabilty: Multi AZ, Backups Performance: single digit milisecond performance, DAX for caching reads, performance doesn’t degrade if your application scales Cost: Pay per provisioned capacity and storage usage (no need to guess in advance any capacity - can use auto scaling) S3 Overview S3 is a… key / value store for objects Great for big objects, not so great for small objects Serverless, scales infinitely, max object size is 5TB Strong consistency Tiers: S3 Standard, S3 IA, S3 One Zone IA, Glacier for backups Features: Versioning, Encryption, Cross Region Replication, etc… Security: IAM, Bucket Policies, ACL Encryption: SSE-S3, SSE-KMS, SSE-C, client side encryption, SSL in transit Use Case: static files, key value store for big iles, website hosting S3 for Solutions Architect Operations: no operations needed Security: IAM bucket Policies, ACL, Encryption (Server/Client), SSL Reliability: 99.99999999% durability / 99.99% availability, Multi AZ, CRR Performance: scales to thousands of read / writes per second, transfer acceleration / multi-part for big files Cost: pay per storage usage, network cost, requests number Athena Overview Fully Serverless database with SQL capabilities Used to query data in S3 Pay per query Output results back to S3 Secured through IAM use Case; one time SQL queries, serverleess queries on S3, log analytics Athena for Solutions Architect Operations: no operations needed, serverless Security: IAM + S3 security Reliability: managed service, uses Presto engine, highly available Performance: queries scale based on data size Cost: pay per query / per TB of data scanned, serverless Redshift Overview Redshift is based on PostgreSQL, but it’s not used for OLTP It’s OLAP - online analytical processing (analytics and data warehousing) 10x better performance than other data warehouses, scale to PBs of data Columnar storage of data (instead of row based) Massively Parallel Query Execution (MPP) Pay as you go based on the instances provisioned Has a SQL interface for performing the queries BL tools such as AWS Quicksight or Tableau integrate with it Redshift Continued… Data is loaded from S3, DynamoDB, DMS, other DBs… From 1 node to 128 nodes, up to 28TB of space per node Leader node: for query planning, results aggreegation Compute node: for performing the queries, send results to leader Redshift Spectrum: perform queries directly againt S3 (no need to load) Backup &amp; Restore, Security VPC / IAM / KMS, Monitoring Redshift Enhanced VPC routing: COPY / UNLOAD goes through VPC Redshift – Snapshots &amp; DR Redshift has no “Multi-AZ” mode Snapshots are point-in-time backups of a cluster stored internally in S3 Snapshots are incremental (only what has changed is saved) You can restore a snapshot into a new cluster Automated: every 8 hours, every 5 GB, or on a schedule. Set retention Manual: snapshot is reetained until you delete it You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region Loading data into Redshift Amazon Kinesis Data Firehose S3 using COPY command EC2 Instance JDBC driver copy customerfrom 's3://mybucket/mydata’iam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole'; Redshift Spectrum Query data that is already in S3 wiithout loading it Must have aRedshift cluster available to start the query The query is then submitted to thousands of redshift spectrum nodes Redshift for Solutions Architect Operations: like RDS Security: IAM, VPC, KMS, SSL (like RDS) Reliability: auto healing features, cross-region snapshot copy Performance: 10x performance vs other data warehousing, compression Cost: pay per node provisioned, 1/10 th of the cost vs other warehouses vs Athena: faster queries / joins / aggregations thanks to indexes Remember: Redshift = Analytics / BI / Data Warehouse AWS Glue Managed extract, transform, and load (ETL) service Useful to prepare and transform data for analytics Fully serverless service Glue Data Catalog Glue Data Catalog: catalog of datasets Neptune Fully managed graph database When do we use Graphs? High relationship data Social Networking: Users friends with Users, replied to comment on post of user and likes other comments. Knowledge graphs (Wikipedia) Highly available across 3 AZ, with up to 15 read replicas Point-in-time recovery, continuous backup to Amazon S3 Support for KMS encryption at rest + HTTPS Neptune for Solutions Architect Operations: similar to RDS Security: IAM, VPC, KMS, SSL (similar to RDS) + IAM Authentication Reliability: Multi-AZ, clustering Performance: best suited for graphs, clustering to improve performance Cost: pay per node provisioned (similar to RDS) Remember: Neptune = Graphs ElasticSearch Example: In DynamoDB, you can only find by primary key or indexes. With ElasticSearch, you can search any field, even partially matches It’s common to use ElasticSearch as a complement to another database ElasticSearch also has some usage for Big Data applicationss3 You can provision a cluster of instances Built-in integrations: Amazon Kinesis Data Firehose, AWS IoT, and Amazon CloudWatch Logs for data ingestion Security through Cognito &amp; IAM, KMS encryption, SSL &amp; VPC Comes with Kibana (visualization) &amp; Logstash (log ingestion) – ELK stack ElasticSearch for Solutions Architect Operations: similar to RDS Security: Cognito, IAM, VPC, KMS, SSL Reliability: Multi-AZ, clustering Performance: based on ElasticSearch project (open source), petabyte scale Cost: pay per node provisioned (similar to RDS) Remember: ElasticSearch = Search / Indexing" }, { "title": "AWS Container: Docker, ECS and EKS", "url": "/posts/aws-container-docker-ecs-and-eks/", "categories": "Fullstack, Architect, Blogging", "tags": "aws, ECS, EKS, Docker", "date": "2024-02-06 00:00:00 +0700", "snippet": "Container Section Docker is a software development platform to deploy apps Apps are packaged in containers that can be run on any OS Apps run the same, regardless of where they’re run A...", "content": "Container Section Docker is a software development platform to deploy apps Apps are packaged in containers that can be run on any OS Apps run the same, regardless of where they’re run Any machine No compatibility issues Predictable behavior Less work Easier to maintain and deploy Works with any language, any OS, any technology Use cases: microservices architecture, lift and shift apps from on-premises to the AWS cloud,… Where are Docker images stored? Docker images are stored in Docker Repositories Docker Hub (https://hub.docker.com) public repossitory Find base images for many technologies or OS (e.g., Ubuntu, MySQL, … Amazon ECR (Amazon Elastic Container Registry) Private repository Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws) Docker vs. Virtual Machines Docker is ”sort of ” a virtualization technology, but not exactly Resources are shared with the host =&gt; many containers on one server Getting Started with Docker Docker Containers Management on AWS Amazon Elastic Container Service (Amazon ECS) Amazon’s own container platform Amazon Elastic Kubernetes Service (Amazon EKS) Amazon’s managed Kubernetes (open source) AWS Fargate Amazon’s own Serverless container platform Works with ECS and with EKS Amazon ECR Store container images Amazon ECS - EC2 Launch Type ECS = Elastic Container Service Launch Docker containers on AWS = Launch ECS Tasks on ECS Clusters EC2 Launch Type: you must provision &amp; maintain the infrastructure (the EC2 instances) Each Ec2 Instance must run the ECS agent to register in the ECS Cluster AWS takes care of starting / stoping containers Amazon ECS – Fargate Launch Type Launch Docker containers on AWS You do not provision the infrastructure (no EC2 instances to manage) It’s all Serverless! You just create task definitions AWS just runs ECS Tasks for you based on the CPU / RAM you need To scale, just increase the number of tasks. Simple - no more EC2 instances Amazon ECS – IAM Roles for ECS EC2 Instance Profile (EC2 Launch Type only): Used by the ECS agent Makes API calls to ECS service Send container logs to CloudWatch Logs Pull Dockerimage from ECR Rederence sensitive data in secrets manager or SSM Parameter Store ECS Task Role Allows each task to have a specific role Use different roles for the different ECS Services you run Task Role is defined in the task definition Amazon ECS – Load Balancer Integrations Application Load Balancer supported and works for most use cases Network Load Balancer recommended only for high throughput / high performance use cases, or to pair it with AWS Private Link Elastic Load Balancer supported but not recommended (no advanced features – no Fargate) Amazon ECS – Data Volumes (EFS) Mount EFS file systems onto ECS tasks Works for both EC2 and Fargate launch types Tasks running in any AZ will share the same datain the EFS file system Fargate + EFS = Serverless Use cases: persistent multi-AZ shared storage for your containers Note: FSx For Lustre is not supported Amazon S3 cannot be mounted as a file system ECS Service Auto Scaling Automatically increase/decrease the desired number of ECS tasks Amazon ECS Auto Scaling uses AWS Application Auto Scaling ECS Service Average CPU Utilization ECS Service Average Memory Utilization - Scale on RAM ALB Request Count Per Target – metric coming from the ALB Target Tracking – scale based on target value for a specific CloudWatch metric Step Scaling – scale based on a specified CloudWatch Alarm Scheduled Scaling – scale based on a specified date/time (predictable changes) ECS Service Auto Scaling (task level) ≠ EC2 Auto Scaling (EC2 instance level) Fargate Auto Scaling is much easier to setup (because Serverless) EC2 Launch Type – Auto Scaling EC2 Instances Accommodate ECS Service Scaling by adding underlying EC2 Instances Auto Scaling Group Scaling Scale your ASG based on CPU Utilization Add EC2 instances over time ECS Cluster Capacity Provider Used to automatically provision and scale the infrastructure for your ECS Tasks Capacity Provider paired with an Auto Scaling Group Add EC2 Instances when you’re missing capacity (CPU, RAM…) ECS Scaling – Service CPU Usage Example ECS Rolling Updates When updating from v1 to v2, we can control how many tasks can be started and stopped, and in which order ECS Rolling Update – Min 50%, Max 100% ECS Rolling Update – Min 100%, Max 150% ECS tasks invoked by Event Bridge ECS tasks invoked by Event Bridge Schedule ECS – SQS Queue Example Amazon ECR ECR = Elastic Container Registry Store and manage Docker images on AWS Private and Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws) Fully integrated with ECS, backed by Amazon S3 Access is controlled through IAM (permission errors =&gt; policy) Supports image vulnerability scanning, versioning, image tags, image lifecycle, … Amazon EKS Overview Amazon EKS = Amazon Elastic Kubernetes Service It is a way to launch managed Kubernetes clusters on AWS Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) application It’s an alternative to ECS, similar goal but different API EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers Use case: if your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP…) For multiple regions, deploy one EKS cluster per region Amazon EKS - Diagram " }, { "title": "Serverless Overview", "url": "/posts/serverless-overview/", "categories": "Fullstack, Architect, AWS", "tags": "lambda, dynamodb, api, APIGateway, Cognito, SAM, CodeDeploy", "date": "2024-02-05 00:00:00 +0700", "snippet": "Serverless Overview Serverless is a new paradigm in which the developers don’t have to manage servers anymore… They just deploy code They just deploy… functions ! Initially… Serverless == FaaS ...", "content": "Serverless Overview Serverless is a new paradigm in which the developers don’t have to manage servers anymore… They just deploy code They just deploy… functions ! Initially… Serverless == FaaS (Function as a Service) Serverless was pioneered by AWS Lambda but now also includes anything that’s managed: “databases, messaging, storage, etc.” Serverless does not mean there are no servers… it means you just don’t manage / provision / see them Serverless in AWS AWS Lambda, DynamoDB, AWS Cognito, AWS API Gateway, Amazon S3, AWS SNS &amp; SQS, AWS Kinesis Data Firehose, Aurora Serverless, Step Functions, Fargate Why AWS Lambda EC2 Virtual Servers in the Cloud Limited by RAM and CPU Continuously running Scaling means intervention to add / remove servers Amazon Lambda Virtual functions – no servers to manage! Limited by time - short executions Run on-demand Scaling is automated! Benefits of AWS Lambda Easy Pricing Pay per request and compute time Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time Integrated with the whole AWS suite of services Integrated with many programming languages Easy monitoring through AWS CloudWatch Easy to get more resources per functions (up to 10GB of RAM!) Increasing RAM will also improve CPU and network! AWS Lambda language support Node.js, python, java, C#, Golang, C# Powershell, Ruby, Custom Runtim API (community supported, example Rust) Lambda Container Image The container image must implement the Lambda Runtime API ECS / Fargate is preferred for running arbitrary Docker images AWS Lambda Integrations Main ones AWS Lambda Pricing: example You can find overall pricing information here: Pay per calls: First 1,000,000 requests are free $0.20 per 1 million requests thereafter ($0.0000002 per request) Pay per duration: (in increment of 1 ms) 400,000 GB-seconds of compute time per month for FREE == 400,000 seconds if function is 1GB RAM == 3,200,000 seconds if function is 128 MB RAM After that $1.00 for 600,000 GB-seconds It is usually very cheap to run AWS Lambda so it’s very popular AWS Lambda Limits to Know - per region Execution: Memory allocation: 128 MB – 10GB (1 MB increments) Maximum execution time: 900 seconds (15 minutes) Environment variables (4 KB) Disk capacity in the “function container” (in /tmp): 512 MB Concurrency executions: 1000 (can be increased) Deployment: Lambda function deployment size (compressed .zip): 50 MB Size of uncompressed deployment (code + dependencies): 250 MB Can use the /tmp directory to load other files at startup Size of environment variables: 4 KB Lambda@Edge You have deployed a CDN using CloudFront What if you wanted to run a global AWS Lambda alongside? Or how to implement request filtering before reaching your application? For this, you can use Lambda@Edge: deploy Lambda functions alongside your CloudFront CDN Build more responsive applications You don’t manage servers, Lambda is deployed globally Customize the CDN content Pay only for what you use Lambda@Edge You can use Lambda to change CloudFront requests and responses: After CloudFront receives a request from a viewer (viewer request) Before CloudFront forwards the request to the origin (origin request) After CloudFront receives the response from the origin (origin response) Before CloudFront forwards the response to the viewer (viewer response) You can also generate responses to viewers without ever sending the request to the origin Lambda@Edge: Global application Lambda@Edge: Use Cases Web security and privacy Dynamic Web application at the edge Search Engine Optimization (SEO) Intelligently Route Across Origin and Data Centers Bot Mitigation at the Edge Realtime image transformation A/B Testing User Authentication and Authorization User Prioritization User Tracking andAnalytics Amazon DynamoDB Fully managed, highly available with replication across multiple AZs NoSQL database - not a relational database Scales to massive workloads, distributed database Millions of requests per seconds, trillions of row, 100s of TB storage Fast an consistent in performance (low latency on retriieval) Integrated with IAM for security, authorization and administration Enables event driven programming with DynamoDB Streams Low cost and auto-scaling capabilities Standard &amp; Infrequent Access (IA) Table Class DynamoDB - Basics DynamoDB is made of Tables Each table has a Primary Key (must be decided at creation time) Each table can have an infinite number of items (= rows) Each item has attributes (can be added over time – can be null) Maximum size of an item is 400KB Data types supported are: Scalar Types – String, Number, Binary, Boolean, Null Document Types – List, Map Set Types – String Set, Number Set, Binary Set DynamoDB – Table example DynamoDB – Read/Write Capacity Modes Control how you manage your table’s capacity (read/write throughput) Provisioned Mode (default) you specify the number of reads/writes per second You need to plan capacity beforehand Pay for provisioned Read Capacity Units (RCU) &amp; Write Capacity Units (WCU) Possibility to add auto-scaling mode for RCU &amp; WCU On-Demand Mode Read/writes automatically scale up/down with your workloads No capacity planning needed Pay for what you use, more expensive ($$$) Great for unpredictable workloads DynamoDB Accelerator (DAX) Fully-managed, highly available, seamless in-memory cache for DynamoDB Help solve read congestion by caching Microseconds latency for cached data Doesn’t require application logic modification (compatible with existing DynamoDB APIs) 5 minutes TTL for cache (default) DynamoDB Accelerator (DAX) vs. ElastiCache DynamoDB Streams Ordered stream of item-level modifications (create/update/delete) in a table Stream records can be: Sent to Kinesis Data Streams Read by AWS Lambda Read by Kinesis Client Library applications Data Retention for up to 24 hours Use cases: react to changes in real-time (welcome email to users) Analytics Insert into derivative tables Insert into ElasticSearch Implement cross-region replication DynamoDB Streams DynamoDB Global Tables Make a DynamoDB table accessible with low latency in multiple-regions Active-Active replication Applications can READ and WRITE to the table in any region Must enable DynamoDB Streams as a pre-requisite DynamoDB – Time To Live (TTL) Automatically delete items after an expiry timestamp Use cases: reduce stored data by keeping only current items, adhere to regulatory obligations, … DynamoDB - Indexes Global Secondary Indexes (GSI) &amp; Local Secondary Indexes (LSI) High level: allow to query on attributes other than the Primary Key With Indexes, we can query by Game ID, Game_TS, Score, Result, etc… DynamoDB - Transactions Example: Building a Serverless API AWS API Gateway AWS Lambda + API Gateway: No infrastructure to manage Support for the WebSocket Protocol Handle API versioning (v1, v2…) Handle different environments (dev, test, prod…) Handle security (Authentication and Authorization) Create API keys, handle request throttling Swagger / Open API import to quickly define APIs Transform and validate requests and responses Generate SDK and API specifications Cache API responses API Gateway – Integrations High Level Lambda Function Invoke lambda function Easy way to expose REST API backed by AWS Lambda HTTP Expose HTTP endpoints in the backend Example: internal HTTP API on premise, Application Load Balancer… Why? Add rate limiting, caching, user authentications, API keys, etc… AWS Service Expose any AWS API through the API Gateway? Example: start an AWS Step Function workflow, post a message to SQS Why? Add authentication, deploy publicly, rate control… API Gateway - Endpoint Types Edge-Optimized (default): For global clients Requests are routed through the CloudFront Edge locations (improves latency) The API Gateway still lives in only one region Regional: For clients within the same region Could manually combine with CloudFront (more control over the caching strategies and the distribution) Private: Can only be accessed from your VPC using an interface VPC endpoint (ENI) Use a resource policy to define access API Gateway – Security IAM Permissions Create an IAM policy authorization and attach to User / Role API Gateway verifies IAM permissions passed by the calling application Good to provide access within your own infrastructure Leverages “Sig v4” capability where IAM credential are in headers Lambda Authorizer (formerly Custom Authorizers) Uses AWS Lambda to validate the token in header being passed Option to cache result of authentication Helps to use OAuth / SAML / 3 rd party type of authentication Lambda must return an IAM policy for the user Cognito User Pools Cognito fully manages user lifecycle API gateway verifes identity automatically from AWS Cognito No custom inplementation required Cognito only helps with authentication, not authorization API Gateway – Security – Summary IAM: Great for users / roles already within your AWS account Handle authentication + authorization Leverages Sig v4 Custom Authorizer Great for 3 rd party tokens Very flexible in terms of what IAM policy is returned Handle Authentication + Authorization Pay per Lambda invocation Pay per Lambda invocation You manage your own user pool (can be backed by Facebook, Google login etc…) No need to write any custom code Must implement authorization in the backend AWS Cognito We want to give our users an identity so that they can interact with ourapplication. Cognito User Pools: Sign in functionality for app users Integrate with API Gateway Cognito Identity Pools(Federate Identity) Provide AWS credentials to users so they can access AWS resources directly Integrate with Cognito User Pools as an identity provider Cognito Sync Synchronize data from device to Cognito. May be deprecated and replaced by AppSync AWS Cognito User Pools (CUP) Create a serverless database of user for your mobile apps Simple login: Username (or email) / password combination Possibility to verify emails / phone numbers and add MFA Can enable Federated Identities (Facebook, Google, SAML…) Sends back a JSON Web Tokens (JWT) Can be integrated with API Gateway for authentication AWS Cognito – Federated Identity Pools Goal Provide direct access to AWS Resourcesfrom the Client Side How: Log in to federated identity provider – orremain anonymous Get temporary AWS credentials back fromthe Federated Identity Pool These credentials come with a pre-defined IAM policy stating their permissions Example: provide (temporary) access to write to S3bucket using Facebook Login AWS Cognito Sync Deprecate - use AWS AppSync now Store preferences, configuration, state of app Cross device synchronization (any platform – iOS, Android, etc…) Offline capability (synchronization when back online) Requires Federated Identity Pool in Cognito (not User Pool) Store data in datasets (up to 1MB) Up to 20 datasets to synchronise AWS SAM - Serverless Application Model SAM = Serverless Application Model Framework for developing and deploying serverless applications All the configuration is YAML code: Lambda Functions DynamoDB tables API Gateway Cognito User Pools SAM can help you to run Lambda, API Gateway, DynamoDB locally SAM can use CodeDeploy to deploy Lambda functions" }, { "title": "Serverless Architectures Examples", "url": "/posts/serverless-architectures-examples/", "categories": "Fullstack, Architect, AWS", "tags": "serverless, bigdata, pipeline, Micro Services, kinesis, firehorse, athena, QuickSight, RedShift", "date": "2024-02-05 00:00:00 +0700", "snippet": "Serverless ArchitecturesServerless hosted website: MyBlog.com This website should scale globally Blogs are rarely written, but often read Some of the website is purely static files, the rest is ...", "content": "Serverless ArchitecturesServerless hosted website: MyBlog.com This website should scale globally Blogs are rarely written, but often read Some of the website is purely static files, the rest is a dynamic REST API Caching must be implement where possible Any new users that subscribes should receiv a welcom email Any photo uploaded to the blog should have a thumbnail generated S3 static website, API Gateway, ElasticeCahce for S3, Kinesis with SNS, DynamoDB, Simple Email Service (SES) Serving static content, globally Adding a public serverless REST API Leveraging DynamoDB Global Tables User Welcome email flow Thumbnail Generation flow SQS, SNS, S3, Lambda, S3, CLoudFront AWS Hosted Website Summary We’ve seen static content being distributed using CloudFront with S3 The REST API was serverless, didn’t need Cognito because public We leveraged a Global DynamoDB table to serve the data globally (we could have used Aurora Global Database) We enabled DynamoDB streams to trigger a Lambda function The lambda function had an IAM role which could use SES SES (Simple Email Service) was used to send emails in a serverless way S3 can trigger SQS/SNS/Lambda to notify events Micro Services architecture We want to switch to a micro service architecture Many services interact with each other directly using a REST API Each architecture for each micro service may vary in form and shape We want a micro-service architecture so we can have a leaner development lifecycle for each service API Gateway, AWS Dynamo global stream, Aurora Global, Lambda, Route53, Elastic Load Balancer, ElasticCache Micro Services Environment Dicussions on Micro Services You are free to design each micro-service the way you want Synchronous patterns: API Gateway, Load Balancers Asynchronous patterns: SQS, Kinesis, SNS, Lambda triggers (S3) Challenges with micro-services: repeated overhead forr creating each new microservice issues with optimizing server density/utilization complexity of running multiple versions of multiple microservices simultaneously complexity of running multiple versions of multiple microservices simultaneouly proliferation of client-side code requirements to integrate with many separate services Some of the challenges are solved by Serverless patterns API Gateway, Lambda scale automatically and you pay per usage You can easily clone API, reproduce environments Generated client SDK through Swagger integration for the API Gateway Distributing paid content We sell videos online and users have to paid to buy videos Each videos can be bought by many different customers We only want to distribute videos to users who are premium users We have a database of premium users Links we send to premium users should be short lived Our application is global We want to be fully serverless DynamoDB, Lambda, REST HTTPS, Cognito, CloudFront, OAI Start simple, premium user service Add authentication Add Videos Storage Service Distribute Globally and Secure Distribute Content only to premium users Premium User Video service We have implemented a fully serverless solution: Cognito for authentication DynamoDB for storing users that are premium 2 serverless applications Premium user registration CloudFront Signed url generator Content is stored in S3 (serverless and scalable) Integrated with CloudFront with OAI for security (users can’t bypass) CloudFront can only be used using Signed URLs to prevent unauthorizeed users What about S2 Signed URL? They’re not efficient for global access Software updates offloading We have an application running on EC2, that distributes software updates once in a while When a new software update is out, we get a lot of request and the content is distributed in mass over the network. It’s very costly We don’t want to change our application, but want to optimize our cost an CPI, how can we do it? ECS, load balancer, Cloudront, S3 + OAI, Lambda, DynamoDB Our application current state auto scaling group, multi-AZ, Amazon EFS Easy way to fix things! CloudFront No changes to architecture Will cache software update files at the edge Software update files are not dynamic, they’re static (never changing) Our EC2 instances aren’t serverless But CloudFront is, and will scale for us Out ASG will not scale as much, and we’ll save tremendously in EC2 We’ll also save in availability, networl bandwith cost, etc Easy way to make an existing application more scalaable and cheaper! Big Data Ingestion Pipeline We want the ingestion pipeline to be fully serverless We want to collect data in real time We want to transform the data We want to query the tranformed data using SQL The reports created using the queries should be in S3 We want to load that data into a warehouse and create dashboards Dynamo realtime, S3 select, Athena, Kinesis Redshift, Kinesis stream, S3 static Big Data Ingestion Pipeline IoT devices, Kinesis Data streams, Kinesis Data Firehose, AWS Lambda, S3 Bucket, Simple queue Service, AWS Lambda, Athena, S3, Amazon QuickSight, Amazon Redshift Big Data Ingestion Pipeline discussion IoT Core allows you to harvest data from IoT devices Kinesis is greate for real-time data collection Firehose helps with data delivery to S3 in near real-time (1 minutes) Lambda can help Firehose with data transformation Amazon S3 can trigger notifications to SQS Lambda can subscrible to SQS (we could have connecter S3 to Lambda) Athena is a serverless SQL service and results are stored in S3 The reporting bucket contains anlyzed data and can be used by reporting tool such as AWS QuickSight, Redshift, etc…" }, { "title": "AWS S3 and AWS Storage Gateway Services:", "url": "/posts/aws-s3-and-aws-storage-gateway-services/", "categories": "Fullstack, Architect, AWS", "tags": "aws, architecture, fullstack, s3, Glacier, EFS, FSx, EBS, kms, database, Snow family, CloudFront, InstanceStorage, StorageGateway", "date": "2024-02-05 00:00:00 +0700", "snippet": "Advannce S3S3 MFA Delete MFA (mutil factor authentication) forces uses to genenrate a code in a device (usually a mobile phone or hardware) before doing important operation on S3 To use MFA-Delet...", "content": "Advannce S3S3 MFA Delete MFA (mutil factor authentication) forces uses to genenrate a code in a device (usually a mobile phone or hardware) before doing important operation on S3 To use MFA-Delete, enable versioninng on the S3 bucket You will eed MFA to permanetly delete ann object versio susped versioing on the bucket You won’t need MFA for enabling versionnig listing deleted versions Only the bucket owner (root account) can eable/disab;e MFA delete MFA-Delete currently can only enabled using CLI S3 Default Encryption vs Bucket Policies One way tp “force encryption” is to use a bucket policy ad refuse any API ccall to PUT a S3 object without encryption headers: Another way is to use the “default ecryption” option in S3 Note: Bucket Policies are evaluated before “default encryption” S3 Access Logs For audit purpose, you may want to log all access to S3 buckets Any request made to S3, from any accout, authorized or denied, will be logged into aother S3 bucket That data can be analyzed using data analysis tools… Or Amazon Athena as we’ll see later i this secction S3 Access Logs: Warning Do not set your logging bucket to be the monitored bucket It will create a logging loop, and your bucket will grow in size exponetially S3 Replication (CRR &amp; SRR) Must enable versioing in source and destiation Cross Region Repliccationn (CRR) Same Region Replication (SRR) Buckets ca be in different accounts copying is asynchronous Must give proper IAM permissions to S3 CRR - Use cases: compliancce, lower latency access, replicatio across acccounts SRR - Use ccases: log aggregatio, live replication between production annd test accounnts S3 Replication - Notes After activating, onnly new objects are replicated Optioally, you can replicate existing objects usig S3 batch replication Replicates existing objects and objects that failed replication For DELETE operationns: Can replicate delete markers from source to target (optional settinng) Deleteions with a version ID are not replicated (to avoid malicious deletes) There is no “chaining” of replication If bucket I replcatio into bicket 2, which has replication into bucket 3 Then object create inn bucket I are not replicated to bucket 3 S3 Pre-Signed URLs Cann generate pre-signed URLs using SDK or CLI For downloads (easy, can use the CLI) For uploads (harder, must use the SDK) Valid for a default of 3600 secods, can change timeout with –expireds-in [TIME_BY_SECONDS] argument Users given a pre-signed URL innherit the permissions of the perso who gennerated the Url for GET / PUT Examples: Allow only logged-in users to download a premium video on your S3 bucket Allow a ever changing list of users to download files by generatig URLs dynnamically Allow temporarily a user to upload a file to a precise loccation in our bucket S3 Storage Classes Amazon S3 Standard - Genneral Purpose Amazon S3 Stadard-ifrequent Access(IA) Amazon S3 One Zone-Infrequent Access Amazon S3 Glacier Instant Retrieval Amazon S3 Glacier Flexible Retrieval Amazo S3 Glacier Deep Archive Amazon S3 Intelligent Tierinng Can move betwee classes maually or using S3 lifecycle cofigurations S3 Durability and Availability Durability High durability (99,999999999%   9’s) of objects across multiple AZ If you store 10.000.000 objects with Amazon S3, you ccan on average expect to incur a loss of a single object once every 10.000 years Same for all storage classes Availability Measures how readily available a service is Varies depedig o storage class Example: S3 standard has 99.99% availability = not avaiable 53 minutes a year S3 Standard - General Purpose 99.99% Availability Used for frequently accessed data Low latenccy annd high throughput Sustai 2 concurennt faccility failures Use CAses: Big Data analytics, mobile &amp; gaming appliccations, conntet distribution… S3 Storage Classes - Innfrequent Access For data that is less frequently accessed, but requires rapid access when needed Lower cost than S3 Standard Amazon S3 Standard- Ifreqeunt Access (S3 Standard-IA) 99.9% Availability Use cases: Disaster recovery, backups Amazon S3 Oe Zonne-Infrequet Acccess (S3 One Zone-IA) High durability (99.999999999%) in a sigle AZ; data lost when AZ is destroyed 99.5% Availability Use Cases: Storing secodary backup copies of on-premises data, or data you cann recreate Amazo S3 Glacier Storage Classes Low cost objecct storage meant for archving / backup Pricing: pricce for storage + object retrieval cost Amazon S3 Glacier Instant Retrieval Milisecond retrieval, greate for data accessed once a quarter Minimum storage duration of 90 days Amazon S3 Glacier Fexible Retrieval (formerly Amazon S3 Glacier): Expedited (1 to 5 minutes), Standard (3-5 hours), Bulk (5-12 hours)- free Amazon S3 Glaccier Deep Archive - for long term storage Standard (12 hours), Bulk (48 hours) Minimum storage duration of 180 days S3 Intelliget-Tiering Small mothly moitorig ad auto tierig fee Moves objects automatically between Acccess Tiers based on usage There are o retrieval charges i S3 Itelliget-Tierig Frequet Access tier (automatic): default tier ifrequet Access tier (automatic): objet ot accessd for 30days Archive Istant Access tier (automatic): object ot accessed for 90days Archive Access tier (optioal): configuratio from 90 days to 700+ days Deep Arcchive Access tier (optionnal): ccofig, from 180 days to 700+ days S3 - Moving between storage classes You can transition objeccts betwee storage classes For infrequently accessed objecct, move them to STADARD_IA For arcchive objects you don’t need in real-time, GLACIER or DEEP_ARCHIVE Moving objects can be automated using a lifecycle configuration S3 Lifecycle rules Transition actions: It defines whe objects are transitioned to aother storage class Move objects to Standard IA class 60 days after creatio Move to Glacier for archiving after 6 months Expiration actions: configure objects to expire (delete) after some time Access log files can be set to delete after a 365 days Can be used to delete old versions of files (if versioning is enabled) Rules can be created for a certai prefix (ex- s3://mybucket/mp3/*) Rules can be created for certain object tags (ex: Departmet Fiancer) S3 Lifecycle Rules - Sceario I Your applicationn o Ec2 creates images thuhmbails after profile photos are uploaded to Amazon S3. These thumbails can be easily recreated, ad only need to kept for 45 days. The source images should be able to be immediately retrieved for these 45 days, and afterwards, the user can wait up to 6 hours. How would you desig this? S3 source images can be on STADARD, with a lifecycle configuration to transitionn them to GLACCIER after 45 days. S3 thumbnails cann ben o OEZOE_IA, with a lifecycle configurationn to expire them (delete them) after 45 days.es S3 Lifecyccle Rules - Scennario 2 A rule in your company stated that you should be able to recover your deleted S3 objects immediatedly for 15days, although this happen rarely. after this time, and for up to 365 days, deleted objects should be recoverable within 48 hours. You need to enable S3 versioning in order to have objecct versions, so thhat “deleted objects” are in fact hidden by a “delete marker” ad ccan be recored You can transition these “ocurrent versions” of the object to S3_IA You can transition afterwards these “noncurrent versios” to DEEP_ARCHIVE S3 Analyticcs - Storage class analysis You can setup S3 Abakytics too help determine wheh to trannsition objects from Stadard to Standard_IA Does ot work for ONNCCEZONE_IA or GLACIER Report is updated daily Takes about 24h to 48h hours to first start Good first step to put together lifecycle rules (or improve them)! S3 Baseline Performace Amazon S3 automatically scales to high request rates, latecy 100-200 ms Your applicationnn ccan achieve at lease 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD request per secod per prefix in a bucket. There are o limits to the number of prefixes in a bucket Example (object path -&gt; prefix): bucket/folder1/sub/file =&gt; /folder1/sub1/ If you spread read across all four prefixes evenly, you can achieve 22000 requests per second for GET and HEAD S3 KMS Limitatio If you use SSE-KMS, you may be impacted by the KMS limits When you upload, it calls the GenerateDataKey KMS API When you download, it calls the Deccrypt KMS API Cout towards the KMS qouta per seconnd You can request a qouta increase usinng the service qoutas console S3 Performacce Multipart upload recomeded for files &gt; 100MB, must use for files &gt; 5GB can help parallelized uploads (speed up transfers) S3 Trasfer Acceleration Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket i thhe target region Compatible with multi-part upload S3 Performace - S3 Byte Rage Fetches Parallelize GETs by requesting specificc byte rages Better resiliece in case of failures S3 Select &amp; Glacier Select Retrieve less data using SQL by performing server side filterig Ca filter by rows &amp; columns (simple SQL statemennnts) Less network transfer, less CPU cost client-side S3 Evet ootifications S3:ObjecctCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication… Object name filterig possible (*.jpg) Use case: geerate thumbnails of images uploaded to S3 Can create as many “S3 events” as desired S3 evet notifications typically deliver evennts i seconnds but ca sometimes take a minute or logger S3 Evennt Notifications with Amazon EvenntBridge Adanced filterig options with JSON rules (metadata, object size, name…) Multiple Destiations - ex Step Fuctios, Kinesis Stream/ Firehose… EvenntBrdge CApabilities - Archive, Replay Evet, Reliable delivery S3 - Requester Pays Ingeeral, bucket owners pay for all Amazon S3 storage ad data trasfer ccosts assocciatted with their bucket With Requester Pays buckets, the requester instread of the bucket owner pays the cost of the request and the data download from the bucket Helpful when you want to share large datasets with other accoounts The requester must be authenticated in AWS (cannot be anonymous) Amazon Athena Serverless query service to perform analytics against S3 objects Uses Stadard SQL language to query files Supports CSV, JSON, ORC, Avro, and Parquet (built on Presto) Pricinng: $5.00 per TB of data scanned Use compressed or ccolumnnar data for cost-savinng (less sccann) Use cases: Business intelligece / analytics / reporting, analyze &amp; query VPC Flow Logs, ELB Logs, CloudTrail trails, etc… ExamTip: aalyze data in S3 usig serverless SQL, use Athena Glaccier Vault Locck Adopt a WORM (write Once Read Many) model Lock the policy for future edits (can no loger be changed) Helpful for compliannce and data retention S3 Object Lock (versioning must be enabled) Adopt a WORM (Write Onncce Read Many) model Block an object version deletion for a specified amount of time Object retentio: Retention Period: specifies a fixed period Legal Hold: Same protecctio, no expiry date Models: Governance mode: users can’t overwrite or delete an object version or alter its lock setting unless they have special permissios Compliance mode: a protected object version ccan’t be overwrittenn or deleted by compliance mode, its retention mode ccann’t be changed, and its retetion period can’t be shortened. S3 AWS CloudFront Content Delivery etwork (CDN) Improves read perfomance, conntennnt is cached atn the edge 216 poit of presence globally (edge locations) DDOS protection, inntegration witth shield aws web application firewall Can expose external HTTPS ad cann talk to internnal HTTPS backends CloudFront - Origins S3 bucket For distributing files and cachig them at the edge Ennhaced security with CloudFrot Origi Access Identity (OAI) CloudFrot can be used as an igress (to upload files to S3) custom Origi (HTTP) Applicatio Load Balancer EC2 instance S3 website (must first enable the buccket as a stati S3 website) Any HTTP backend you want CloudFront - S3 as ann Origin Allow Public IP of edge locations Allow security group of load balancer OAI + S3 bucket policy CloudFront Geo Restriction whitelist and blacklist CloudFront vs S3 Cross Region Replication Cloudfront Global edge network Files are cacched for TTL (maybe a day) Great for static contet that must be available for everywhere S3 Cross Regio Replicatio Must be setup for eacch region you want replication to happen Files are updated i near real-time Readonly Greate for dynamic content that needs to be available at low-latency in few regions AWS CloudFront Hands On We’ll create an S3 bucket We’ll create a CloudFront distribution We’ll create an Origi Access Idetity We’ll limit the S3 bucket to be accessed only using this identity CloudFront Siged URL / Signed Cookies You want to distribute paid shared cotent to premium users over tthe world We can use CloudFront Signed URL/Cookies. We attach a policy with: Include URL expiration Include IP ranges to access the data from Trusted signers (which AWS accounts ccan create signed URLs) How long should be URL be valid for? Shared content (movie, music): make it short (a few minnutes) Private content (private to the user): you can make it last for years Signed URL = access to individual files (one signed URL per file) Signed cookies = access to multiple files (one siged cookie for may files) CloudFront Siged URL Diagram CloudFront Signed URL vs S3 Pre-Signed URL CloudFront Signed URL Allow access to a path, no matter the origin Account wide key-pair, only the root can manage it Can filter by IP, path, date, expiratio Can leverage caching features S3 Pre- Signed URL Issues a request as the person who pre-signed the URL Uses the IAM key of the signing IAM pricipal Limited lifetime CloudFront Pricce class Price class: All, 200, 100 CloudFront Origin Based on path pattern: • /images/* • /api/* • /* Origin group: high availability and do failover CloudFront – Field Level Encryption protected user sensitive iformation through application stack, HTTPS, asymmetric encryption Adds an additional layer of security along with HTTPS Usage: Specify set of fields in POST requests that you want to be encrypted (up to 10 fields) • Specify the public key to encrypt them AWS Global Accelerator Anycast IP: all servers hold the same IP address and the client is routed to the nearest one Unicast IP: one server holds one IP address Leverage the AWS internal etwork to route to your applicatio 2 anycast IP are created for your application. The anycast IP send traffic directly to edge locations. The Edge locations send traffic to your application Performance Intelligent routing to lower latecy any fast regional failover No issue with client cache Interal AWS network Health Checcks Global Acccelerator performs a health check Help make your appliccationn global (failover lesstha 1 mintes) Great for disater recovery Security Only 2 external IP need to be whhitelisted DDos protectio thanks to AWS shield AWS Global Accelerator vs CloudFront They both use the AWS global nnetwork and its edge locations around the workld Both services inntegrate with AWS Shiled for DDos protectionn CloutFront Improves performance for both cache able content (such as images and video) Dynamic content (such as API acceleratio and dynamic site delivery) Content is served at the edge Global Accelerator Improves performance for a wide range of application over TCP and UDP Proxying packes at the edge to applications running in one or more AWS regions Good fit for non-HTTP use cases as gaming (UDP), IoT (MQTT), or Voice over IP Good for HTTP use cases that require static IP address Good for HTTP use cases that required determistic fast regional failover AWS Snow Family Highly - secure, portable devices to collect and process data at the edge, and migrate data into and out to AWS Data migration: snowcone, snowballedge, snowmobile Edge computing: snowcone, snowball edge Data migrations with AWS snow Family High speeds: 100mbs, 1Gbs, 10Gbs Limited connectivity Limited bandwidth High network cost Shared bandwidth Connection stability offline, more than one week Snowball Edge Physical data transport solution: move Tbs, Pbs of data in or out AWS Pay network fees, pay per data transfer job 80TB of HDD capacity for block volume and S3 compatible object storage(storage) 42TB of HDD capacity for block volume and S3 compaitible object storage(compute) up to 10 nodes Use cases: large data cloud migrations, DC deccommisions, disaster recovery AWS snowcore small, portable computing, anywhere, regged &amp; secure, withstands harsh environments Light (4.5 pounds, 2.1 Kg) Device used for edge omputing, storage and data transfer 8Tbs usable storage Use Snowcore when snowball does not fit(space costrained environment) Must provide your own battery / cables Can be sent back to AWS offline, or connect it to internet and use AWS DataSync to send data AWS snowmobile Transfer exabytes of data Each snowmobile has 100PB of capacity High security: temperature control, GPS, 24/7 video surveillance Better than snowball if you transfer more than 10 PB Usage Process request snowball devices from the AWS console for delivery Install the snowball client / AWS OpsHub on your servers Connect the snowball to your servers and copy files using servers Ship back the device when you’re done (goes to the right AWS facility) Data will loaded into an S3 bucket Snowball is completely wiped What is Edge computing Process data while it’s being created, on an edge location These locations may have Limited / no internet access Limited / no easy access to computing power We setup a Snowball Edge / Snowcone device to do edge computing Use case of edge computing Preprocess data Machine learning at the edge Transcoding media streams Eventually (if need be) we can ship back the device to AWS (for transfering data) Snow family - edge computing Snowcone(smaller) 2 CPUs, 4 GB of memory, wired or wireless access • USB-C power using a cord or the optional battery Snowball Edge - Compute opimized 52 vCPUs, 208 GiB of RAM. Optional GPU (useful for video processing or machine learning) • 42TBusablestorage Snowball Edge - Storage Optimized Up to 40 vCPUs,80 GiB of RAM. Object storage clustering available All: can run EC2 instances &amp; AWS Lamba functios (using AWS IoT Greengrass) Long-term deployment options: 1 and 3 years discounted pricing AWS OpsHub Historical to use Snow family devices, you needed a CLI (Command Line Interface tool) Today, you can use AWS OpsHub (a software you install onn your computer latop) to manage your snow family device unlocking and configured single or cluster devices transfering and managig instances runninng on Snow Family devices Launcing and managinng isntannces runnnig on Snow family Devices Monitor devie mmetrics (storage capacity, active isntancces on your device) Launch compatible AWS services on your devices Snowball to Glacier Snow cannot import to Glacier directly You must use Amazone S3 first, in combination with an S3 life cycle policy Amazonne FsX - Overview Launch 3rd party high-performance file systems on AWS Fully managed service FSx for Lustre FSx for windows file server FSx for Netapp ontap Amazon Fsx for windows (file server) EFS is a shared POSIX system for linux systems. FSX for windows is a fully managed widoes file system shared drive Supports SMB protocol &amp; windoes NTFS Microsoft active directory intefratio ACLS user qoutas Can be nmounted o Linux EC2 instances Scale up to 10s of GB/s, milios IOPS, 100s PB of data Storage options: SSD: latency sensitive workloads (database, media proccessing data analytics, ,,,) HDD: broad spectrum of workloads (home directory, CMS, …) Can be accessed from your o-premises infrastrucure Cann be configured to be Multi-AZ Data is backed up daily to S3 Amazon FSx for Lustre (High performance computing) Lustre is a type of parallel distributted file system, for large scale computing The name Lustre is derived from “linux” and “cluster” Machine learning, high performance computing (HPC) Video processing, financial modeling, electronic design automation Scale up to 100s GB/s, milions of IOPS, sub-ms latencies Storage Options: SSD: low-latency, IOPS intensive workloads, small &amp; random file operations HDD: throughput-intensive workloads, large &amp; sequential file operations Seamless integration with S3 Can “read” S3 as file system (through FSx) Can “write” the output of the computatioos back to S3(through FSx) Can be used fron on-premises serves (VPN or Direct Connect) FSx File System deployment options Scratch File system Temporary storage Data is not replicated (doesn’t persit if file server fails) High burst (6x faster, 200MBs per TiB) Usage short-term processing, optimize costs Persistent File System Long term storage Data is replicated within same AZ Replace failed files within minutes Usage: long term processing, sensitive data Hybrid Cloud for Storage AWS is pushing for “hybrid cloud” Part of your infrstruccture is on the cloud Part of your infrastructure is on-premises This can be due to Long cloud migrations Security requirements CCCompiliance requirements It Stragegy S3 is a proprietary storage technology (uunlike EFS, NFS), so how do you expose the S3 data on premises AWS Storage gateway! Cloud Storage Cloud Native Options Block: EBS, ECS instance store File: Amazon EFS, Amazon FsX Object: S3, Glacier Storage Gateway Bridge between on-premise data and cloud data in S3 Use cases: disater reovery, backup &amp; restore, tiered storage 3 types of storage gateway File Gateway Volume Gateway Tape Gateway File Gateway Configured S3 bucckket are accessible using the NFS and SMB protocol Supports S3 standard, S3 IA, S3 One Zoe IA Buckket acess using IAM roles for each file Gateway Most recently used data is cached in the file Gateway Can be mounted on many servers Inntegrated with Active Direcctory (AD) for user authentication Volume Gateway Block storage using iSCSI protocol backed by S3 Backed by EBS snapshots which can help restore on-premises volumes! Cached volumes: low latency acess to most recent data Stored volumes: entire dataset is on premise, scheduled backups to S3 Tape Gateway Some companies have backup processes using physical tapes (!) With Tape Library (VTL) backed by Amazon S3 and Glacier Tape Gateway, compaies use same process but in the cloud Backup data using existing tape-based processes (annd iSCSI interface) Works witth leading backup software vendors Storage Gateway - Hardware appliance Using Storage Gateway means you need on-premises virtualization Otherwise, you can use a Storage Gateway Hardware Appliancce You can buy it o amazon.com Works with File Gateway Colume Gatewaym TapeGateway Helpful for daily NFS backups in small data centers AWS Storage Gateway Summary Exam tip: Read the question well, it will hinnt at which gateway to use On-premises data to the cloud =&gt; Storage Gateway File access / NFS - user auth with Active Directory =&gt; File Gateway (S3) Volume / Block Storage / iSCSI =&gt; Volume gateway (backed by EBS snapshots) VTL Tape solution / backup with iSCSI =&gt; Tape Gateway (backed by S3 and Glacier) No on-premises virtualization =&gt; Hardware appliance Amazon FSx File Gateway Native access to Amazon FSx for Windows File Server Local cache for frequenncy accessed data Windows nnative compatibility (SMB, NTFS, Active Directory…) Useful for group file shares and home directories AWS Trannsfer Family A fully-mannaged service for file transfer into annd out of amazon S3 or Amazon EFS using the FTP protocol Supported Protools AWS Transfer for FTP FTPS SFTP Managed infrastucture, Scalable, Reliable, Highly Avaiable (multi AZ) Pay per provisioned endpoint per hour + data transfer in GB Store and manage user’s credentials within the service Integrate with existing authenticattion systems (Microsoft Active Directory, LDAP, Okta, Amazon Cognito, custtom) Usage: Sharing files, public datasets, CRM, ERP, … Storage Comparison S3: Object Storage Glacier: Object Archival EFS: Network file system for linux instances, POSIX filesystem FSx for Windows: Netwoork file system for windows servers FSx for Lustre: High performance computing linuxx file system EBS volumes: Network Storage for one EC2 instance at a time Instance Storage: Physiccal storage for your EC2 instance (hhigh IOPS) Storage Gateway: File Gateway, Volume Gateway (cache &amp; stored), Tape Gateway Snowball/snowmobile: to move large amount of data to theh cloud, physically Database: for specific workloads, ussually with indexxing and querying" }, { "title": "Advance S3", "url": "/posts/advance-s3/", "categories": "Fullstack, Architect, AWS", "tags": "fullstack, architecture, s3, MFA, KMS", "date": "2024-02-05 00:00:00 +0700", "snippet": "Advanced S3S3 MFA-Delete MFA (multi factor authentication) forces user to generate a code on a device (ussually a mobile phone or hardware) before doing important operations on S3 To use MFA-Dele...", "content": "Advanced S3S3 MFA-Delete MFA (multi factor authentication) forces user to generate a code on a device (ussually a mobile phone or hardware) before doing important operations on S3 To use MFA-Delete, enable Versioning on the S3 bucket You will need MFA to Permanently delete an object version suspend versioning on the bucket You won’t need MFA for enabling version listing deleted versions Only the bucket owner (root account) can enable/disable MFA delete MFA Delete currently can only S3 Default Encryption vs Bucket Policies One way to “force encryption” is to use bucket policy and refuse any API call to PUT an S3 object without encryption headers: Another way is to use the “default encryption” option in S3 Note: Bucket Policies are evaluated before “default encrytion” S3 Access Logs For audit purpose, you may want to log all access to S3 buckets Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket That data can be analyzed using data analysis tools… Or Amazon Athena as we’ll see later in this section! The log format is at:https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html S3 Access Logs: Warning Do not set your logging bucket to be the monitored bucket It will create a logging loop, and your bucket S3 Replication (CRR &amp; SRR) Must enable versioning in source and destination Cross Region Replication (CRR) Same Region Replication (SRR) Buckets can be in different accounts Copying is asynchronous Must give proper IAM permissions to S3 CRR - Use cases: compliance, lower latency access, replication across accounts SRR - Use cases: log aggregation, live replication between production and test accounts S3 Replication - Notes After activating, only new objects are replicated Optionally you can replicate existing object using S3 Batch Replication Replicates existing objects and objects that failed replication For Delete operations: Can replicate delete markers from source to target (optional setting) Deletions with a version ID are not replicated (to avoid malicious deletes) There is no “chaining” of replication If bucket I has replication into bucket 2, which has replication into bucket 3 The objects created in bucket I are not replicated to bucket 3 S3 Pre-Signed URLs Can generate pre-signed URLs using SDK or CLI For downloads (easy, can use the CLI) For uploads (harder, must use the SDK) Valid for a default of 3600 seconds, can change timeout with –expires-in [TIME_BY_SECONDS] argument Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT Examples: Allow only logged-in users to download a premium video on your S3 bucket Allow an ever changing list of users to download files by generating URLs dynamically Allow temporarily a user to upload a file to a precise location in our bucket© Stephane Maarek S3 Storage Classes Amazon S3 Standard - General Purpose Amazon S3 Standard-Infrequent Access (IA) Amazon S3 One Zone-Infrequent Access Amazon S3 Glacier Instant Retrieval Amazon S3 Glacier Flexible Retrieval Amazon S3 Glacier Deep Archive Amazon S3 Intelligent Tiering Can move between classes manually or using S3 Lifecycle configurations S3 Durability and Availability Durability High durability (99.99999999%,   9’s) of objects across multiple AZ If you store 10000000 objects with Amazone S3, you can on average expect to incur a loss of a single object once every 10000 years Same for all storage classes Availability Measures how readily available a service is Varies depending on storage class Example S3 standard has 99.99% availability = not available 53 minutes a year S3 Standard – General Purpose 99,99% Availability Used for frequentlly accessed data Low latency and high throughput Sustain 2 concurent facility failures Use Cases: Big Data analytics, mobile &amp; gaming applications, content distribution… S3 Storage Classes – Infrequent Access For data that is less frequently accessed, but requires rapid access when needed Lower cost than S3 standard Amazon S3 standard-infrequent Access (S3 standard-IA) 99.99% availability Use cases: Disater recovery, backups Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) High durability (99.999999%) in a single AZ; data lost when AZ is destroyed 99.5 Availability Use Cases: Storing secondary backup copies of on-premises data, or data you can recreate Amazon S3 Glacier Storage Classes Low cost object storage meant for archiving / backup Pricing: price for storage + object retrieval cost Amazon S3 Glacier Instant Retrieval Milisecond retrieval, great for data accessed once a quarter Minimum storage duration of 90 days Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier) Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free Minimum storage duration of 90 days Amazon S3 Glacier Deep Archive – for long term storage Standard (12 hours), Bulk (48 hours) Minimum storage duration of 180 days S3 Intelligent-Tiering Small monthly monitoring and auto-tiering fee Moves objects automatically between Access Tier based on usage There are no retrieval charges in S3 Intelligent-Tiering Frequent Access tier (automatic): default tier Infrequent Access tier (automatic): objects not accessed for 30 days Archive Access tier (optional): configurable from 90 days to 700+ days Deep Archive Access tier (optional): config. from 180 days to 700+ days S3 Storage Classes Comparison S3 – Moving between storage classes You can transition objects between storage classes For infrequently accessed object, move them to STANDARD_IA For archive objects you don’t need in real time, GLACIER or DEEP-ARCHIVE moving objects can be automated using a lifecycle configuration S3 Lifecycle rules Transition actions: it defines when objects are transitioned to another storage class Move objects to Standard IA class 60 days after creation Move to Glacier for archiving after 6 months Expiration actions: configure oobjects to expire (delete) after some time Access log files can be set to delete after a 365 days Can be used to delete old versions of files (if versioning is enabled) Can be used to delete incomplete multi-part uploads Rules can be created for a certain prefix (ex - s3://mybucket/mp3/*) Rules can be created for a certain object tags (ex - Department Finance) S3 Lifecycle Rules – Scenario 1 Your application on Ec2 creates images thumbnails after profile photos are uploaded to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 45 days. The source images should be able to be immediately retrieved for these 45 days, and afterwards, the user can wait up to 6 hours. How would you design this? S3 source images can be on STANDARD, with a lifecycle configuration to expire them (delete them) after 45 days. S3 Lifecycle Rules – Scenario 2 A rule in your company states that you should be able to recover your deleted S3 objects immediately for 15 days, although this may happen rarely. After this time, and for up to 365 days, deleted objects should be recoverable withn 48 hours. You need to enable S3 versioning in order to have object versions, so that “deleted objects” are in fact hidden by a “delete marker” and can be recovered You can transition these “noncurrent versions” of the object to S3_IA You can transition afterwards these “noncurrent versions” to DEEP_ARCHIVE S3 Analytics – Storage Class Analysis You can setup S3 analytics to help determine when to transition objects from standard to standard_IA Does not work for ONEZONE_IA or GLACIER Report is updated daily Takes about 24h to 48h hours to first start Good first step to put together lifecycle rules (or improve them)! S3 Baseline performance Amazon S3 automatically scales to high request rates, latency 100-200ms Your application can achieve at least 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. Example (object path =&gt; prefix): bucket/folder1/sub1/file =&gt; /folder1/sub1/ bucket/folder1/sub2/file =&gt; /folder1/sub2/ bucket/1/file =&gt; /1/ bucket/2/file =&gt; /2/ If you spread reads across all four prefixes evenly, you can achieve 22000 requests per second for GET and HEAD S3 – KMS Limitation If you use SSE-KMS, you may be impacted by the KMS limits When you upload, it call the GenerateDataKey KMS API When you download, it call the Decrypt KMS API Count towards the KMS qouta per second (5500, 10000. 30000 req/s based on region) You can request a qouta increase using Service Qoutas Console S3 Performance Multi-part upload recommended for files &gt; 100MB, must use for files &gt; 5GB Can help parallelize uploads (speed up transfers) S3 Transfer Acceleration Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region. Compatible with multi-part upload S3 Performance - S3 Byte Range Fetches Parallelize GETs by requesting specific byte ranges Better resilience in case of failures Can be used to speed up downloads Can be used to retrieve only partial data (for example the head of a file) S3 Select &amp; Glacier Select Retrieve less data using SQL by performing server side filtering Can filter by rows &amp; columns (simple SQL statements) Less network transfer, less CPU cost client-side" }, { "title": "AWS Api gateway, Lambda, Serverless, and Services part I", "url": "/posts/aws-api-gateway-lambda-serverless-and-services/", "categories": "Fullstack, Architect, AWS", "tags": "aws, sqs, sns, serverless, service, sts", "date": "2022-12-05 00:00:00 +0700", "snippet": "AWS API Gateway AWS Lambda + API Gateway: No infrastructure to manage Support for the WebSocket Protocol Handle API versioning (v1, v2…) Handle different environments (dev, test, prod…) Handle...", "content": "AWS API Gateway AWS Lambda + API Gateway: No infrastructure to manage Support for the WebSocket Protocol Handle API versioning (v1, v2…) Handle different environments (dev, test, prod…) Handle security (Authentication and Authorization) Create API keys, handle request throttling Swagger / Open API import to quickly define APIs Transform and validate requests and responses Generate SDK and API specifications Cache API responsesAPI Gateway – Integrations High Level Lambda Function Invoke Lambda function Easy way to expose REST API backed by AWS Lambda HTTP Expose HTTP endpoint in the backend Example: internal HTTP API on premise, Application Load Balancer… Why? Add rate limiting, caching, user authentications, API keys, etc… AWS Service Expose any AWS API through the API Gateway? Example: start an AWS Step Function workflow, post a message to SQS Why? Add authentication, deploy publicly, rate control… API Gateway - Endpoint Types Edge-Optimized (default): For global clients Requests are routed through the CloudFront Edge locations (improves latency) The API Gateway still lives in nly one region Regional: For client within the same region Cloud manually combine with CloudFront (more control over the caching strategies and the distribution) Private: Can only be accessed from your VPC using an interface VPC endpint (ENI) Use a resource policy to define access API Gateway – Security: IAM Permissions Create an IAM policy authorization and attach to User / Role API Gateway verifies IAM permissions passed by the calling application Good to provide access within your own infrastructure Leverages “Sig v4” capability where IAM credential are in headersAPI Gateway – Security: Lambda Authorizer (formerly Custom Authorizers) Uses AWS Lambda to validate the token in header being passed Option to cache result of authentication Help to use Oauth / SAML / 3rd party type of authentication Lambda must return an IAM policy for the userAPI Gateway – Security: Cognito User Pools Cognito fully manages user lifecycle API gateway verifies identity automatically from AWS Cognito No custom implementation required Cognito only helps with authentication, not authorizationAPI Gateway – Security – Summary IAM Greate for users/ roles already within your AWS account Handle authentication + authorization Leverages Sig v4 Custom Authorizer: Greate for 3rd party tokens Very flexible in terms of what IAM policy is returned Handle Authentication + Authorization Pay per Lambda invocation Cognito User Pool you manage your user pool (can be backed by Facebookm Google login etc…) No need to write any custom code Must implement authorization in the backend AWS Integration &amp; MessaginngSQS, SNS &amp; Kinesis When we start deploying multiple applications, they will inevitably need to communicate with one another There are two patterns of application communication Synchromous communication (app to app) Asynchronous / Evenbased (app to queue to app) Introdution Synchronnous between applications can be problematic if there are sudden spikes of traffic What if you need to suddenly encode 1000 videos but usually it’s 10? in that case, it better to decouple your applications, SQS: queue model SNS: pub/sub model Kinesis: real-time streaming model These services can scale independently from our application!Amazon SQS, What’s a queue?SQS - Standard queue Oldest offering (over 10 years old) Fully managed service, used to decouple applications Attributes: unlimited throughput, unlimited number of messages in queue Default retention of message: 4 days, maxximum of 14 days Low latenccy (&lt; 10ms on publish and receive) Limitation of 256kb per message sent Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering)SQS - product messages Produced to SQS using the SDK (SendMessage API) The message is persited in SQS until consumer deletes it Message retention: default 4 days, upto 14days Example: order id customer id any attributes you want SQS standard: unlimited throughputSQS - Consuming messages Consumes (running on EC2 instances, Servers, or AWS lambda)… Poll SQS for messages (receive up to 10 messages at a time) Process the messages (example: insert the message inyo an RDS database) Delete the messages using thhe deleteMessage APISQS - Mutiple EC2 Innstances Consumers Consumers receive and process messages in parallel At least once delivery Best effort message ordering Consumers delete messages after processig them We can scale consumers horzontally to improve throughput of processingSQS with auto scaling group (ASG)SQS to decouple between appliccation tiersAmazon SQS - Security Encryption In-flight encryption using HTTPS API At rest encryption using KMS keys Client side encrytion if the client wants to perform encryptioon/decrytion itself Access Controls: IAM policies too regulate access to SQS API SQS Access Policies (similiar to S3 bucket policies) Useful for cross account access to SQS queues Useful for allowinng other services (SNS, S3…) to write to an SQS queue SQS Queue Access Policy Cross Acccount Access Publish S3 Event Notifications to SQS QueueSQS Message Visiblity Timeout After a message is polled by a consumer, it becomes invisible to other consumers By default, the “message visibility timeout” is 30 seconds That means the message has 30 seconnds to be processed After the message visibility timeout is over, the message is “visible” in SQS If a message is not processed within the visibility timeout, it will be processed twice A consumer could call the ChangelMessageVisibility API tto get more time If visibility timeout is high (hours), and consumer crashes, re-processing will take ttime If visiblity ttimeouut is too low (seconds), we may get duplicatesAmazon SQS - Dead Letter Queue If a consumer fails to process a message within the visibility timeout… the message goes back to the queue! We can set a threshold of how manny times a message can go back to the queue After tthe mmaximumreceivves tthreshold is exceeded, the message goes intto a dead lettter queue (DLQ) Userful for debugginng! Make sure to process the messages in the DLQ before they expire: Good to sett a rettenttion oof 14days in the DLQ SQS DLQ - Redrive to Source Feature to help consume messages in the DLQ to understand what is wrong with them. When our code is fixed, we can redrive the messages from the DLQ backk into the source queue (or any otther queue) inn batches without writing custtom codeAmazon SQS - Delay Queue Delay a message (consummers don’t see it immediately) up to 15 minuttes Defaultt is 0 secconnds (message is available rightt away) Can set a defaultt at queue level Can override tthe defaultt o sennd using the DelaySecccond parametersAmazon SQS - Long Polling When a consumer requests messages from the queue, it can optioally “wait” for messages to arrive if tthere are nonne in the queue This is called long pollinng LongPolling dereases the nuumber of API calls made tto SQS while increasign tthe efficiency and reduinng lattenccy of your applicationn The wait time can be between 1 se tto 20 sec (20 sec preferable) Lonng polling is preferable to short polling long polling level using waittimeseconndsAmazon SQS – Dead Letter Queue If a consumer fails to process a message within theVisibility Timeout… We can set a threshold of how many times a message can go back to the queue After the maximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) Useful for debugging! Make sure to process the message in the DLQ before they expire: Good to set a retention of 14days in the DLQ SQS DLQ – Redrive to Source Redrive to Source Feature to help consume messages in the DLQ to understand what is wrong with them When our code is fixed, we can redrive the messages from the DLQ back into the source queue (or any other queue) in batches without writing custom codeAmazon SQS – Delay Queue Delay a message up to 15minutes Default is 0 seconds Can set a default at queue level Can override the default on send using the delayseconds parameterAmazon SQS - Long Polling When a consumer requests messages from the queue, it can optionally “wait” for messages to arrive if there are none in the queue This is called long polling Longpolling decrease the number of API calls mafe to SQS while increasing the The wait time can be between 1 sc to 20 sec Long polling is preferable to short polling Long polling can be enabled at the queue level or at the API level using WaitTimeSecondsSQS – Request-Response Systems To implement this pattern: use the SQS Temporary Queue Client It leverages virtual queues instead of creating / deleting SQS queues (cost-effective)Amazon SQS – FIFO Queue FIFO = First In First Out (ordering of messages in the queue) Limited throughput: 300 msg/s without batching, 3000 msg/s with Exactly-once send capability (by removing duplicates) Messages are processed in order by the consumerKinesis Overview Makes it easy to collect, process, and analyze streaming data in real-time Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data… Kinesis Data Streams: capture, process, and store data streams Kinesis Data Firehose: load data streams into AWS data stores Kinesis Data Analytics: analyze data streams with SQL or Apache Flink Kinesis Video Streams: capture, process, and store video streamsKinesis Data Streams Retention between 1 day to 365 days Ability to reprocess (replay) data Once data is inserted in Kinesis, it can’t be deleted (immutability) Data that shares the same partition goes to the same shard (ordering) Producers: AWS SDK, Kinesis Producer Library(KPL), Kinesis Agent Consumers: Write your own: Kinesis Client Library (KCL), AWS SDK Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics Kinesis Data Streams – Capacity Modes Provisioned mode: You choose the number of shards provisioned, scale manually or using API Each shard get 1MB/s in (or 1000 records per second) Each shard get 2MB/s out (classic or enhanced fan-out consumer) You pay per shard provisioned per hour On-demand mode: No need to provision or manage the capacity Default capacity provisioned (4 MB/s in or 4000 records per second) Scales automatically based on observed throughput peak during the last 30days Pay per stream per hour &amp; data in/out per GB Kinesis Data Firehose Fully managed service, no administration, automatic scaling, serverless AWS: Redshift / Amazon S3 / ElasticSearch 3rd party partner: Splunk / MongoDB / DataDog / NewRelic / … Custom: send to any HTTP endpoint Pay for data going through Firehose Near Real Time 60 seconds latency minimum for non full batches Or minimum 32 MB of data at a time Supports many data formats, conversions, transformations, compression Supports custom data transformations using AWS Lambda Can send failed or all data to a backup S3 bucketKinesis Data Streams vs FirehoseKinesis Data Streams Streaming service for ingest at scale Write custom code (producer / consumer) Realtime (~200ms) Manage scaling (shard splitting / merging) Data storage for 1 to 365 days Suports replay capabilityKinesis Data Firehose Load streaming data into S3 / Redshift /ES / 3 rd party / custom HTTP Fully managed Near real-time (buffer time min. 60 sec) Automatic scaling No data storage Doesn’t support replay capabilityKinesis Data Analytics (SQL application) Perform real-time analytics on Kinesis Streams using SQL Fully managed, no servers to provision Automatic scaling Real-time analytics Pay for actual consumption rate Can create streams out of the real-time queries Use cases: Time-series analytics Real-time dashboards Real-time metrics Ordering data into Kinesis Imagine you have 100 trucks (truck_1, truck_2,.., truck_100) on the road sending their GPS positions regularly into AWS You want to consume the data in order for each truck, so that you can track their movment accurately How should you send that data into Kinesis? Answer: send using a “Partition Key” value of the “truck_id” The same key will always go to the same shardOrdering data into SQS For SQS standard, there is no ordering For SQS FIFO, if you don’t use a Group ID, messages are consumed in the order they are sent, with only one consumer You want to scale the number of consumers, but you want messages to be “grouped” when they are related to each other Then you use a Group ID (similar to Partition Key in Kinesis)Kinesis vs SQS ordering Let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO Kinesis Data Streams: (500 consumer) On average you’ll have 20 trucks per shard Trucks will have their data ordered within each shard The maximum amount of consumers in parallel we can have is 5 Can receive up to 5 MB/s of data SQS FIFO You only have one SQS FIFO queue You will have 100 Group ID You can have up to 100 Consumers (due to the 100 Group ID) You have up to 300 messages per second (or 3000 if using batching) Amazon SNS What if you want to send one message to many receivers? The “event producer” only sends message to one SNS topic As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications Each subscriber to the topic will get all the messages (note: new feature to filter messages) Up to 12,500,000 subscriptions per topic 100,000 topics limitSNS integrates with a lot of AWS services Many AWS services can send data directly to SNS for notificationsAmazon SNS – How to publish Topic Publish (using the SDK) Create a topic Create a subscription (or many) Publish to the topic Direct Publish (for mobile apps SDK) Create a platform application Create a platform endpoint Publish to the platform endpoint Works with Google GCM, Apple APNS, Amazon ADM… Amazon SNS – Security Encryption: In-flight encryption using HTTPS API At-rest encryption using KMS keys Client-side encryption if the client wants to perform encryption/decryption itself Access Controls: IAM policies to regulate access to the SNS API SNS Access Policies (similar to S3 bucket policies) Useful for cross-account access to SNS topics Useful for allowing other services ( S3…) to write to an SNS topic SNS + SQS: Fan Out Push once in SNS, receive in all SQS queues that are subscribers Fully decoupled, no data loss SQS allows for: data persistence, delayed processing and retries of work Ability to add more SQS subscribers over time Make sure your SQS queue accesss policy allows for SNS to writeApplication: S3 Events to multiple queues For the same combination of: event type (e.g. object create) and prefix (e.g. images/) you can only have one S3 Event rule If you want to send the same S3 event to many SQS queues, use fan-outApplication: SNS to Amazon S3 through Kinesis Data Firehose SNS can send to Kinesis and therefore we can have the following solutions architecture:Amazon SNS – FIFO Topic FIFO = First In First Out (ordering of messages in the topic) Similar features as SQS FIFO: Ordering by message group ID (all messages in the same group are ordred) Deduplication using a Deduplication ID or Content Based Deduplication Can only have SQS FIFO queues as subscribers Limited throughput (same throughput as SQS FIFO)SNS FIFO + SQS FIFO: Fan Out In case you need fan out + ordering + deduplicationSNS – Message Filtering JSON policy used to filter messages sent to SNS topic’s subscriptions If a subscription doesn’t have a filter policy, it receives every messageSQS vs SNS vs KinesisSQS Consumer “pull data” Data is deleted after being consumed Can have as many workers (consumers) as we want No need to provision throughput Ordering guarantes only on FIFO queues Individual message delay capabilitySNS Push data to many subscribers Up to 12500000 subcribers Data is not persisted (lost if not delivered) Pub/sub Up to 100000 topics No need to provision throughput Integrates with SQS for fan-out architecture pattern FIFO capability for SQS FIFO Kinesis Standard: Pull data 2Mb per shard Enhanced-fan out: push data 2MB per shard per consumer Possibility to replay data Meant for real-time big data, analytics and ETL Ordering at the shard level Data expires aftr X days Provisioned mode or on-demand capacity modeAmazon MQ SQS, SNS are “cloud-native” service, and they’re using proprietary protocols from AWS Traditional applications running from on-premises may use open protocols such as: MQTT, AMQP, STOMP, Openwire, WSS When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ Amazon MQ = managed Apache ActiveMQ Amazon MQ doesn’t “scale” as much as SQS / SNS Amazon MQ runs on a dedicated machine, can run in HA with failover Amazon MQ has both queue feature (~SQS) and topic features (~SNS)Amazon MQ – High AvailabilitySERVERLESS AND APPLICATION SERVICES Tiered Architecture When we start deploying multiple applications, they will inevitably need to communicate with one another. There are two patterns of application communication. Evolving with queues Event-driven architecture Amazon SQS – Standard Queue Oldest offering (over 10 years old) Fully managed service, used to decouple applications Attributes: Unlimited throughput, unlimited number of messages in queue Default retention of messages: 4 days, maximum of 14 days Low latency (&lt;10 ms on publish and receive) Limitation of 256KB per message sent Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering)SQS – Producing Messages Produced to SQS using the SDK (SendMessage API) The message is persisted in SQS until a consumer deletes it Message retention: default 4 days, up to 14 days Example: send an order to be processed Order id Customer id Any attributes you want SQS standard: unlimited throughputSQS – Consuming Messages Consumers (running on EC2 instances, servers, or AWS Lambda)… Poll SQS for messages (receive up to 10 messages at a time) Process the messages (example: insert the message into an RDS database) Delete the messages using the DeleteMessage APILambda: function as a service- You are billed for the duration that a function runs- The environment has a direct memory (indirect CPU) allocation- Deployment a package with 50MB zipped and 250MB unzipped- 512 MB storage available as /tmp- Serverless application (S3, API Gateway, Lambda)- File Processing (S3, S3 event, lamba)- Database Triggers (DynamoDB, Streams, Lambda)- Serverless CROn (EventBrige/CWEvents + Lambda)- Realtime Stream Data Processing (kinesis + Lambda)Lambda in depth:- Public Lambda:![](https://data.terabox.com/thumbnail/9fb8e7f5d4deb514c210c436daa33b68?fid=4401547290288-250528-754626766169061&amp;rt=pr&amp;sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-AyXAee6ddgI%2bVvNASA%2fyuIDC100%3d&amp;expires=8h&amp;chkbd=0&amp;chkv=0&amp;dp-logid=125173180413113238&amp;dp-callid=0&amp;time=1667199600&amp;size=c1920_u1080&amp;quality=90&amp;vuk=4401547290288&amp;ft=image&amp;autopolicy=1)- Private Lambda:![](https://data.terabox.com/thumbnail/33d179bf393395941a01511d2a744ce3?fid=4401547290288-250528-1097958099823614&amp;rt=pr&amp;sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-45Ti2FPZBOQcIaJFI2fcCqPuJCU%3d&amp;expires=8h&amp;chkbd=0&amp;chkv=0&amp;dp-logid=125190523939933141&amp;dp-callid=0&amp;time=1667199600&amp;size=c1920_u1080&amp;quality=90&amp;vuk=4401547290288&amp;ft=image&amp;autopolicy=1) - Lambda run in VPC obey all VPC networking rules- Lambda security: - Lambda execution roles are IAM roles attached to lambda functions which control the permissions the lambda function receives ... - Lambda resource policy controls what services and accounts can invoke lambda functions- Lambda Logging: - Lambda uses Cloudwatch, cloudwatch logs &amp; x-ray - Logs from lambda executions - cloudwatchlogs - Metrics - invocation success/failure, retries, latency ... stored in cloudwatch - lambda can be integrated with x-ray for distributed tracing - Cloudwatch logs requires permissions via execution roleLambda in depth: Invocation Invocation: Synchronous cli / api, client communicates with APIGW , proxied to lambda function Asynchronous (typically used when AWS services invoke lambda functions) S3 isn’t waitting for any kind of response. The event is generated and S3 stops tracking If processing of the event fails, lambda will retry between 0 and 2 time (configurable). Lambda handles the retry logic The lambda function needs to be idempotent reprocessing a result should have the same end state Kinesis data stream: producers(telemetry) An execution context is the environment a lambda function run in. A cold start is a full creation and configuration including function code dowload.Cloudwatch events and eventbridge If X happen, or at Y times .. do Z Eventbrigde is cloudwatch api v2 a default event bus for account cloudwatch event has only one bus eventbrigde have additional bus rules match event route the events to +1 targets as LambdaCloudWatch Events and EventBridge have visibility over events generated by supported AWS services within an account.They can monitor the default account event bus - and pattern match events flowing through and deliver these events to multiple targets.They are also the source of scheduled events which can perform certain actions at certain times of day, days of the week, or multiple combinations of both - using the Unix CRON time expression format.Both services are one way how event driven architectures can be implemented within AWS.SQS – Multiple EC2 Instances Consumers Consumers receive and process messages in parallel At least once delivery Best-effort message ordering Consumers delete messages after processing them We can scale consumers horizontally to improve throughput of processing" }, { "title": "AWS concept and classic Solutions Architecture", "url": "/posts/aws-concept-and-classic-solutions-architecture/", "categories": "", "tags": "", "date": "2022-12-04 00:00:00 +0700", "snippet": "AWS Concept for solution architectAWS History 2002 Internally launched 2003 Amazon infrastructure is one of their core strength. Idea to market 2004 Launched publicly with SQS 2006 Re-launched ...", "content": "AWS Concept for solution architectAWS History 2002 Internally launched 2003 Amazon infrastructure is one of their core strength. Idea to market 2004 Launched publicly with SQS 2006 Re-launched publicly with SQS, S3 &amp; EC2 2007 Launched in EuropeAWS Cloud Number Facts In 2019, AWS had $35.02 billion in annual revenueAWS Cloud Use Cases AWS enables you to build sophisticated, scalable applications Applicable to a diverse set of industries Use cases include - Enterprise IT, Backup &amp; Storage, Big Data analytics - Website hosting, Mobile &amp; Social Apps - GamingAWS Global Infrastructure AWS Region AWS Availability Zones AWS Data centers AWS Edge Locations / Points of presenceAWS Regions AWS has regions all around the world Names can be us-east-1, eu-west-3… A region is a cluster of data centers Most AWS seervices are region-scopedHow to choose an AWS Region Compliance with data goverance and legal requirements data never leaves a region without your explicit permission Proximity to customers: reduced latency Avaiable services with in a Region new services and new features aren’t avaiable in every Region Pricing pricing varies region to region and is transparent in the sevice pricing pageAWS Availability Zones Each region has many availability zones (usually 3, min is 2, max is 6). ap-southeast-2a ap-southeast-2b ap-southeast-2c Each availability zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity They’re separate from each other, so that they’re isolated from disasters. They’re connected with high bandwidth ultra-low latency networkingAWS Points of Presence Amazon has 216 points of presence (205 edge locations &amp;   Regional caches) in 84 cities across 42 countries Content is delivered to end users with low latencyTour of the AWS console AWS hash Global Services Identty and Access Management(IAM) Route 53(DNS service) CloudFront (Content Delivery Network) WAF (Web application firewall) Most AWS services are region-scoped Amazone EC2 (Infrastructure as a Service) Elastic Beanstalk Lambda (Function as a Service) Rekognition (Software as a Service) Region TableDNSSEC Domain Name System Security ExtensionsDNSSEC strengthens authentication in DNS using digital signatures based on public key cryptography. With DNSSEC , it’s not DNS queries and responses themselves that are cryptographically signed, but rather DNS data itself is signed by the owner of the data.AWS OrganizationsIt’s architecture and some of the benefits for businesses managing larger numbers of AWS Accounts. The GENERAL account will become the MASTER account for the organisation We will invite the PRODUCTION account as a MEMBER account and create the DEVELOPMENT account as a MEMBER account. Finally - we will create an OrganizationAccountAccessRole in the production account, and use this role to switch between accounts. Create AWS Role Create Aws Account name Email inviteService Control Policies (SCP)I introduce service control policies - a feature of AWS Organizations which allow restrictions to be placed on MEMBER accounts in the form of boundaries.SCPs can be applied to the organization, to OU’s or to individual accounts.Member accounts can be affected, the MANAGEMENT account cannot.SCPs DON’T GIVE permission - they just control what an account CAN and CANNOT grant via identity policies.ex: apply an SCP to the PRODUCTION account to test their capabilities.Security token service(STS)AWS provides AWS Security Token Service (AWS STS) as a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users). This guide describes the AWS STS API.By default, AWS Security Token Service (AWS STS) is available as a global service, and all AWS STS requests go to a single endpoint at https://sts.amazonaws.com. sts:AssumeRoleIAM Role Temporary Security credentialsYou can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permissions to do so. security-credentials example: Revoking Temporary Credentials by curl permission revoke Policies and permissions in IAM Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies. Version – Specify the version of the policy language that you want to use. As a best practice, use the latest 2012-10-17 version. Statement – Use this main policy element as a container for the following elements. You can include more than one statement in a policy. Sid (Optional) – Include an optional statement ID to differentiate between your statements. Effect – Use Allow or Deny to indicate whether the policy allows or denies access. Principal (Required in only some circumstances) – If you create a resource-based policy, you must indicate the account, user, role, or federated user to which you would like to allow or deny access. If you are creating an IAM permissions policy to attach to a user or role, you cannot include this element. The principal is implied as that user or role. Action – Include a list of actions that the policy allows or denies. Resource (Required in only some circumstances) – If you create an IAM permissions policy, you must specify a list of resources to which the actions apply. If you create a resource-based policy, this element is optional. If you do not include this element, then the resource to which the action applies is the resource to which the policy is attached. Condition (Optional) – Specify the circumstances under which the policy grants permission. example: experience a few ways to access S3 using cross-account access example: access S3 using bucket policyService linked roleA service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role. A service might automatically create or delete the role. It might allow you to create, modify, or delete the role as part of a wizard or process in the service. Or it might require that you use IAM to create or delete the role. Resource Access ManagerResource Access Manager (RAM) allows AWS resources to be shared between AWS Accounts.It can be used to support certain common architectures such as a Shared Services VPC.Service endpoints and quotas To connect programmatically to an AWS service, you use an endpoint. Service quotas, also referred to as limits, are the maximum number of service resources or operations for your AWS account. Example: Cloudwatch A quota request template aws service-quotas list-service-quotas \\ --service-code cloudformationlist-aws-default-service-quotas --service-code &lt;value&gt; [--cli-input-json | --cli-input-yaml] [--starting-token &lt;value&gt;] [--page-size &lt;value&gt;] [--max-items &lt;value&gt;] [--generate-cli-skeleton &lt;value&gt;] [--debug] [--endpoint-url &lt;value&gt;] [--no-verify-ssl] [--no-paginate] [--output &lt;value&gt;] [--query &lt;value&gt;] [--profile &lt;value&gt;] [--region &lt;value&gt;] [--version &lt;value&gt;] [--color &lt;value&gt;] [--no-sign-request] [--ca-bundle &lt;value&gt;] [--cli-read-timeout &lt;value&gt;] [--cli-connect-timeout &lt;value&gt;] [--cli-binary-format &lt;value&gt;] [--no-cli-pager] [--cli-auto-prompt] [--no-cli-auto-prompt]request-service-quota-increase --service-code &lt;value&gt; --quota-code &lt;value&gt; --desired-value &lt;value&gt; [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton &lt;value&gt;] [--debug] [--endpoint-url &lt;value&gt;] [--no-verify-ssl] [--no-paginate] [--output &lt;value&gt;] [--query &lt;value&gt;] [--profile &lt;value&gt;] [--region &lt;value&gt;] [--version &lt;value&gt;] [--color &lt;value&gt;] [--no-sign-request] [--ca-bundle &lt;value&gt;] [--cli-read-timeout &lt;value&gt;] [--cli-connect-timeout &lt;value&gt;] [--cli-binary-format &lt;value&gt;] [--no-cli-pager] [--cli-auto-prompt] [--no-cli-auto-prompt]{ \"Quotas\": [ { \"ServiceCode\": \"xray\", \"ServiceName\": \"AWS X-Ray\", \"QuotaArn\": \"arn:aws:servicequotas:us-west-2::xray/L-C6B6F05D\", \"QuotaCode\": \"L-C6B6F05D\", \"QuotaName\": \"Indexed annotations per trace\", \"Value\": 50.0, \"Unit\": \"None\", \"Adjustable\": false, \"GlobalQuota\": false }, { \"ServiceCode\": \"xray\", \"ServiceName\": \"AWS X-Ray\", \"QuotaArn\": \"arn:aws:servicequotas:us-west-2::xray/L-D781C0FD\", \"QuotaCode\": \"L-D781C0FD\", \"QuotaName\": \"Segment document size\", \"Value\": 64.0, \"Unit\": \"Kilobytes\", \"Adjustable\": false, \"GlobalQuota\": false }, { \"ServiceCode\": \"xray\", \"ServiceName\": \"AWS X-Ray\", \"QuotaArn\": \"arn:aws:servicequotas:us-west-2::xray/L-998BFF16\", \"QuotaCode\": \"L-998BFF16\", \"QuotaName\": \"Trace and service graph retention in days\", \"Value\": 30.0, \"Unit\": \"None\", \"Adjustable\": false, \"GlobalQuota\": false } ]}Amazon Resource Names (ARNs)Questions1. What functionality does STS provide?it generates short term credentials which can be used to interact with AWS resources.2. Which of the following is NOT an example of a ‘real’ identity which can be referenced by ARNs in resource policies?IAM Groups3. How are role sessions revoked (Choose one)A inline policy is added to the role with an explicit deny for role assumptions before .. NOW4. If an SCP on the AWS account allows S3, a managed policy attached to your identity allows S3 and an inline policy denys S3.. what is your effective access (Choose one)Denied5.An SCP on account B denies S3. A resource policy on the bucket in account B allows account A. An identity policy on Bob in Account A allows access to S3. What is the effective access when Bob accesses the bucket in account B (choose one)HashSSL/TLSDigital SignalAWS Public vs Private ServicesAWS Default Virtual Private Cloud (VPC)A default VPC is created once per region when an AWS account is first created.There can only be one default VPC per region, and they can be deleted and recreated from the console UI .They always have the same IP range and same ‘1 subnet per AZ’ architecture.This lesson details and demos the functionality of a default VPC.Elastic Compute Cloud (EC2) Basics IAAS: virtual services Private service by default - uses VPC networkingIAM SectionIAM: Users &amp; Groups IAM = Identity and Access Management, Global service Root account created by default, shouldn’t be used or shared Users are people within your organization, and can be grouped Groups only contain users, not other groups Users don’t have to belong to a group, and user can belong to multiple groupsIAM: Permissions Users or Groups can be assigned JSON documents called policies These policies define the permissions of the users In AWS you apply the least privilege principle don’t give more permission than a user needs{\"Version\": \"2012-10-17\",\"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"elasticloadbalancing:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" } ]}IAM Policies inheritanceIAM Policies Structure Json structure version: policy language version, alway include “2012-10-17” ID: an identifier for the policy Statement: one or more individual statements Sid Effect Principal Action Resource Condition IAM – Password Policy Strong passwords: higher security for your account In AWS, you can setup a password policy Set a minimum password length Require specific character types includng uppercase letter lowercase letters numbers non-alphanumeric characters Allow all IAM users to change their own passwords Require users to change their password after some time Prevent password re-use Multi Factor Authentication - MFA Users have access to your account and can possibly change configurations or delete resources in your AWS account You want to protect your Root Accounts and IAM users MFA = password you know + security device you own Main benefit of MFA: If a password is stolen or hacked the password is not compromised MFA virtual app or USB hardware Factor Hardware Key Fob MFA Device, Hardware Key Fob MFA DeviceHow can users access AWS To access AWS, you have three options AWS Management Console AWS Command line interface AWS software developer kit Access keys are generated through the AWS Console, password don’t share Users manage their own access keys Access Key ID Secret Access Key Example Access key ID: AKIASK4E37PV4983d6C Secret Access Key: AZPN3zojWozWCndIjhB0Unh8239a1bzbzO5fqqkZq What’s the AWS CLI? A tool that enables you to interact with AWS services using commands inyour command-line shell Direct access to the public APIs of AWS services You can develop scripts to manage your resources It’s open-source https://github.com/aws/aws-cli Alternative to using AWS Management ConsoleWhat’s the AWS SDK? AWS Software Development Kit (AWS SDK) Language-specific APIs (set of libraries) Enables you to access and manage AWS servicesprogrammatically Embedded within your application Supports SDKs (JavaScript, Python, PHP, .NET, Ruby, Java, Go, Node.js,C++) Mobile SDKs (Android, iOS, …) IoT Device SDKs (Embedded C, Arduino, …) Example: AWS CLI is built on AWS SDK for PythonIAM Roles for Services Some AWS service will need to perform actions on your behalf To do so, we will assign permissions to AWS services with IAM Roles Common roles: EC2 Instance Roles Lamba function roles Roles for CloudFormation IAM Security Tools IAM Credentials Report: A report that list all your account’s users and the status of their various credentials IAM Access Advisor (user-level): Access advisor shows the service permissions granted to a user and when those. You can use this information to revise your policies.IAM Guidelines &amp; Best Practices Don’t use the root account except for AWS account setup One physical user = One AWS user Assign user to groups and assign permissions to groups Create a strong password policy User and enforce the use of Multi Factor Authentication(MFA) Create and use Roles for giving permissions to AWS services User Access keys for programmactic access Audit permissions of your account with the IAM credentials report Never share IAM users &amp; Access KeysIAM Section – Summary Users: mapped to a physical user, has a password for AWS console Groups: contains users only Policies: JSON document that outlines permissions for users or groups Roles: for EC2 instances or AWS services Security: MFA + Password policy Acess keys: access AWS using the CLI or SDK Audit: IAM credential reports &amp; IAM Access AdvisorEC2 BasicsAmazon EC2 EC2 is one of the most popular of AWS offering EC2 = Elastic Compute Cloud = IAAS Renting virtual machines EC2 Storing data on virtual drives EBS Distributing load acroos machines ELB Scaling the services using an auto-scaling group ASG Knowing EC2 is fundamental to understand how the Cloud worksEC2 sizing &amp; configuration options Operating System (OS): Linux, Windows, MAC OS How much compute power and core: CPU How much random-access memory: RAM How much storage space: Network-attached: EBS &amp; EFS Harkware: EC2 Instance Store Network card speed of the card, public ip address Firewall rules: security group Boostrap script: EC2 User DataEC2 User Data It is possible to bootstrap our instances using an EC2 User data script bootstrapping means launching commands when a machine starts That script is only run once at the instace first start EC2 user data is used to automate boot tasks such as Install updates Install software Download common files from the internet Anything you can think of The EC2 User data script runs with the root user Example: Web server is launched using EC2 user dataEC2 Instance Types - Overview References to EC2 instances example: m5.2xlarge m: instance class 5: generate 2xlarge: size within the instance class EC2 Instance Types – General Purpose Great for a diversity of workloads such as web servers or code repositories Balance between: Compute Memory Networking Ex: Mac, t4g, t3, t3a, t2, M6g, M6I, M6a, M5, M5a, M5n, M5zn, M4, A1EC2 Instance Types – Compute Optimized Great for compute intensive tasks that require high performance processors: Batch processing workload Media transcoding High performance web servers High performance computing HPC Scientific modeling &amp; machine learning Dedicated gaming servers Ex: C7g, C6g, C6gn, C6I, C6a, Hpc6a, C5, C5a, C4EC2 Instance Types - Memory Optimized Fast performance for workloads that process large data set in memory Use cases: High performance, relational/non-relational databases Distributed web scale cache store In-memory database optimized for BI Applications performing real-time processing of big unstructured data Ex: R6a, R6g, R6I. R5, R5a, R5b, R5n, R4, X2gd, X2ldn, X2ledn, X2lezn, X1, High Memory, z1dEC2 Instance Types – Storage Optimized Great for storage-intensive tasks that require high, sequential read and writeaccess to large data sets on local storage Use cases: High frequency online transaction processing (OLTP) systems Relational &amp; NoSQL databases Cache for in-memory databases (for example, Redis) Data warehousing applications Distributed file systems Ex: Im4gn, Is4gen, I4i, I3, I3en, D2, D3, D3en, H1EC2 Instance Types – Accelerated Computing Hardware accelerators, co-processors, perform functions, floating point number calculations, graphics processing, data pattern matching Use case: Machine learning, high performace computing, computanional fluid dynamics Ex: P4, P3, P2, DL1, Trn1, Irn1, Inf1, G5, G5g, G4gn, G4ad, G3, F1, VT1Introduction to Security Groups Security groups are the fundamental of network security in AWS They control how traffic is allowed into or out of our EC2 instances Security groups only contain allow rules Security groups rules can reference by IP or by security groupSecurity Groups Deeper Dive Security groups are acting as a “firewall” on EC2 instances They regulate: Access to Ports Authorised IP ranges: IPv4 and IPv6 Control of inbound network (from other to the instance) Control of outbound network (from the instance to other)Security Groups Good to know Can be attaced to multiple instances Locked down to a region / VPC combination Does live “outside” the EC2 - if traffic is blocked the EC2 instance won’t see it It’s good to maintain one separate security group for SSH access If your application is not accessible (time out), then it’s a security group issue If your application gives a “connection refused” error, then it’s an application error or it’s not lauched All inbound traffic is blocked by default All outbout traffic is authorised by defaultClassic Ports to know 22 = SSH 21 = FTP 22 = SFTP 80 = HTTP 443 = HTTPS 3389 = RDP Ec2 instance connect is popular tool for ssh, ssh commancd is avaiable for all opeartion but withou windows version less than 10, so on you need PuttyEC2 Instance Connect Connect to your EC2 instance within your browser No need to use your key file that was downloaded The “magic” is that a temporary key is uploaded onto EC2 by AWS Work only out of the box Amazon Linux 2 Need to make sure the port 22 is still opened!EC2 Instances Purchasing Options On-demand Instances: short workload, predictable pricing, pay by second Reserved (1 &amp; 3 years) Reserved instances: long workloads Convertible Reserved Instances: long workloads with flexible instances Savings plans (1 &amp; 3 years): Commitment to an amount of usage, long workload Spot Instances: short workloads, cheap, can lose instances (less reliable) Dedicated Hosts: Book an entire physical server, control instance placement Dedicated Instances: No other customers will share your hardware Capacity Reservations: Reserve capacity in a specific AZ for any durationEC2 On Demand Pay for what you use Linux or WIndows: Bulling per second, after the first minute All other operating systems: billing per hour Has the highest cost but no upfront payment No long-term commitment Recommended for short-term and un-interupted workloads where you can’t predict how the application will behaveEC2 Reserved Instances Up to 72% discounr compared to On-demand You reserve a specific instance attributes (Instance Type, Region, Tenancy, OS) Reservation Period - 1 year(+discount) or 3 years (+++discount) Payment Options - No upfront(+), Partial upfront(++), All upfront(+++) Reserved Instance’s Scope - Regional or Zonal (reserve capacity in an AZ) Recommended for steady-state usage applications (think database) You can buy and sell in the Reserved Instance marketplace Convertible Reserved Instance Can change the EC2 instance type, instance family, OS, scope and tenacy Up to 66% discountEC2 Savings Plans Get a discount based on long-term usage (up to 72% - same as RIs) Commit to certain type of usage ($10/hour for 1 or 3 years) Usage beyond EC2 savings plans is billed at the On-Demand price Locked to a specific instance family &amp; AWS region (e.g, M5 in us-east-1) Flexible across: Instance Size OS Tenancy (Host, Dedicated, Default)EC2 Spot Instances Can get a discount of up to 90% compared to On-demand Instances that you can “lose” at any point of time if your max price is less than the current spot price The Most cost-efficient instances in AWS Useful for workloads that are resilient to failure Batch jobs Data analysis Image processing Any distributed workloads Workloads with a flexible start and end time Not suitable for critical jobs or databasesEC2 dedicated hosts A physical server with EC2 instance capacity fully dedicated to your use Allows you address compliance requirements and use your existing server bound software licenses (per-socket, per-core, pe—VM software licenses) Purchasing Options: On-demand: pay per second for active Dedicated Host. Reserved: 1 or 3 years (No Upfront, Partial Upfront, All Upfront) The most expensive option Useful for software that have complicated licensing model: BYOL Bring your own license Or for companies that have strong regulatory or compliance needs.EC2 Dedicated Instances Instances run on hardware that’s dedicated to you May share hardware with other instances in same account No control over instance placement (Can move hardware after Stop/Start)EC2 Capacity Reservations Reserve On-demand instance capacity in a specific AZ for any duration You always have access to EC2 capacity when you need it No time commitment, no billing discount Combine with Regional reserved instances and savings plans to benefit from billing discounts You’re charged at On-Demand rate whether you run instances or not Suitable for short-term, uninterrupted workloads that needs to be in a specific AZWhich purchasing option is right for me? On demand: coming and staying in resort whenever we like, we pay the full price Reserved: like planing ahead and if we plan to stay for a long time, we may get a good discount. Saving Plans: pay a cerain amount per hour for certain period and stay in any room type Spot Instance: The hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms. You can get kicked out at any time Dedicated Hosts: We book an entire building of the resort Capacity Reservations: you book a room for a period with full price even you don’t stay in itPrice Comparison Example – m4.large – us-east-1EC2 Spot Instance Requests Can get a discount of up to 90% compared to On-demand Define max spot price and get the instance while current spot price &lt; max The hourly spot price varies based on offer and capacity If the current spot price &gt; your max price you can choose to stop or terminate your instance with a 2 minutes grace period Other strategy: Spot Block “block” spot instance during a specified time frame (1 to 6 hours) without interuptions In rare situations, the instance may be reclaimed Used for batch jobs, data analysis, or workloads that are resilient to failures. Not greate for critical jobs or databasesHow to terminate Spot Instances?Spot Fleets Spot Fleets = set of Spot instances + (optional) On-Demand instances The spot fleet will try to meet the target capacity with price constraints Define possible launch pools: instance type (m5.large), OS, Avaiability Zone Can ave multiple launch pools, so that the fleet can choose Spot Fleet stops launching instances when reaching capacity or max cost Strategies to allocate Spot Instances lowestPrice: from the pool with the lowest price (cost optimizations, short workload) diversified: distributed across all pool (great for availability, long workloads) capacityOptimized: pool with the optimal capacity for the number of instances. EC2 Section – Summary EC2 Instance: AMI (OS) + Instance Size (CPU + RAM) + Storage +security groups + EC2 User Data Security Groups: Firewall attached to the EC2 instance EC2 User Data: Script launched at the first start of an instance SSH: start a terminal into our EC2 instances EC2 Instance Role: link to IAM roles Purchasing Options: On-Demand, Spot, Reversed (Standard + Convertible + Scheduled), Dedicated Host, Dedicated InstanceEC2 – Associate Private vs Public IP (IPv4) Networking has two sorts of IPs. IPv4 and IPv6: Ipv4: 1.160.10.240 IPv6: 3ffe:1900:4545:3:200:f8ff:fe21:67cf In this course, we will only be using IPv4. Ipv4 is still the most common IPv6 is newer and solves problems for the internet of Thing (IoT) IPv4 allow for 3.7 bilion different address in the public space IPv4: [0-255].[0-255].[0-255].[0-255].Private vs Public IP (IPv4) Public IP public IP means the machine can be identified on the internet Must be unique across the whole web Can be geo-located easily Private IP: Private Ip means the machine can only be identified on a private network only The IP must be unique across the private network But two different private networks (two companies) can have the same IPs Machine connect to WWW using a NAT + internet gateway Only a specified range of IPs can be used as private IP Elastic IPs When you stop and then start an EC2 instance, it can change its public IP. If you need to have a fixed public IP for your instance, you need an Elastic IP An Elastic IP is a public IPv4 IP you own as long as you dont delete it You can attach it to one instance at a timeElastic IP With an Elastic IP address, you can mask the failure of an instance or software by rapidy remapping the address to another instace in your account. You can only have 5 Elastic IP in your account Overall, try to avoid using Elastic IP: They often reflect poor architectural decisions Instead, use a random public IP and register a DNS name to it Or as we’ll see later, use a load balancer and dont use a public IP In AWS EC2 – Hands On By default, your EC2 machine comes with: A private IP for the internal AWS Network A public IP, for the WWW When we are doing SSH into our EC2 machines: We can’t use a private IP, because we are not in the same network We can only use the public IP If your machine is stopped and then started the public IP can changePlacement Groups Cluster Pros: Greate network (10 Gbs bandwith between instances with Enhanced Networking enabled) Cons: If the rack fails, all instances fail at the same time Use case: Big Data job that needs to complete fast Application that needs extremely low latency and high network throughput Placement Groups Spread Pros: Can span across avaiability zones (AZ) Reduced risk is simultaneous failure EC2 Instances are on different phusical hardware Cons: Limited to 7 instances per AZ per placement group Use case: Application that needs to maximize high availability Critical Applications where each instance must be isolated from failure from each other Placements Groups Partition Up to 7 partions per AZ Can span across multiple AZs in the same region Up to 100s of EC2 instances The instances in a partion do not share racks with the instances in the other partitions A partition failure can affect many EC2 but won’t affect other partitions EC2 instances get access to the partition information as metadata Use cases: HDFS, HBase, Cassandra, KafkaElastic Network Interfaces (ENI) Logical component in a VPC that represents a virtual network card The ENI can have the following attributes: primary private IPv4, one or more secondary IPv4 One Elastic IP (IPv4) per private IPv4 One public IPv4 One or more security groups A MAC address You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover Bound to a specific availability zone (AZ)Ec2 Hibernate We know we can stop, termiate instances stop - the data on disk (EBS) is kept intact in the next start terminate - any EBS volumes (root) also set-up to be destroyed is lost On start, the following happens: First start: the OS boots &amp; the EC2 User Data script is run Following starts the OS boots up Then your application starts, caches get warmed up, and that can take time! EC2 Hibernate Introducing Ec2 Hibernate The in-memory (RAM) state is preserved The instance boot is much faster! Under the hood: the RAM state is written to a file in the root EBS volume The root EBS volume must be encrypted Long-running processing Saving the RAM state Services that take time to initializeEC2 Hibernate – Good to know - Accelerate EC2 Supported Instance Families: C3, C4, C5, I3, M3, M4, R3, R4, T2, T3, … Instance RAM size: must be less than 150 GB Instance Size: not supported for bare metal instances AMI: Amazon Linux 2, Linux AMI, Ubuntu, RHEL, CentOS &amp; Windows… Root volume: must be EBS, encrypted, not instance store and large Available for On-Demand, Reserved and Spot instances An instance can NOT be hibernated more than 60 daysEC2 Nitro Underlying platform for the next generation of EC2 instances New virualization technology Allows for better performance Better networking options (enhanced networking, HPC, IPv6) Higher speed EBS (Nitro is necessary for 64.000 EBS IOPS - max 32000 on non-Nitro) Better underlying security Instance types example: Virtualized: AI, C5, C5a, C5ad, C5d, C5n, C6g, C6gn, D3, D3en, G4, I3en, Infi, M5a, M5ad, M5d, M5dn, M5n, …. Bare metal: a l.metal, c5.metal, c5n.metal, c6g.metal. c6gd.metal… EC2 – Understanding vCPU Multiple threads can run on one CPU (multithreading) Each thread is represented as a vitual CPU (vCPU) Example: m5.2xlarge 4 CPU 2 threads per CPU =&gt; 8 vCPU in total EC2 – Optimizing CPU options: Reservations EC2 instances come with a combination of RAM and vCPU But in some cases, you may want to chage the vCPU options: of CPU cores: you can decrease it (helpful if you need high RAM and low number of CPU) - to decrease licensing costs of threads per core: disable multithreading to have I thread per CPU - helpful for high performance computing (HPC) workloads Only specified during instance launchEC2 – Capacity Reservations: Reservations Capacity reservations ensure you have EC2 capacity when needed Manual or planned end date for the reservation No need for 1 or 3 year commitment Capacity access is immediate, you get billed as soon as it starts Specify: The Availability Zone in which to reserve the capacity The number of instances for which to reserve capacity The instance attributes, including the instance type tenancy, and platform/OS Combine with REserved Instances and Savings Plans to do cost savingEC2 Instance Storage SectionWhat’s an EBS Volume? An EBS (Elastic Block Store) Volume is a network drive you can attack to your instances while they run It allows your instances to persist data, even after their termination They can only be mounted to one instance at a time (at the CCP level) They are bound to a specific availability zone Analogy Think of them as a “network USB stick” Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per monthEBS Volume It’s a network drive It uses the network to communicate the instance, which means there might be a bit of latency It can be detached from an EC2 instance and attached to another one quickly It’s locked to an Availability Zone An EBS volume in us-east-1 a cannot be attached to us-east-1 b To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GBs, and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time EBS – Delete on Termination attribute Controls the EBS behaviour when an EC2 instance terminates By default, the root EBS volume is deleted (attribute enabled) By default, any other attached EBS volume is not deleted (attribute disabled) This can be controlled by thw AWS console / AWS CLI Use case: preserve root volume when instance is terminatedEBS Snapshots Make a backup (snapshot) of your EBS volume at a point in time Not necessary to detach volume to do snapshot, but recommended Can copy snapshots across AZ or RegionEBS Snapshots Features EBS Snapshot Archive Move a snapshot to an “archive tier” that is 75% cheaper Takes within 24 to 72 hours for restoring the archive Recycle Bin for EBS Snapshots Setup rules to retain deleted snapshots so you can recover them after an accidental deletion Specify retention (from 1 day to 1 year) AMI Overview AMI = Amazone Machine Image AMI are a customization of an EC2 instance You add your own software, configuration, operating system, monitoring… Faster boot / configuration time because all your software is pre-packaged AMI are built for a specfic region (and can be copied across regions) You can launch EC2 instances from: A Public AMI: AWS provided Your own AMI: you make and maintain them yourself An AWS Marketplace AMI: an AMI someone else made (and potentially sells) AMI Process (from an EC2 instance) Start an EC2 instance and customize it Stop the instance (for data integrity) Build an AMI - this will also create EBS snapshots Launch instances from other AMIsEC2 Instance Store EBS volumes are network drives with good but “limited” performance If you need a high performance hardware disk, use EC2 Instance Store Better I/O performance EC2 Instance Store lose their storage if they’re stopped (ephemeral) Good for buffer / cache / scratch data / temporary content Risk of data loss if hardware fails Backups and replication are your responsibilityEBS Volume Types EBS Volumes come in 6 types gp2 / gp3 (SSD): General purpose SSD volumne that balances price and performance for a wide variety of workloads io1 / io2 (SSD) Highest-performance SSD volume for mission critical low latency or high throughput workloads stl (HDD): Low cost HDD volume designed for frequency accessed, throughput-intensive workloads scl (HDD): Lowest cost HDD volume designed for less frequently accessed workloads EBS Volumes are characterized in Size Throughput IOPS (I/O Ops Per Sec) When in doubt always consult the AWS documentation Only gp2/gp3 and io1/io2 can be used as boot volumesGeneral Purpose SSD Cost effective storage, low-latency System boot volumes, Virtual desktops, Development and test environments 1 GiB - 16 TiB gp3: Baseline of 3,000 IOPS and throughput of 125 MiB/s Can increase IOPS up to 16,000 and throughput up to 1000 MiB/s independently gp2: Small gp2 volumes can burst IOPS to 3,000 Size of the volume and IOPS are linked, max IOPS is 16,000 3 IOPS per GB, means at 5,334 GB we are at the max IOPS Provisioned IOPS (PIOPS) SSD Critical business applications with sustained IOPS performance Or applications that need more than 16000 IOPS Great for databases workloads (sensitive to storate perf and consistency) io1/io2 (4Gib - 16TB) Max PIOPS: 64,000 for Nitro EC2 instances &amp; 32,000 for other Can increase PIOPS independently from storage size io2 have more durability and more IOPS per GiB (at the same price as io1) io2 Block Express (4 GiB – 64 TiB): Sub-millisecond latency Max PIOPS: 256,000 with an IOPS:GiB ratio of 1,000:1 Supports EBS Multi-attachHard Disk Drives (HDD) Cannot be a boot volume 125 GB to 16 TB Throughput Opitimed HDD (stl) Big Data, Data warehouse, Log processing Max throughput 500 Mib/s - max IOPS 500 Cold HDD (scl): For data that is infrequently accessed Scenarios where lowest cost is important Max throughput 250 MiB/s - max IOPS 250 EBS Multi-Attach – io1/io2 family Attach the same EBS volume to multiple EC2 instances in the same AZ Each instance has full read &amp; write permissions to the volume Use case: Archieve higher application availability in cluster Linux application Application must manage concurent write operations Must use a file system that’s cluster-aware (not XFS, EX4, etc…)EBS Encryption When you create an encrypted EBS volume, you get the following Data at rest is encrypted inside the volume All the data in flight moving between the instance and the volume is encrypted All snapshots are encrypted All volumes created from the snapshot Encryption and decryption are handled transparently (you have nothing to do) Encryption has a minimal impact on latency EBS encryption leverages keys from KMS (AES-256) Copying an unencrypted snapshot allows encryption Snapshots of encrypted volumes are encryptedEncryption: encrypt an unencrypted EBS volume Create an EBS snapshot of the volume Encrypt the EBS snapshot ( using copy ) Create new ebs volume from the snapshot (the volume will also be encrypted) Now you can attach the encrypted volume to the original instanceAmazon EFS – Elastic File System Managed NFS (network file system) that can be mounted on many EC2 EFS works with EC2 instances in multi-AZ Highly available, scalable, expensive (3x gp2), pay per useAmazon EFS – Elastic File System Use cases: content management, web serving, data sharing, Wordpress Use NFSv4.1 protocol Uses security group to control access to EFS Compatible with Linux bases AMI (not windows) Encryption at rest using KMS POSIX file system (~Linux) that has a standard file API File system scales automatically, pay per use, no capacity planning!EFS – Performance &amp; Storage Classes EFS Scale 1000s of concurrent NFS clients, 10 GB+/s throughput Grow to Petabyte-scale network file system, automatically Perfornmance mode (set at EFS creation time) General purpose(default): latency sensitive use cases (web serve, CMS, etc…) Max I/O - higher latency, throughput, highly parallel (big data, media processing) Throughput mode Bursting (1TB = 50Mib/s + brust of up to 100MiB/s) Provisioned: set your throughput regardless of storage size, ex: 1Gib/s for | TB storage EFS – Storage Classes Storage Tiers (lifecycle management features - move file after N days) Standard: for frequently accessed files Infrequent access (EFS-IA): cost to retrieve files, lower price to store. Enable EFS-IA with a lifecycle policy Availability and durability Regional: Multi-AZ, great for prod One Zone: One AZ, greate for dev, backup enabled by default, compatible with IA (EFS one Zone-IA) Over 90% in cost savingsEBS vs EFS – Elastic Block Storage EBS volumes… Can be attached to only one instance at a time Are locked at the Availability Zone (AZ) level GP2: IO increases if the disk size increases IO1: Can increase IO independently To migrate an EBS volume across AZ Take a snapshot Restore the snapshot to another AZ EBS backups use IO and you shouldn’t run them while your application is handling a lot of traffic Root EBS Volumes of instances get terminated by default if the EC2 instance get terminated (you can disable that) EBS vs EFS – Elastic File System Mounting 100s of instances across AZ EFS share website files (WordPress) Only for Linux Instances (POSIX) EFS has a higher price point than EBS Can leverage EFS-IA for cost savings Availability Zone 1 Availability Zone 2 Remember: EFS vs EBS vs Instance StoreAWS Fundamentals – Part II Load Balancing, Auto Scaling Groups and EBS VolumesScalability &amp; High Availability Scalability means that an application / system can handle greater loads by adapting There are two kinds of scalability Vertical scalability Horizontal scalability Scalability is linked but different to high availability Let’s deep dive into the distinction, using a call center as an exampleVertical saclability Vertically scalability means increasing the size of the instance For example, you application runs on a t2.micro Scaling that application vertically means running it on a t2.large Vertical scalability is very common for non distributed systems, such as a database. RDS, ElastiCache are service that can scale vertically There’s uusually a limit to how much you can vertically scale (hardware limit)Horizontal Scalability Horizontal scalability means increasing the number of instances / systems for your application Horizontal scaling implies distributed systems. This is very common for web applications / modern applications It’s easy to horizontally scale thanks the cloud offerings such as Amazon EC2High Avaiability High availability usually goes hand in hand with horizontal scaling High Avaiability means running your application / system in at least 2 data centers(==AZ) The goal of high availability can be passive (for RDS Multi AZ for example) The high availability can active (for horizontal scaling)High Availability &amp; Scalability For EC2 Vertical Scaling: increase instance size (= scale up / down) From: t2.nano - 0.5G of RAM, 1 vCPU To: u-12tb1.metal – 12.3 TB of RAM, 448 vCPUs Horizontal scaling: Increase number of instances (scale out / in) Auto Scaling Group Load Balancer High Availability: Run instances for the same application across multi AZ Auto Scaling group multi AZ Load Balancer multi AZ What is load balancing? Load Balances are servers that forward traffic to multiple servers downstreamWhy use a load balancer? Spread load across multiple downstream instances Expose a single point of access DNS to your application Seamlessly handle failures of downstream instances Do regular health checks to your instances Provide SSL termination HTTPS for your websites Enforce stickiness with cookies High availability across zones Separate public traffic from private trafficWhy use an Elastic Load Balancer? An Elastic Load Balancer is a managed load balancer AWS gurantees that it will be working AWS takes care of upgrades, maintenace, high availability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end It is integrated with many AWS offering / services EC2, EC2 auto scaling groups, amazone ECS AWS Certificate Manager (ACM), CloudWatch Route 53, AWS WAF, AWS Global Accelerator Why use an Elastic Load Balancer? An Elastic load balancer is a managed load balancer AWS guarantees that it will be wroking AWS takes care of upgrades, maintaince, high vailability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end It is integrated with many AWS offering / services EC2, EC2 Auto scaling groups, amazon ECS AWS Certificate Manager (ACM), CloudWatch Route 53, AWS WAF, AWS Global Accelerator Health Checks Health checks are crucial for load balancers They enable the load balancer to know if instances it forwards traffic to are available to reply to requests The health check is done on a prt and a route (/health is common) If the response is not 200 (OK), then the instance is unhealthyTypes of load balancer on AWS AWS has 4 kind of managed load balancers Classic Load Balancer (v1 - old generation) - 2009 - CLB HTTP, HTTPS, TCP, SSL (secure TCP) Application load balancer (v2-new generation) - 2016 - ALB HTTP, HTTPS, Web Socket Network load balancer (v2 - new generation) - 2017 - NLB TCP, TLS (secure TCP), UDP Gateway load balancer - 202 -GWLB Operates at layer 3 (Network layer) - IP Protocol Overall, It is recommended to use the newer generation load balancers as they provide more features Some load balancers can setup as internal or external ELBsClassic Load Balancers (v1) Supports TCP (layer 4), HTTP &amp; HTTPS (layer 7) Health checks are TCP or HTTP based Fixed hostname XXX.region.elb.amazoneaws.comApplication Load Balancer (v2) Application load balancers is layer 7 (HTTP) Load balancing to multiple applications on the same machine Support for HTTP/2 and websocket Support redirects (from HTTP to HTTPS for example)Application load balancer (v2) Routing tables to different target groups Routing based on path in URL based on Hostname in URL based on query string headers ALB are a great fit for micro services &amp; container-based application Has a port mapping feature to redirect to a dynamic port in ECS In comparison, we’d need multiple classic load balancer per applicationApplication Load Balancer (v2) Target Groups EC2 instances (can be managed by an Auto Scaling Group) - HTTP ECS tasks (managed by ECS itself) - HTTP Lamba functions - HTTP request is transalted into a JSON event IP Addresses - must be private IPs ALB can route to multiple target groups Health checks are at the target group level Application Load Balancer (v2) Good to Know Fixed hostname (XXX.region.elb.amazoneaws.com) The application servers dont see the IP of the client directly The true IP of the client is inserted in the header X-FORWARDED-For We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)Network Load Balancer (v2) Network load balancers (layer 4) allow to: Forward TCP &amp; UDP traffic to your instances Handle milions of request per seconds Less latency ~ 100ms (vs 400 ms for ALB) NLB has one static Ip per AZ, and supports assigning Elastic IP(helpful for whitelisting specific IP) NLB are used for extreme performance, TCP or UDP traffic Not included in the AWS free tierNetwork load balancer (v2) TCP (layer 4) based trafficNetwork Load Balancer – Target Groups EC2 instance IP Address - must be private IPs Application load balancerGateway Load Balancer Deploy, scale, and manage a fleet of 3 party network virtual appliances in AWS Example: Firewalls, Instrusion detection and prevention systems, deep packet inspection systems, payload manipulation, … Operates at layer 3 (Network layer) Ip packets Combines the following functions: Transparent Network Gateway: single entry/exit for all traffic Load Balancer: distributes traffic to your virtual appliances Uses the Geneve protocol on port 6081GAteway load balancer - Target Groups EC2 instances IP Addresses - must be private IPsSticky Sessions (Session Affinity) It is possible to implement stickiness so that the same client is always redirected to the same instances behind a load balancer This works for classic load balancers &amp; application load balancers The “cookie” used for stickiness has an expiration date you control Use case: make sure the use doesn’t lost his session data Enabling stickiness may bring imbalance to the load over the backend EC2 instancesSticky Sessions - Cookie Names Application-based Cookies Custom cookie Can include any custom attributes required by the application Cookie name must be specified individually for each target group Don’t use AWSALB.AWSALBTB (reserved for use by the ELB) Application cookie Generated by the load balancer Cookie name is AWSALBAPP Duration based cookies Cookie generated by the load balancer Cookie name is AWSALB for ALB, AWSELB for CLB Cross-Zone Load Balancing With Cross-Zone Load Balancing: Each load balancer instance distributes evently across all registered instances in all AZ Without Cross zone load balancing: Request are distributed in the instances of the node of the elastic load balancerCross-Zone Load Balancing Application load balancer Always on (can’t be disabled) No charges for inter AZ data Network Load Balancer Disabled by default you pay charge($) for inter AZ data if enabled Classic Load Balancer Disabled by default No charges for inter AZ data if enabled High Availability (HA)aims to ensure an agreed level of operational performance ussually uptime, for a higher than normal period. 99.9%(Three 9’s) = 8.77 hours p/year downtime 99.999%(Five 9’s) = 5.26 minutes p/year downtime aims zero downtime Minimise any outagesFault-Tolerance (FT) is the property that enables a system to continue operating properly in the event of the failure of some (one or more faults within) of its components Systems design with plan fault tolerance Operate through faultsDisater recovery(DR) a set of polocies, tools and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-inclued disaster Pre planning ——————–&gt; DR Process Used when these don’t work # Classic Solutions Architecture Section Introduction These solutions architectures are the best part of this course Let’s understand how all the technologies we’ve seen work together This is a section you need to be 100% comfortable with We’ll see the progression of a Solution’s architect mindset through many sample case studies: WhatIsTheTime.Com MyClothes.Com MyWordPress.Com Instantiating applications quickly Beanstalk Stateless Web App: WhatIsTheTime.com WhatIsTheTime.com allows people to know what time it is We don’t need a database We want to start small and can accept downtime We want to fully scale vertically and horizontally, no downtime Let’s go through the Solutions Architect journey for this app Let’s see how we can proceed! Stateless web app: What time is it? Starting simple Stateless web app: What time is it? Scaling verticallyStateless web app: What time is it? Scaling horizontallyStateless web app: What time is it? Scaling horizontallyStateless web app: What time is it? Scaling horizontally, adding and removing instancesStateless web app: What time is it? Scaling horizontally, with a load balancerStateless web app: What time is it? Scaling horizontally, with an auto-scaling groupStateless web app: What time is it? Making our app multi-AZMinimum 2 AZ =&gt; Let’s reserve capacityIn this lecture we’ve discussed… PUblic vs Private IP and EC2 instances Elastic IP vs Route 53 vs Load Balancers Route 53 TTL, A records and Alias Records Maintaining EC2 instances manually vs Auto Scaling Groups Multi AZ to survive disasters ELB health Checks Security GRoup Rules Reservation of capacity for costing saving when possible We’re considering 5 pillars for a well architected application: costs, performance, reliability, security, operational excellenceStateful Web App: MyClothes.com MyClothes.com allows people to buy clothes online. There’s a shopping cart Our website is having hundreds of users at the same time We need to scale, maintain horizontal scalability and keep our web application as stateless as possible Users should not lose their shopping cart Users should have their details (address, etc) in a database Let’s see how we can proceed!Stateful Web App: MyClothes.comStateful Web App: MyClothes.com Introduce Stickiness (Session Affinity)Stateful Web App: MyClothes.com Introduce User CookiesStateful Web App: MyClothes.com Storing User Data in a databaseStateful Web App: MyClothes.com Scaling ReadsStateful Web App: MyClothes.com Scaling Reads (Alternative) – Write ThroughStateful Web App: MyClothes.com Multi AZ – Survive disastersStateful Web App: MyClothes.com Security GroupsIn this lecture we’ve discussed… 3-tier architectures for web applications ELB sticky sessions Web clients for storing cookies and making our web app sateless ElasticCache For storing sessions (alternative: dynamoDB) For caching data from RDS Multi AZ RDS For storing user data Read replicas for caling reads Multi AZ for disaster recovery Tigh Security with security groups referencing each otherStateful Web App: MyWordPress.com We are trying to create a fully scalable WordPress website We want that website to access and correctly display picture uploads Our user data, and the blog content should be stored in a MySQL database Let’s see how we can achieve this!Stateful Web App: MyWordPress.com RDS layerStateful Web App: MyWordPress.com Scaling with Aurora: Multi AZ &amp; Read ReplicasStateful Web App: MyWordPress.com Storing images with EBSStateful Web App: MyWordPress.com Storing images with EBSStateful Web App: MyWordPress.com Storing images with EFSIn this lecture we’ve discussed… Aurora Database to have easy Multi-AZ and Read-Replicas Storing data in EBS (single instance application) Vs Storing data in EFS (distributed application)Instantiating Applications quickly When launching a full stack (EC2, EBS, RDS), it can take time to: Install applications Insert initial (or recovery) data Configure everything Launch the application We can take advantage of the cloud to speed that up!Instantiating Applications quickly EC2 Instances: Use a Golden AMI: Install your applications, OS dependencies etc.. beforehand and launch your EC2 instance from the Golden AMI Bootstrap using User Data: For dynamic configuration, use User Data scripts Hybrid: mix Gloden AMI and user Data (Elastic Beanstalk) RDS Databases: Restore from a snapshot the database will have schemas and data ready! EBS Volumes: Restore from a snapshot: the disk will already be formatted and have data! Typical architecture: Web App 3-tierDeveloper problems on AWS Manging infrastructure Deploying Code Configuring all the databases, load balancers, etc Scaling concerns Most web apps have the same architecture (ALB + ASG) All the developers want is for their code to run! Possibly, consistency across different applications and environmentsElastic Beanstalk - Overview Elastic Beantalk is a developer centric view of delpoying an application on AWS It uses all the component’s we’ve seen before: EC2, ASG, ELB, RDS, … Managed service: Automatically handles capacity provisoning, load balancing, scaling, application health monitoring, instance configuration, … Just the application code is the responsibility of the developer We still have full control over the configuration Beanstalk is free but you pay for the underlying instancesElastic Beanstalk – Components Application: collection of ElasticBeantalk components (environments, versions, configurations, …) Application version: an iteration of your application code Environment Collection of AWS resources running an application version (only one application version at a time) Tiers: Web Server Environment Tier &amp; Workerd Environment Tier you can create multiple environment (dev, test, prod) Elastic Beanstalk – Supported Platforms Go Java SE Java with Tomcat .NET Core on Linux .NET on Windows Server Node.js PHP Python Ruby Packer Builder Single Container Docker Multi-container Docker Preconfigured Docker If not supported, you can writeyour custom platform (advanced)Web Server Tier vs. Worker Tier" }, { "title": "AWS Advance: Aurora", "url": "/posts/aws-advance-elasticache-rds-and-aurora-part-3/", "categories": "Fullstack, Architect, AWS", "tags": "aurora, database", "date": "2022-12-04 00:00:00 +0700", "snippet": "Amazon Aurora Aurora is a proprietary technology from AWS (not open sourced) Postgress and MYSQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or ...", "content": "Amazon Aurora Aurora is a proprietary technology from AWS (not open sourced) Postgress and MYSQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MYSQL database) Aurora is “AWS cloud optimized” and claims5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS Aurora storage automatically grows in increements of 10GB, up to 128 T B. Aurora have 15 replicas while MYSQL has 5, and the replication process is faster (sub 10ms replica lag) Failover in Aurora is instantaneous. It’s HA (high Avaiability) native. Aurora costs more than RDS** (20% more)** - but is more efficientAurora High Availability and read scaling 6 copies of your data across 3 AZ: 4 copies out of 6 needed for writes 3 copies out of 6 need for reads Self healing with peer to peer replication Storage is striped across 100s of volumes One Aurora instance takes write (master) Automated failover for master in less than 30 seconds Master + up to 15 Aurora read replicas serve reads Support for Cross Region Replication Shared storage Volume Replication + self healing + auto expandingAurora DB Cluster Write endpoint Pointing to the master Reader Endpoint Connection Load Balancing Features of Aurora Automatic Failover Backup annd Recovery Isolation and security Industry compliance Push-button scaling Automated Patching with Zero Downtime Advanced Monitoring Routine Maintenance Backtrack: restore data at anypoint of time without using backupsAurora Security Similar to RDS because uses the same engies Encryption at rest using KMS Automated backups, snapshots and replicas are also encrypted Encryption in flight using SSL (same proccess as MYSQL or Postgres) Possibility to autheticate usig IAM token (same method as RDS) You are responsible for protecting the instance with security groups You can’t SSHAurora Repliccas - Auto Scaling Write endpoint Many request to Reader endpoint and edpoint extended above replicas auto sccaling policies and shared storage volumeAurora - Custom Endpoints Define a subset of Aurora Instannces as a Custom endpoint Example: Run analytical queries on specific replicas The reader Endpoint is generally not used after defining custom endpointsAurora Serverless Automated database instantiation and auto scaling based on actual usage Good for infrequent, intermittent or unpredictable workloads No capacity plannig needed Pay per second, can be most cost-effective Proxy fleet is managed by AuroraAurora Multi-Master In case you want immediate failover for write node (HA) Every node does R/W - vs promoting a RR as the new masterGlobal Aurora Aurora cross region read replicas Usefule for disaster recovery Simple to put in place Aurora Global Database (recommended): 1 primary region (read / write) Up to 5 secodary (read only) regions, replication lag is less than 1 second Up to 16 read repliccas per secondary region Helps for decreasing latency Promotig another region (for disater recovery) has an RTO of &lt; 1 minute Aurora Machine Learning Enables you to add ML-based predictions to your applications via SQL Simple, optimized and secure integration between Aurora and AWS ML services Supported services Amazon SageMaker (use with any ML model) Amazon Comprehend (for sentiment analysis) You don’t need to have ML experience Use cases: fraud detection, ads targeting, sentiment analysis, product recommendationsAmazon ElastiCache Overview The same way RDS is to get managed Relational Databases… ElastiCache is to get managed Redis or Memcached Caches are in-memory databases with really high performance, low latency Helps reduce load off of databases for read intensive workloads Helps make your application stateless AWS takes care of OS maintanance / patching, optimizations, setup, configuration, monitoring, failure recovery and backups Using ElastiCache involves heavy application code changesElastiCache Solution Architecture - DB Cache Applications queries Elasticacche, if not available, get from RDS and store in ElastiCache Helps relieve load in RDS Cache must have an invalidation strategy to make sure only the most current data is used in there.ElastiCache Solution Architecture - User Session Store User logs into any of the application The application writes the session data into ElastiCache The user hits another instance of our application The instance retrieves the data ans the user is already logged in The user retrieve session from ElastiCacheElasticache: Redis vs Memcached Redis: Multi AZ with Auto Failover Read replicas to scale read and have availability Data durability using AOF persistence Backup and restore features Memcached Multi-node for pariniong of data (sharding) No high availability (replicationn) Non persistent No Backup and restore Multi threaded architedture ElastiCache - Cache Security All caches in ElastiCache Do not support IAM authentication IAM policies on ElastiCache are only used for AWS API-level security Redis AUTH You can set a “password/token” when you ccreate a Redis cluster This is an extra level of security for your cache (on top of security groups) Support SSL in flight encryptionn Memcached Supports SASL-based authentication (advanced) Patterns for ElastiCache Lazy Loading: all the read data is cached, data can become state in cache Write Through: Adds or update data in the cache when written to a DB (no scale data) Session Store: store temporary session data in a cache (using TTL features) Qoute: There are only two hard things in Computer Science: cache invalidation and naming thingsElastiCache - Redis Use Case Gaming leadeerboards are coomputationally complex Redis Sorted sets guarantee both uniqueness and element ordering Each time a new elemennt added, it’s ranked in real time, then added in correct order" }, { "title": "AWS Advance: RDS", "url": "/posts/aws-advance-alb-clb-rds-and-aurora-part-2/", "categories": "Fullstack, Architect, AWS", "tags": "aws, RDS, Encryption, DisaterRecovery, Security, Encapsolution", "date": "2022-12-04 00:00:00 +0700", "snippet": "RDSAWS RDS Overview RDS stannds for Relatioal Database service It’s a managed DB service for DB use SQL as a query language. It allow you to create databases in the cloud that are managed by AWS...", "content": "RDSAWS RDS Overview RDS stannds for Relatioal Database service It’s a managed DB service for DB use SQL as a query language. It allow you to create databases in the cloud that are managed by AWS Postgres MYSQL MariaDB Oracle Microsoft SQL Server Aurora(AWS Proprietary database) Advantage over using RDS versus deploying DB on EC2 RDS is a managed service: Automated provisioning, OS patching Conitnous backups and restore to specific timestamp (Point in Time Restore)! Monitoring dashboard Read replicas for improved read performance Maintennace windows for upgrade Sacling capacity (vertical and horizotal) Storage backed by EBS (gp2 or io1) But you can’t SSH into your instancesRDS backups Backups are automatically enabled in RDS Automated backups: Daily full backup of the database (durinng the maintenancce window) Transaction logs are baccked-up by RDS every 5 minutes ability to restore to any point in time (from oldest to 5 minutes ago) 7 days retention (can be increased to 35 days) DB snapshots: Manually triggered by the user Retention of backup for as long as you want RDS Storage Auto Scaling Helps you increase storage on your RDS DB instannce dynamically When RDS detects you are running out of free database storage, it scales automatically Avoid manually scaling your database storage You have to set Maximum Storage Threshold (maximum limit for DB storage) Automatically modify storage if Free storage is less than 10% of alloccated storage Low storage lasts at least 5 minutes 6 hours have passed sincce last modification RDS Read Replicas for read scalability Up to 5 read replicas Within AZ, Cross AZ or Cross Region Replication is ASYNC, so reads are eventually consistent Replicas can be promoted to their own DB Applications must update the connection string to leverage read replicasRDS Read replicas - Use Cases You have a production database that is taking on normal load You want to run a reporting application is unfacted Read replicas are used for SELECT(=read) only kind of statements (not INSERT, UPDATE, DELETE)RDS Read Replicas - Network Cost In AWS there’s a networ cost when data goes from one AZ to another For RDS read replicas within the same region, you don’t pay that feeRDS Multi AZ (Disater Recovery) Sync replication One DNS name - automactic app failover too standby Increase availability Failover in case of loss AZ, loss of networkk, instance or storage failure No manual intervention in apps Not used for scaling Multi AZ replication is free Note: The Read Replicas bet setup as Multi AZ for** Disater Recovery**RDS - From Single AZ to Multi AZ Zero downtime operation (no need to stop the DB) Just click on “modify” for the database The following happens internally: A snapshot is taken A new DB is retored from the snapshot in a new AZ Syncchronization is establishhed between the two databases RDS Security - Encryption At rest ecryption Possibility to encrypt the master &amp; read replicas with AWS KMS - AES-256 encryption Enccryption has to be defined at launch time If the master is not encrypted the read replicas cannot be encrypted Transparent Data Encryption (TDE) available for Oracle and SQL Server-** In-flight encryption** SSL certificates to encrypt data to RDS in flight Provide SSL options with trust certificate when connecting to database To enforce SSL: PostgreSQL: rds.force_ssl=1 i QWS RDS console MYSQL: Within the DB Grant Usage On . To ‘mycluster’@’%’ Require SSL; RDS Encryption Operations Encrypting RDS backups Snapshots of un-crypted RDS databases are un-crypted Sapshots of enncrypted RDS databases are encrypted Can copy a snapshot into an encrypted one To encrypt an un-encrypted RDS database Create a snapshot of the un-encrypted database Copy the snapshot and enable encryption for the snapshot Restore the database from the encrypted snapshot Migrate applications to the new database, and delete the old database RDS Security - Network &amp; IAM Network Security RDS databases are usually deployed within a private subnet, not in a public one RDS security works by leveraging seccurity groups (the same concept as for EC2 instances) it controls which ip / security group can comminicate with RDS Access Management IAM policies help control who can manage AWS RDS (through the RDS API) Tranditionnal Username annd Password can be used to login into the database IAM-based authentication can be used to login into RDS MYSQL &amp; PostgreSQL RDS - IAM Authentication IAM database authentication works with MYSQL and PostgreSQL You don’t need a password, just an authentication token obtained through IAM &amp; RDS API calls Auth token has a llifetime of 15 minutes Benefits: Networkk in/out must be encrypted using SSL IAM to cenntrally manage users instead of DB Cann leverage IAM roles and EC2 instance profiles for easy integration RDS Security - Summary Encryptionn at rest is done only when you first create the DB instance or: unencrypted DB =&gt; snapshot =&gt; ccopy snapshot as ecrypted =&gt; create DB from sapshot Your resposibility: Checck the ports / IP / security group inbound rules in DB’s SG In-database user creation and permission or manage through IAM Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections AWS responsibility No SSH access No manual DB patching No manual OS patching No way to audit the underlying instance " }, { "title": "AWS Certified Solutions Architect - Associate (SAA-C03) Exam Guide", "url": "/posts/aws-certified-solutions-architect-associate-saa-c03-exam-guide/", "categories": "Fullstack, Architect, AWS", "tags": "aws", "date": "2022-12-03 00:00:00 +0700", "snippet": "Content outlineThis exam guide includes weightings, test domains, and task statements for the exam. It is not acomprehensive listing of the content on the exam. However, additional context for each...", "content": "Content outlineThis exam guide includes weightings, test domains, and task statements for the exam. It is not acomprehensive listing of the content on the exam. However, additional context for each of the taskstatements is available to help guide your preparation for the exam. The following table lists the maincontent domains and their weightings. The table precedes the complete exam content outline, whichincludes the additional context. The percentage in each domain represents only scored content. Domain % exam Domain 1: Design Secure Architectures 30% Domain 2: Design Resilient Architectures 26% Domain 3: Design High-Performing Architectures 24% Domain 4: Design Cost-Optimized Architectures 20% Total 100% Domain 1: Design Secure ArchitecturesTask Statement 1: Design secure access to AWS resources.knowledge of: Access controls and management across multiple accounts AWS federated access and identity services (for example, AWS Identity and Access: IAM, AWS SSO AWS global infrastructure (Availbility zone and Access Managment , AWS Region) AWS Security best practicle The AWS shared responsibility modelSkill in: Applying AWS security best practices to IAM users and root users (for example mutil-factor authentication [MFA]) Designing a flexible authoriation model that includes IAM users, groups, roles, and policies Designing a role based access control strategy (for example AWS Security Token Service [AWS STS], role switching, cross account access) Designing a security strategy for multiple AWS accounts (for example, AWS Control Tower, service control policies [SCPs]) Determining the appropriate use of resource policies for AWS services Determining when to federate a directory service with IAM rolesTask Statement 2: Design secure workloads and applications.Knowledge of: Application configuration and credentials security AWS service endpoints Control ports, protocol, and network traffic on AWS Secure appplication access Security services with appropriate use cases (for example Amaon Cognito, Amazon GuardDuty, Amazon Macie) Threat vector external to AWS (for example DDos, SQL Injection)Skill in: Designing VPC architectures with security components (for example, security groups, route table, network ACLs, NAT gateways) Determining network segmentation stategies (for example, using public subnets and private subnets) Intergrating AWS services to secure applications (for example, AWS Shield, AWS WAF, AWS SSO, AWS Secrets Manager) Securing external network connections to and from the AWS Cloud (for example VPN, AWS Direct Connect)Task Statement 3: Determine appropriate data security controlsKnowledge of: Data access and governance Data recovery Data retention and classification Encryption and appropriate key managementSkills In: Aligning AWS technologies to meet compliance requirements Encription data at rest (AWS Key Management Service [AWS KMS]) Encrypting data in transit (AWS certificate manager ACM using TLS) Implementing acces policies for encryption keys Implementing policies for data access, life cycle and protection Rotating encryption key and renewing certificatesDomain 2:Design Resilient Architecturestask statement 1: Determine approriate data security controlsknowledge of: API creation and management (API Gateway, REST API) AWS managed services with appropriate use cases (AWS Transfer family, Amazon simple queue services, secret manager) Caching strategies Design principles for microservices (stateless workloads compared with stateful workloads) Event-driven architectures Horizontal scaling and vertical scaling How to appropriately use edge accelerators ( content delivery network CDN) How to migrate application into container Load balancing concept (application load balancer) Multi-tier architectures Queuing and messaging concepts (publish/subcrile) Serverless technologies and patterns (for example, AWS Fargate, AWS Lambda) Storage types with associated characteristics (for example, object, file, block) The orchestration of containers (for example, Amazon Elastic Container Service [Amazon ECS],Amazon Elastic Kubernetes Service [Amazon EKS]) When to use read replicas Workflow orchestration (for example, AWS Step Functions)Skills in: Designing event-driven, microservice, and/or multi-tier architectures based on requirements Determining scaling strategies for components used in an architecture design Determining the AWS services required to achieve loose coupling based on requirements Determining when to use containers Determining when to use serverless technologies and patterns Recommending appropriate compute, storage, networking, and database technologies basedon requirements Using purpose-built AWS services for workloadsTask Statement 2: Design highly available and/or fault-tolerant architectures.Knowledge of: AWS global infrastructure (for example, Avaialbility ones, AWS Regions, Amazon Route 53) AWS managed services with approiate use cases (Amazon comprehend, Amazone Polly) Basic networking concepts (for example, route tables) Disaster recovery strategies(DR),*backup and restore, pilot light, warm standby, active - active failover, recovery point objective [RPO], recovery time objective [RTO]) * Distributed design pattern Failover strategies Immutable infrastructure load balancing concept (Application load balancer) Proxy concept (Amazon RDS Proxy) Service quotas and throttling (how to configure the service qoutas for a workload in a standby environment) Storage options and characteristics (for example, durability, replication) Workload visibility (for example, AWS X-Ray)Skills in: Determining automation strategies to ensure infrastructure integrity Determining the AWS services required to provide a highly available and/or fault-tolerant Determining the AWS services required to provide a highly available and/or fault-tolerant Identifying metrics based on business requirements ti deliver a high available solution Implementing designs to mitigate single point of failure Implementing strategies to ensure the durability and availability of data (for example, backups) Selecting an appropriate** DR strategy** to meet business requirements Using AWS services that improve the reliability of legacy applications and applications not builtfor the cloud (for example, when application changes are not possible) Using purpose-built AWS services for workloadsDomain 3:Design high performing architecturesTask Statement 1: Determine high-performing and/or scalable storage solutionsKnowledge of: Hybrid storage solutions to meet business requirements Storage services with appropriate use cases (for example,Amazon S3, Amazon Elastic File System [Amazon EFS], Amazon Elastic Block Store [Amazon EBS]) Storage types with associated characteristics (for example, object, file, block)Skills in: Determining storage services and configurations that meet performance demands Determining storage services that can scale to accommodate future needsTask Statement 2: Design high-performing and elastc compute solutions.Knowledge of: AWS compute services with appropriate use cases (for example, AWS Batch, Amazon EMR, AWS Fargate) Distributed computing concepts supported by AWS global infrastructure and edge services Queuing and messaging concept (for example, publish/subcrible) Scalabilities with appropriate use cases (EC2 Auto Scaling, AWS Auto Scaling) Serverless technology and patterns (Lambda, fargate) The orchestration of containers (for example, Amazon ECS, Amazon EKS)Skill in: Decoupling workloads so that components can scale independently Identifing metrics and conditions to perform scaling actions Selecting the appropriare resource type and size (the amount of Lambda memory) to meet business requirementsTask Statement 3: Determine high-performing database solutions.Knowledge of: AWS global infrastructure (for example, Availability Zones, AWS Regions) Caching strategies and services (for example, Amazon ElastiCache) Data access patterns (for example, read-intensive compared with write-intensive) Database capacity planning (for example, capacity units, instance types, Provisioned IOPS) Database connections and proxies Database engines with appropriate use cases (for example,heterogeneous migrations,homogeneous migrations) Database replication (for example, read replicas) Database types and services (for example, serverless, relational compared with non-relational, in-memory)Skills in: Configuring read replicas to meet business requirements Designing database architectures Determining an appropriate database engine (for example, MySQL compared with PostgreSQL) Determining an appropriate database type (for example,Amazon Aurora, Amazon DynamoDB) Integrating caching to meet business requirementsTask Statement 4: Determine high-performing and/or scalable network architectures.Knowledge of: Data analytics and visualization services with appropriate use cases (for example,Amazon Athena, AWS Lake Formation, Amazon QuickSight) Data ingestion patterns (for example, frequency) Data transfer services with appropriate use cases (for example, AWS DataSync, AWS Storage Gateway) Secure access to ingestion access points Sizes and speeds needed to meet business requirements Streaming data services with appropriate use cases (for example,Amazon Kinesis)Skill In: Building and securing data lakes Designing data streaming architectures Design data transfer solutions Implementting visualization strategies Selecting appropriate compute options for data processing (Amazon EMR) Selecting appropriate configurations for ingestion Transforming data between formats (.csv to .parquet)Domain 4:Design Cost-Optimized ArchitecturesTask Statement 1: Design cost-optimized storage solutions.Knowledge of: Access options (for example, an S3 bucket with Requester Pays object storage) AWS cost management service features (for example, cost allocation tags, multi-account billing) AWS cost management tools with appropriate use cases (for example, AWS Cost Explorer, AWS Budgets, AWS Cost and Usage Report) AWS storage services with appropriate use cases (for example,Amazon FSx, Amazon EFS, Amazon S3, Amazon EBS) Backup strategies Block storage options (for example, hard disk drive [HDD] volume types, solid state drive [SSD] volume types) Data lifecycles Hybrid storage options (for example , DataSync, Transfer Family, Storage Gateway) Storage access patterns Storage tiering (for example, cold tiering for object storage) Storage types with associated characteristics (for example, object, file, block)Skills in: Designing appropriate storage strategies (for example, batch uploads to Amazon S3 compared with individual uploads) Determining the correct storage size for a workload Determining the correct storage size for a workload Determining when storage auto scaling is required Managing S3 object lifecycles Selecting the appropriate backup and/or archival solution Selecting the appropriate service for data migration to storage services Selecting the appropriate storage tier Selecting the correct data lifecycle for storage Selecting the most cost-effective storage service for a workloadTask Statement 2: Design cost-optimized compute solutionsKnowledge of: AWS cost management service features (for example, cost allocation tags, multi-account billing) AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report) AWS global infrastructure (for example, Availability Zones, AWS Regions) AWS purchasing options (for example, Spot Instances, Reserved Instances, Savings Plans) Distributed compute strategies (for example, edge processing) Hybrid compute options (for example, AWS Outposts, AWS Snowball Edge) Instance types, families, and sizes (for example, memory optimized, compute optimized, virtualization) Optimization of compute utilization (for example, containers, serverless computing, microservices) Scaling strategies (for example, auto scaling, hibernation)Skills in: Determining an appropriate load balancing strategy (for example, Application Load Balancer [Layer 7] compared with Network Load Balancer[Layer 4] compared with Gateway Load Balancer) Determining appropriate scaling methods and strategies for elastic workloads (for example, horizontal compared with vertical, EC2 hibernation) Determining cost-effective AWS compute services with appropriate use cases (for example, Lambda, Amazon EC2, Fargate) Determining the required availability for different classes of workloads (for example, production workloads, non-production workloads) Selecting the appropriate instance family for a workload Selecting the appropriate instance size for a workloadTask Statement 3: Design cost-optimized database solutions.Knowledge of: AWS cost management service features (for example, cost allocation tags, multi-account billing) ** AWS cost management tools** with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report) Caching strategies Data retention policies Database capacity planning (capacity units) Database connection and proxies database engines with appropriate use case (heterogeneous migrations, homogeneous migrations) database replication (read replica) database types and services (relation compared with non-relational, Aurora, Dynamo DB)Skills in: Designing appropriate backup and retention policies (for example, snapshot frequency) Determining an appropriate database engine (for example, MySQL compared with PostgreSQL) ** Determining cost-effective AWS database services with appropriate use cases (for example, DynamoDB compared with Amazon RDS, serverless)** Determining cost-effective AWS database types (for example, time series format, columnar format) Migrating database schemas and data to different locations and/or different database enginesTask Statement 4: Design cost-optimized network architectures.Knowledge of: AWS cost management service features (for example, cost allocation tags, multi-account billing) AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report) Load balancing concepts (for example, Application Load Balancer) NAT gateways (for example, NAT instance costs compared with NAT gateway costs) Network connectivity (for example, private lines, dedicated lines, VPNs) Network routing, topology, and peering (for example, AWS Transit Gateway, VPC peering) Network services with appropriate use cases (for example, DNS)Skills in: Configuring appropriate NAT gateway types for a network (for example, a single shared NAT gateway compared with NAT gateways for each Availability Zone) Configuring appropriate network connections (for example, Direct Connect compared with VPN compared with internet) Configuring appropriate network routes to minimize network transfer costs (for example, Region to Region, Availability Zone to Availability Zone, private to public, Global Accelerator, VPC endpoints) Determining strategic needs for content delivery networks (CDNs) and edge caching Reviewing existing workloads for network optimizations Selecting an appropriate throtting strategy Selecting the appropriate bandwidth allocation for a network device (a single VPN compared with multiple VPNs, Direct Connect speed)" }, { "title": "AWS Advance: Auto scaling and auto load balancer", "url": "/posts/aws-advance/", "categories": "Fullstack, AWS, Architect", "tags": "aws\naurora, elasticcache, rds, aurora, redis, memcache, template, asg, acl, NLB", "date": "2022-12-03 00:00:00 +0700", "snippet": "SSL/TLS - Basics An SSL Certificate allow traffic between your clients and your load balancer to be encrypted in transit (i-flight encryption) SSL refers to Secure Soccket Layeer, used to encrypt...", "content": "SSL/TLS - Basics An SSL Certificate allow traffic between your clients and your load balancer to be encrypted in transit (i-flight encryption) SSL refers to Secure Soccket Layeer, used to encrypt conections TLS refers to transport layer security, which is a newer versio Nowadays, TLS certificates are mainly used, but people still refer as SSL Public SSL certificates are issued by Certificate Authorities (CA) Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc… SSL ccertificates have an expiration date (yout set) and must be renewed\\Load Balacer - SSL Certificates The load balacer uses and** X.509 certificate** (SSL/TLS server certificcate) You can manage certificcates using ACM (AWS Certificate Manager) You can create upload your own certificates alternatively HTTPS listener You must specify a default certificate You can add an optional list of certs to support multiple domains Clients can use SNI (Server Name Indication) to specify the host name they reach Ability to specify a security policy to support older versions of SSL / TLS (legacy clients)\\ ## SSL - Server Name idication (SNI) SNI sloves the problem of loadinng multiple SSL certificates onto one web server (to serve multiple websites)-It’s a ‘newer’ protocol, and requires the client to indicate the hostname of the target server in the initial SSL hanshake The server will then find the correct certificccate, or return the default one Note: Only work for ALB &amp; CLB (newer generation), Cloudfront Does not work for CLB ( older gen)\\ Elastic Load Balancers - SSL Certicates Classic Load Balancer (v1) Support only one SSL certificate Must use multiple CLB for multiple host name with multiple SSL certificates Application Load Balanccer (v2) Supports multiple listeners with multiple SSL certificcates Uses serve name indication (SNI) to make it work Network Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Name Indication (SNI) to make it work Connection Draining Feature naming Connection Draining - for CLB Deregistration Delay - for ALB &amp; NLB Time to complete “i-fight request” while the instance is de-registerinng or unhealthy Stops sending ew request to EC2 instance which is de-registering Between 1 to 3600 seconds ( default 300 seconds) Can be disabled (set value to 0) Set to a low value if your request are short\\What’s an Auto Scaling Group? In real-life, the load on your websites and application can change In the cloud, you can create and get rid of servers very quickly The goal of an Auto Scalinng Group (ASG) is to: Scale out (add EC2 instances) to match an increased load Scale in (remove EC2 instances) to match a decreased load Ensure we have a minimum and a maximum number of machinnes running Automatically Register new instances to a load balancer\\ Auto Sccaling Group in AWS With Load Balancer A launch configuration AMI + Instance Type EC2 User data EBS Volumes Security Groups SSH key pair Min size / Max size / Initial capacity Network + Subnets informatio Load balancer information Scaling Policies\\Auto Scaling Alarms It is possible to scale an ASG based on* CloudWatch alarms* An Alarm monitors a metric (such as Average CPU) Metrics are computed for the overall ASG instances Based on the alarm We can create scale-out policies (increase the number of instances) We can create scale-in policcies (decrease the number of instances)\\ Auto scaling new rules It is now possible to define “better” auto scaling rules that are directly managed by EC2 Target Average CPU Usage Number of request on ELB per instance Average Network In Average Network Out These rules are are easier to setup and can make more sense\\Auto Scaling Custom Metric We can auto scale based on a custom metric ( number of connect users) Send custom metric from application on EC2 to CloudWatch (PutMetric API) Create CloudWatch alarm to react to low / high values Use the CloudWatch alarm as the scaling policy for ASG\\ ASG Brain Dump Scaling policies can be on CPU, Network… and can even be on custom metrics or based on a schedule (if you know your visitor patterns) ASGs use Launch configurations or Launch configuration / launch template IAM roles attached to an ASG will get assigned to EC2 instances ASG are free. You pay for the underlying resources being launched Having instances under an ASG means that if they get terminated for whatever reason, the ASG will automatically create new onces as a replacement. Extra safety! ASG can terminate instances marked as unhealthy by an ALB(and hence replace them)\\Auto Sccaling Groups - Dynamic Scaling Policies Target Trackkinng Scaling Most simple and easy to set up Example: I want the average ASG CPU to stay at around 40% Simple / Step Scaling When a CloudWatch alarm is triggerd(&gt;70%), then add 2 units When a CloudWatch alarm is triggerd(&lt;30%), then remove 1 Sechedule Actions Anticipate a sccaling based on known usage patterns Example: increase the min capacity to 10 at 5 pm on Fridays\\ Auto Scaling Groups - Predictive scaling Predictive scaling: continously forecast load and schedule scaling ahead analysis historical, generate forecast, schedule\\ Good metrics to scale on CPUUtillizatioon: Average CPUutilization across your instances RequestCountPerTarget: to make sure the number of requests per EC2 instances is stable Average Network In/Out (if you’re application is network bound) Any custom metric (that you push using CloudWatch)\\Auto Scaling Groups - Scaling cooldowns After a scaling activity happes, you are in the cooldown period (300second) During the cooldown period, the ASG will not launch or teminate additional instance (to allow for metrics to stabilize) Advice: Use a ready to use AMI to reduce configuration time in order to be serving request faster and reduce the cooldown period\\ASG for Solutions Architects ASG Default termination policy(simplified version) Find the AZ which has the most number of instaces If there are multiple instances in the AZ to choose from, delete the one with the oldest launch configuration ASG tries the balance the number of instances across AZ by default\\ASG for solutions architects lifecycle hooks By default as soon as an istance is launched in an ASG it’s in service You have the** ability to perform **extra steps before the instance goes i service(Pendinng state) You have the** ability to perform some actions** before the innstannce is terminated\\ASG for solutions architect launch template vs launch configuration Both: ID of the Amazon machie Image (AMI), the instance type, a key security groups, and the other parameter that you use to launch EC2 istances(tags, EC2 user-data) Launch Configuration (Legacy): Must be re-created every time Launch Template (newer) Can have multiple versions Create parameters subsets (partial cconfiguration for re-use annd inheritance) PRovision using both On-Demand and Spot instances Can use T2 unlimited brust features Recomend by AWS going forward " }, { "title": "Node.JS concept", "url": "/posts/nodejs-concept/", "categories": "documents, concept, tutorials, nodejs", "tags": "node.js, javascript, js, node, emitt", "date": "2022-09-02 00:00:00 +0700", "snippet": "NodeJS - ConceptNode.js is an open-source and cross platform javascript runtime environment. It is a popular tool for almost any kind of project!NodeJS is javascript on the server, built from v8 en...", "content": "NodeJS - ConceptNode.js is an open-source and cross platform javascript runtime environment. It is a popular tool for almost any kind of project!NodeJS is javascript on the server, built from v8 engine used to read, parse javascript code and executed necessary action. NodeJs i a non blocking event based IO and run on a single thread process.Event loop is constanly running process that monitors the callback queue and the call stack. The process will continously check the call stack, and if the call stack is empty, push the next function from the callback queue to the stack. If there is nothing in the callback queue, nothing will happen. Runs the V8 javascript engine, the core of Google Chrome, outside of browser. This allows Node.js to be very performant Single thread for every request provides a set of asynchronous I/O primitives using non-blocking paradigms, making blocking behaviour the exception rather than the norm. Reading network, database, file system: resume the operation when response is comeback AdonisJS, Egg.js, Express, Fastify, featherJs, Gastby, hapi, koa, loopback.io, meteor, micro, nestjs, nextjs, NX, Remix, Sapper, socker.io, strapi.const http = require('http')const hostname = '127.0.0.1'const port = 3000const server = http.createServer((req, res) =&gt; { res.statusCode = 200 res.setHeader('Content-Type', 'text/plain') res.end('Hello World\\n')})server.listen(port, hostname, () =&gt; { console.log(`Server running at http://${hostname}:${port}/`)})ProsHandle thousands of concurrent connection with a single server withou introducing the burden of managing thread concurency, which could be a significant source of bugs. unique advantage because milions of frontend developers that write javascript for the browser are now able to write client-server. ECMAScript can be used by specifics NodeJs version. Install NVM is a popular way to run Node.js. It allows you to easily switch the Node.Js version and install new version to try and easily rollback if something breaks. Javascript Topics Lexical Structure Unicode Semicolons White space Case sensitive Comments Literals and Identifiers Reserved words Expressions Name Shorthand operator Meaning Assignment x = f() x = f() Addition assignment x += f() x = x + f() Subtraction assignment x -= f() x = x - f() Multiplication assignment x *= f() x = x * f() Division assignment x /= f() x = x / f() Remainder assignment x %= f() x = x % f() Exponentiation assignment x **= f() x = x ** f() Left shift assignment x &lt;&lt;= f() x = x &lt;&lt; f() Right shift assignment x &gt;&gt;= f() x = x &gt;&gt; f() Unsigned right shift assignment x &gt;&gt;&gt;= f() x = x &gt;&gt;&gt; f() Bitwise AND assignment x &amp;= f() x = x &amp; f() Bitwise XOR assignment x ^= f() x = x ^ f() Bitwise OR assignment x \\|= f() x = x \\| f() Logical AND assignment x &amp;&amp;= f() x &amp;&amp; (x = f()) Logical OR assignment x \\|\\|= f() x \\|\\| (x = f()) Logical nullish assignment x ??= f() x ?? (x = f()) TypesJavaScript Types are Dynamic. String, Numbers, Booleans, Arrays, Objects, Undefined, Typeof ClassesClasses are a template for creating objects. They encapsulate data with code to work on that data. // unnamedlet Rectangle = class {constructor(height, width) { this.height = height; this.width = width;}};console.log(Rectangle.name);// output: \"Rectangle\" Variables4 Ways to Declare a JavaScript Variable: Using var Using let Using const Using nothing FunctionsA JavaScript function is a block of code designed to perform a particular task. thisIn JavaScript, the this keyword refers to an object.Which object depends on how this is being invoked (used or called). Arrow FunctionsAn arrow function expression is a compact alternative to a traditional function expression, but is limited and can’t be used in all situations.```JS// ———————-// Arrow Example// ———————-// A simplistic object with its very own “this”.const obj = { num: 100,};// Setting “num” on window to show how it gets picked up.window.num = 2020; // yikes!// Arrow Functionconst add = (a, b, c) =&gt; this.num + a + b + c;// callconsole.log(add.call(obj, 1, 2, 3)); // result 2026// applyconst arr = [1, 2, 3];console.log(add.apply(obj, arr)); // result 2026// bindconst bound = add.bind(obj);console.log(bound(1, 2, 3)); // result 2026Arrow functions cannot be used as constructors and will throw an error when used with new.Arrow functions do not have a prototype property.The yield keyword may not be used in an arrow function's bodyArrow functions can have either a concise body or the usual block body.```JS// An empty arrow function returns undefinedconst empty = () =&gt; {};(() =&gt; 'foobar')();// Returns \"foobar\"// (this is an Immediately Invoked Function Expression)const simple = (a) =&gt; a &gt; 15 ? 15 : a;simple(16); // 15simple(10); // 10const max = (a, b) =&gt; a &gt; b ? a : b;// Easy array filtering, mapping, etc.const arr = [5, 6, 13, 0, 1, 18, 23];const sum = arr.reduce((a, b) =&gt; a + b);// 66const even = arr.filter((v) =&gt; v % 2 === 0);// [6, 0, 18]const double = arr.map((v) =&gt; v * 2);// [10, 12, 26, 0, 2, 36, 46]// More concise promise chainspromise .then((a) =&gt; { // … }) .then((b) =&gt; { // … });// Parameterless arrow functions that are visually easier to parsesetTimeout(() =&gt; { console.log('I happen sooner'); setTimeout(() =&gt; { // deeper code console.log('I happen later'); }, 1);}, 1); Loops for - loops through a block of code a number of times for/in - loops through the properties of an object for/of - loops through the values of an iterable object while - loops through a block of code while a specified condition is true do/while - also loops through a block of code while a specified condition is true Scopes Block scope Function scope Global scope Arrays const person = [];person[\"firstName\"] = \"John\";person[\"lastName\"] = \"Doe\";person[\"age\"] = 46;person.length; // Will return 0person[0]; // Will return undefined Template LiteralsTemplate literals are literals delimited with blacktick (`) characters, allowing for multi-line string, for string interpolation with embedded expressions, and for special constructs called tagged templates. `string text ${expression} string text` SemicolonsSemicolons are an essential part of javascript code. They are read and used by the compiler to distinguising between separate statements so that statements do not lead into other parts of the code. The good news is that JavaScript includes an automatic semicolon feature. Strict ModeStrict directive was new in ECMA Script version 5.It is not a statement, but a literal expression, ignored by earlier versions of JavaScript.The purpose of \"use strict\" is to indicate that the code should be executed in “strict mode”. \"use strict\";x = 3.14; // This will cause an error because x is not declaredfunction myFunction() {\"use strict\";y = 3.14; // This will cause an error}x = {p1:10, p2:20}; // This will cause an errordelete x; // This will cause an error not allowed: duplicated, deleted, undeclare, octal number, octal escape, Writing to a read-only property, Writing to a get-only property , delete undeleteable properties, cant use eval word, a variable can not be used before it is declared,eval() can not declare a variable using the var keyword, eval() can not declare a variable using the let keyword, the arguments, with cant be used as a variable. ECMAScript 6, 2016, 2017With those concept in mind, you are well on your road to become a proficient javascript developer in both browser and in Node.js.Fundamental part of Node.js: Asynchronous programming and callbacks Timers Promises Async and Await Closures The Event Loop Asynchronous programming and callbacks Asynchronous means that things can happen independently of the main program flow.In the current consumer computers, every program runs for a specific time slot and then it stops its exceution to let another program continue their execution. This thing runs in a cycle so fast that it’s impossible to notice. We thing our computer run many programs simultaneously, but this is an illusion.Programs internally use interrupts, a signal that’s emitted to the processor to gain the attention of the system.JavaScript is synchronous by default and is single threaded. This means that code cannot create new threads and run in parallel.The browser provides a way to do it by providing a set of APIs that can handle this kind of functionality. More recently, Node.js introduced a non-blocking I/O environment to extend this concept to file access, network calls and so on.Callback You define an event handler for the click event. This event handler accpet a function, which will be called when the event is triggered const xhr = new XMLHttpRequest();xhr.onreadystatechange = () =&gt; {if (xhr.readyState === 4) { xhr.status === 200 ? console.log(xhr.responseText) : console.error('error');}};xhr.open('GET', 'https://yoursite.com');xhr.send(); browser event(DOM) access file and make XHR request setTimeOut function Handling errors in callbacks The first parameter in any callback function is the error object: error-first callbacks The problem with callbacks However every callback adds a level of nesting, and when you have lots of callbacks, the code starts to be complicated very quickly```jsfunction doStep1(init) {return init + 1;} function doStep2(init) { return init + 2;}function doStep3(init) { return init + 3;}function doOperation() { let result = 0; result = doStep1(result); result = doStep2(result); result = doStep3(result); console.log(result: ${result});}doOperation();Because we have to call callbacks inside callbacks, we get a deeply nested doOperation() function, which is much harder to read and debug. This is sometimes called \"callback hell\" or the \"pyramid of doom\" (because the indentation looks like a pyramid on its side).For these reasons, most modern asynchronous APIs don't use callbacks. Instead, the foundation of asynchronous programming in JavaScript is the Promise### Timers- SetTimeout : you soecify a callback function to execute later and value expressing how later you want it to run, in milisecons.```JSconst myFunction = (firstParam, secondParam) =&gt; { // do something};// runs after 2 secondssetTimeout(myFunction, 2000, firstParam, secondParam);// I changed my mindclearTimeout(id);Promiselet done = trueconst isItDoneYet = new Promise((resolve, reject) =&gt; { if (done) { const workDone = 'Here is the thing I built' resolve(workDone) } else { const why = 'Still working on something else' reject(why) }})const checkIfItsDone = () =&gt; { isItDoneYet .then(ok =&gt; { console.log(ok) }) .catch(err =&gt; { console.error(err) })}checkIfItsDone()A promise is commonly defined as a proxy for a value that will eventually become avaiable. promise is oneway to deal with asynchronous code.How promise working When a promise called, it will start in a pending state. This means that the calling function continues executing, while the promise is pending until it resolves, giving the calling function whatever data was being requested. The created promise will end in resolved state or rejected state. Calling the respective callback function upon finishing. PromisifyingThis technique is a way to be able to use a classic JavaScript function that takes a callback, and have it return a promise.```jsconst fs = require(‘fs’);const getFile = fileName =&gt; { return new Promise((resolve, reject) =&gt; { fs.readFile(fileName, (err, data) =&gt; { if (err) { reject(err); // calling reject will cause the promise to fail with or without the error passed as an argument return; // and we don’t want to go any further } resolve(data); }); });};getFile(‘/etc/passwd’) .then(data =&gt; console.log(data)) .catch(err =&gt; console.error(err));**Consuming a promise**```jsconst isItDoneYet = new Promise(/* ... as above ... */);// ...const checkIfItsDone = () =&gt; { isItDoneYet .then(ok =&gt; { console.log(ok); }) .catch(err =&gt; { console.error(err); });};Chaining promisesconst status = response =&gt; { if (response.status &gt;= 200 &amp;&amp; response.status &lt; 300) { return Promise.resolve(response); } return Promise.reject(new Error(response.statusText));};const json = response =&gt; response.json();fetch('/todos.json') .then(status) // note that the `status` function is actually **called** here, and that it **returns a promise*** .then(json) // likewise, the only difference here is that the `json` function here returns a promise that resolves with `data` .then(data =&gt; { // ... which is why `data` shows up here as the first parameter to the anonymous function console.log('Request succeeded with JSON response', data); }) .catch(error =&gt; { console.log('Request failed', error); });Cascading Errornew Promise((resolve, reject) =&gt; { throw new Error('Error');}) .catch(err =&gt; { throw new Error('Error'); }) .catch(err =&gt; { console.error(err); });Orchestrating promisePromise have two function synchonized Promise.all()```jsconst f1 = fetch(‘/something.json’);const f2 = fetch(‘/something2.json’);Promise.all([f1, f2]) .then(res =&gt; { console.log(‘Array of results’, res); }) .catch(err =&gt; { console.error(err); });- Promise.race()Promise.race() runs when the first of the promises you pass to it settles- Promise.any()Promise settle when any of the promise you pass to it fulfill or all of promise is get rejected(AggregateError)**Common errors**- Uncaught TypeError: undefined is not a promise- UnhandledPromiseRejectionWarning### ThenablesThe JavaScript ecosystem had made multiple Promise implementations long before it became part of the language. Despite being represented differently internally, at the minimum, all Promise-like objects implement the Thenable interface. A thenable implements the .then() method, which is called with two callbacks: one for when the promise is fulfilled, one for when it's rejected. Promises are thenables as well.```jsconst aThenable = { then(onFulfilled, onRejected) { onFulfilled({ // The thenable is fulfilled with another thenable then(onFulfilled, onRejected) { onFulfilled(42); }, }); },};Promise.resolve(aThenable); // A promise fulfilled with 42 Complex promise```js“use strict”;let promiseCount = 0;function testPromise() { const thisPromiseCount = ++promiseCount; const log = document.getElementById(“log”); // begin log.insertAdjacentHTML(“beforeend”, ${thisPromiseCount}) Started&lt;br&gt;); // We make a new promise: we promise a numeric count of this promise, starting from 1 (after waiting 3s) const p1 = new Promise((resolve, reject) =&gt; { // The executor function is called with the ability to resolve or reject the promise log.insertAdjacentHTML( “beforeend”, ${thisPromiseCount}) Promise constructor&lt;br&gt; ); // This is only an example to create asynchronism setTimeout(() =&gt; { // We fulfill the promise ! resolve(thisPromiseCount); }, Math.random() * 2000 + 1000); });// We define what to do when the promise is resolved with the then() call, // and what to do when the promise is rejected with the catch() call p1.then((val) =&gt; { // Log the fulfillment value log.insertAdjacentHTML(“beforeend”, ${val}) Promise fulfilled&lt;br&gt;); }).catch((reason) =&gt; { // Log the rejection reason console.log(Handle rejected promise (${reason}) here.); }); // end log.insertAdjacentHTML(“beforeend”, ${thisPromiseCount}) Promise made&lt;br&gt;);}const btn = document.getElementById(“make-promise”);btn.addEventListener(“click”, testPromise);```html&lt;button id=\"make-promise\"&gt;Make a promise!&lt;/button&gt;&lt;div id=\"log\"&gt;&lt;/div&gt;Modern Asynchronous JavaScript with Async and AwaitAsync function are a combination of promise and generators, and basically, they are higher level abstraction over promises. async/await is built on promise Example when async function return a promise. An await function is calling until the promise is resolved or rejected.```jsconst doSomethingAsync = () =&gt; {return new Promise(resolve =&gt; { setTimeout(() =&gt; resolve(‘I did something’), 3000)})}const doSomething = async () =&gt; { console.log(await doSomethingAsync())}console.log(‘Before’)doSomething()console.log(‘After’)- **Promise vs async/await**```js// thenableconst getFirstUserData = () =&gt; { return fetch('/users.json') // get users list .then(response =&gt; response.json()) // parse JSON .then(users =&gt; users[0]) // pick first user .then(user =&gt; fetch(`/users/${user.name}`)) // get user data .then(userResponse =&gt; userResponse.json()); // parse JSON};getFirstUserData();// implemented by async/awaitconst getFirstUserData = async () =&gt; { const response = await fetch('/users.json'); // get users list const users = await response.json(); // parse JSON const user = users[0]; // pick first user const userResponse = await fetch(`/users/${user.name}`); // get user data const userData = await userResponse.json(); // parse JSON return userData;};getFirstUserData(); Multiple async functions in series```jsconst promiseToDoSomething = () =&gt; {return new Promise(resolve =&gt; { setTimeout(() =&gt; resolve(‘I did something’), 10000)})}const watchOverSomeoneDoingSomething = async () =&gt; { const something = await promiseToDoSomething() return something + ‘\\nand I watched’}const watchOverSomeoneWatchingSomeoneDoingSomething = async () =&gt; { const something = await watchOverSomeoneDoingSomething() return something + ‘\\nand I watched as well’}watchOverSomeoneWatchingSomeoneDoingSomething().then(res =&gt; { console.log(res)})### ClosuresThe combination of function bundled together (enclosed) with references to it surround state (THE LEXICAL environment). a closure gives you access to an outer function's scope from an inner function.```jsfunction showHelp(help) { document.getElementById('help').textContent = help;}function makeHelpCallback(help) { return function () { showHelp(help); };}function setupHelp() { var helpText = [ { id: 'email', help: 'Your e-mail address' }, { id: 'name', help: 'Your full name' }, { id: 'age', help: 'Your age (you must be over 16)' }, ]; for (var i = 0; i &lt; helpText.length; i++) { var item = helpText[i]; document.getElementById(item.id).onfocus = makeHelpCallback(item.help); }}setupHelp();Scoping with let and constTraditionally (before ES6), javascript only had two kind of scope: function scope and global scope. Variables declare with var are either function scoped and global scope, depending on whether they are declared within a function or outside function. This can be tricky, because blocks with curly braces do not create scope With const you can create a block scoped: temporal dead zone! A closure is the combination of a function and the lexical environment within which that function was declared. This environment consits of any local variables that were in-scope at the time the closure was created. Emulating private methods with closures ```JSconst counter = (function () {let privateCounter = 0;function changeBy(val) { privateCounter += val;} return { increment() { changeBy(1); }, decrement() { changeBy(-1); }, value() { return privateCounter; },};})(); console.log(counter.value()); // 0.counter.increment();counter.increment();console.log(counter.value()); // 2.counter.decrement();console.log(counter.value()); // 1.JS, prior to class, didn't have a native way of declaring private methods, but it was possible to emulate private methods using closure. Private methods aren't just useful fo restricting access to code. They also prove a powerful way of managing your global namespace.#### Closure scope- Local scope (own scope)- Enclosing scope (can be block, function, or module scope)- Global scope```JS// global scopeconst e = 10;function sum(a) { return function (b) { return function (c) { // outer functions scope return function (d) { // local scope return a + b + c + d + e; }; }; };}console.log(sum(1)(2)(3)(4)); // log 20You can also write without anonymous functions:// global scopeconst e = 10;function sum(a) { return function sum2(b) { return function sum3(c) { // outer functions scope return function sum4(d) { // local scope return a + b + c + d + e; }; }; };}const sum2 = sum(1);const sum3 = sum2(2);const sum4 = sum3(3);const result = sum4(4);console.log(result); //log 20Event loopIn Nodejs Javascript, event loop is the “killer features” while nodejs run on single thread. There is just one thing happening at a timeThat’s actually helpful as it simplifier alot how a program without worry about concurrency issues. You just need to pay attention to write your code and avoid anything that could block the tread, like synchronous network call and infinite loops. In general, in most broswer there is an event loop for every browser tab, to make every process isolated and avoid webpage with infinite loop and heavy processsing to block your entire broswer. The environment manages multiple concurent event loops, to handle API call for examples. Webworker can handler multiple event loop as well So you need to concerned that your code will run on a single event loop, and write code with this thing in mind to avoid blocking it. Blocking the event loop Any javascript code that take long to return back control to the event loop will block the execution of any javascript code in the page, event block the UI page, and the user cannot click around, scroll page, and so on.All I/O primitive in JS are non-blocking. Network requests, filesystems operation, and so on. Being blocking is the exception, and this is why javascript is based so much on callback, and more recently on promise and async/await. The call stack(LIFO) The event loop continously check the call stack to see if there’s any function that need to run.While doing so, it adds any function call it find in the call stack and executes each one in order.You know the error stack trace you might be familiar. A simple event loop explanation The event loop on every iteration looks if there’s something in the call stack, and executes it until the call stack is empty.```jsconst bar = () =&gt; console.log(‘bar’) const baz = () =&gt; console.log(‘baz’)const foo = () =&gt; { console.log(‘foo’) bar() baz()}foo()### Queuing function executionThe above example looks normal, there's nothing special about it: JavaScript finds things to execute, runs them in order.Let's see how to defer a function until the stack is clear.The use case of setTimeout(() =&gt; {}, 0) is to call a function, but execute it once every other function in the code has executed.```JSconst bar = () =&gt; console.log('bar')const baz = () =&gt; console.log('baz')const foo = () =&gt; { console.log('foo') setTimeout(bar, 0) baz()}foo()When this code runs, first foo() is called. Inside foo() we first call setTimeout, passing bar as an argument, and we instruct it to run immediately as fast as it can, passing 0 as the timer. Then we call baz().The Message QueueWhen setTimeOut is called. Browser or Node.JS start a timer. Once the timer expire as in this case we put 0 as the timeout. The call backfunction intermediately put in the Message queue.Message queue is also where user initiated event like click, scroll, mouse or keyboard, fetch reponse before your code has opportunity to reach to them. Or also DOM event like onload.The loop gives the priority to the callstack, and it’s first find in the callstack, and once there’s nothing in there. Ir’s goes to pich up things in the message queue.ES6 Job QueueECMA script 2015 introduced concept of the job queue, which is used by Promise. It’s a way to execute a result of an async function as soon as posible, rather puting the end of the call stack.Promise is resole the current function ends will execute right after the function current function.Similar to a rollercoaster ride at an amusement park: the message queue puts you at the back of the queue, behind all the other people, where you will have to wait for your turn, while the job queue is the fastpass ticket that lets you take another ride right after you finished the previous one.const bar = () =&gt; console.log('bar')const baz = () =&gt; console.log('baz')const foo = () =&gt; { console.log('foo') setTimeout(bar, 0) new Promise((resolve, reject) =&gt; resolve('should be right after baz, before bar') ).then(resolve =&gt; console.log(resolve)) baz()}foo()That’s a big difference between Promises (and Async/await, which is built on promises) and plain old asynchronous functions through setTimeout() or other platform APIs.Finally, here’s what the call stack looks like for the example above:Differences between Node.js and the Browser NodeJs give Huge advance, you can perform all your work on the web, both client and server. The confor of programming everything - the frontend and the backend in a single languages. Another big difference is that in Node.js you control the environment. Unless you are building an open source application that anyone can deploy anywhere, you know which version of Node.js you will run the application on. This means that you can write all the modern ES6-7-8-9 JavaScript that your Node.js version supports. Node.js supports both the CommonJS and ES module systems (since Node.js v12), while in the browser we are starting to see the ES Modules standard being implemented. This means that you can use both require() and import in Node.js, while you are limited to import in the browser. The V8 JavaScript Engine V8 is the name of the JavaScript engine that powers Google Chrome. Firefox has SpiderMonkey Safari has JavaScriptCore (also called Nitro) Edge was originally based on Chakra but has more recently been rebuilt using Chromium and the V8 engine. The quest for performance V8 is written in C++, and it’s continously improved. It is portable and runs on Mac, Windows, Linux and several other systems. V8 is always evolving, just like the other javascript engines around, to speed up the Web and the Node.js ecosystem. Compilation Javascript is internally compiled by V8 with just in time(JIT), and compilation to speed up the excecution. Run Node.js scripts The usual way to run a Node.js program is to run globally available node command.If your main Node.js application file is app.js, you can call it by typing: chmod u+x app.jsnode app.js The content of file app.js```js#!/usr/bin/env node // your code### Restart the application automaticallyInstall `nodemon` module### How to exit from a Node.js program```JSprocess.exit(1);process.exitCode = 1;Another example:const express = require('express');const app = express();app.get('/', (req, res) =&gt; { res.send('Hi!');});const server = app.listen(3000, () =&gt; console.log('Server ready'));process.on('SIGTERM', () =&gt; { server.close(() =&gt; { console.log('Process terminated'); });});Express is a framework that uses the http module under the hood, app.listen() returns an instance of http. You would use https.createServer if you needed to serve your app using HTTPS, as app.listen only uses the http module.Node.js, accept arguments from the command linenode app.js var_1=abcHow to read environment variables from Node.jsUSER_ID=239482 USER_KEY=foobar node app.js// process by JSprocess.env.USER_ID; // \"239482\"process.env.USER_KEY; // \"foobar\" Note: process does not require a “require”, it’s automatically available.If you have multiple environment variables in your node project, you can create .env file in the root directory of your project, and use the dotenv package to load them during runtime. # .env fileUSER_ID=\"239482\"USER_KEY=\"foobar\"NODE_ENV=\"development\" The way you retrieve it is using the process object built into Node.jsIt exposes an argv property, which is an array that contains all the command line invocation arguments. process.argv.forEach((val, index) =&gt; { console.log(`${index}: ${val}`);});const args = require('minimist')(process.argv.slice(2));args.name; ```JSrequire(‘dotenv’).config();process.env.USER_ID; // “239482”process.env.USER_KEY; // “foobar”process.env.NODE_ENV; // “development”&gt; You can also run your js file with `node -r dotenv/config index.js` command if you don't want to import the package in your code.You can install `minimist` package using `npm`. and when retrieve variable from nodejs use: `node app.js --name=joe`### Output to the command line using Node.jsYou can pass multiple variables to `console.log````JSconst x = 'x';const y = 'y';console.log(x, y);console.log('My %s has %d ears', 'cat', 2);console.clear()const x = 1const y = 2const z = 3console.count( 'The value of x is ' + x + ' and has been checked .. how many times?')console.count( 'The value of x is ' + x + ' and has been checked .. how many times?')console.count( 'The value of y is ' + y + ' and has been checked .. how many times?')Print stach tracefunction2 = () =&gt; { console.trace()}function1 = () =&gt; function2();function1();Output consoleTrace at function2 (repl:1:33) at function1 (repl:1:25) at repl:1:1 at ContextifyScript.Script.runInThisContext (vm.js:44:33) at REPLServer.defaultEval (repl.js:239:29) at bound (domain.js:301:14) at REPLServer.runBound [as eval] (domain.js:314:12) at REPLServer.onLine (repl.js:440:10) at emitOne (events.js:120:20) at REPLServer.emit (events.js:210:7)Calculate time spentCalculate how much time a function you take to run: timetime, timeEndconst doSomething = () =&gt; console.log('test');const measureDoingSomething = () =&gt; { console.time('doSomething()'); // do something, and measure the time it takes doSomething(); console.timeEnd('doSomething()');};measureDoingSomething();Create a progress barProgress is an awesome package that help create an progress bar in console. Installing it using npm install progress.const ProgressBar = require('progress');const bar = new ProgressBar(':bar', { total: 10 });const timer = setInterval(() =&gt; { bar.tick(); if (bar.complete) { clearInterval(timer); }}, 100);Server side Pagination in Node.js with Sequelize &amp; MySQLServer side pagination is better for: Larger data set Faster initial page load Accessibility for those running javascript Complex view running Node.js Pagination with Sequelize and MySQL overview Assume display a table below Node.js Express Server will exports API for pagination (with/without filter), here are some url samples: /api/tutorials?page=1&amp;size=5 /api/tutorials?size=5: using default value for page /api/tutorials?title=data&amp;page=1&amp;size=3: pagination &amp; filter by title containing ‘data’ /api/tutorials/published?page=2: pagination &amp; filter by ‘published’ statusResult by json: { \"totalItems\": 8, \"tutorials\": [...], \"totalPages\": 3, \"currentPage\": 1} Node.js Sequelize for Pagination Sequelize provide way to implement pagination with offset and limit offset: quantity of items to skip limit: quantity of items to fetchExample: { offset: 3, limit: 2 }: skip first 3 items, fetch 4th and 5th items. Sequelize findAll model.findAll({limit: 2,offset: 3,where: { title: { [Op.like]: `%js%` } }, // conditions}); The result: [ { \"id\": 4, \"title\": \"bezkoder Tut#4 Rest Apis\", \"description\": \"Tut#4 Description\", \"published\": false, \"createdAt\": \"2020-06-05T11:55:07.000Z\", \"updatedAt\": \"2020-06-05T11:55:07.000Z\" }, { \"id\": 5, \"title\": \"bezkoder Tut#5 MySQL\", \"description\": \"Tut#5 Description\", \"published\": false, \"createdAt\": \"2020-06-05T11:55:11.000Z\", \"updatedAt\": \"2020-06-05T11:55:11.000Z\" }] Sequelize findAndCountAll Template model.findAndCountAll({limit: 2,offset: 3,where: {}, // conditions}); Result { \"count\": 8, \"rows\": [ { \"id\": 4, \"title\": \"bezkoder Tut#4 Rest Apis\", \"description\": \"Tut#4 Description\", \"published\": false, \"createdAt\": \"2020-06-05T11:55:07.000Z\", \"updatedAt\": \"2020-06-05T11:55:07.000Z\" }, { \"id\": 5, \"title\": \"bezkoder Tut#5 MySQL\", \"description\": \"Tut#5 Description\", \"published\": false, \"createdAt\": \"2020-06-05T11:55:11.000Z\", \"updatedAt\": \"2020-06-05T11:55:11.000Z\" } ]} Setup Node.js Express Application Install necessary modules: express, sequelize, mysql2Run the command: npm install express sequelize mysql2 cors --save The Node.js project structure that we only need to add some changes to make the pagination work well. Configure MySQL database &amp; Sequelize DB configure module.exports = {HOST: \"localhost\",USER: \"root\",PASSWORD: \"123456\",DB: \"testdb\",dialect: \"mysql\",pool: { max: 5, min: 0, acquire: 30000, idle: 10000}}; Initialize Sequelize Create app/models/index.js with the following code: const dbConfig = require(\"../config/db.config.js\");const Sequelize = require(\"sequelize\");const sequelize = new Sequelize(dbConfig.DB, dbConfig.USER, dbConfig.PASSWORD, {host: dbConfig.HOST,dialect: dbConfig.dialect,operatorsAliases: false,pool: { max: dbConfig.pool.max, min: dbConfig.pool.min, acquire: dbConfig.pool.acquire, idle: dbConfig.pool.idle}});const db = {};db.Sequelize = Sequelize;db.sequelize = sequelize; Don’t forget to call sync() method in server.js ...const app = express();app.use(...);const db = require(\"./app/models\");db.sequelize.sync();/*db.sequelize.sync({ force: true }).then(() =&gt; {console.log(\"Drop and re-sync db.\");});*/... Create Data Model In models folder, create tutorial.model.js file like this: odule.exports = (sequelize, Sequelize) =&gt; {const Tutorial = sequelize.define(\"tutorial\", { title: { type: Sequelize.STRING }, description: { type: Sequelize.STRING }, published: { type: Sequelize.BOOLEAN }});return Tutorial;}; After initializing Sequelize, we don’t need to write CRUD functions, Sequelize supports all of them. Now you can easily use following methods with pagination: get all Tutorials: findAll({ limit, offsCreate Node.js Express API layeret }) find all Tutorials by title: findAll({ where: { title: … }, limit, offset }) find and count all Tutorials: findAndCountAll({ limit, offset }) find and count all Tutorials by title: findAndCountAll({ where: { title: … }, limit, offset }) Controller with Pagination So, let’s write the function to map default response to desired structure: const getPagination = (page, size) =&gt; {const limit = size ? +size : 3;const offset = page ? page * limit : 0;return { limit, offset };};const getPagingData = (data, page, limit) =&gt; {const { count: totalItems, rows: tutorials } = data;const currentPage = page ? +page : 0;const totalPages = Math.ceil(totalItems / limit);return { totalItems, tutorials, totalPages, currentPage };}; Now the code in tutorial.controller.js will look like this: const db = require(\"../models\");const Tutorial = db.tutorials;const Op = db.Sequelize.Op;const getPagination = ...;const getPagingData = ...;// Retrieve all Tutorials from the database.exports.findAll = (req, res) =&gt; {const { page, size, title } = req.query;var condition = title ? { title: { [Op.like]: `%${title}%` } } : null;const { limit, offset } = getPagination(page, size);Tutorial.findAndCountAll({ where: condition, limit, offset }) .then(data =&gt; { const response = getPagingData(data, page, limit); res.send(response); }) .catch(err =&gt; { res.status(500).send({ message: err.message || \"Some error occurred while retrieving tutorials.\" }); });};// find all published Tutorial Create Node.js Express API layer module.exports = app =&gt; {const tutorials = require(\"../controllers/tutorial.controller.js\");var router = require(\"express\").Router();// Retrieve all Tutorialsrouter.get(\"/\", tutorials.findAll);// Retrieve all published Tutorialsrouter.get(\"/published\", tutorials.findAllPublished);...app.use('/api/tutorials', router);}; Accept input from the command line in Node.js readable stream such as process.stdin and use module readline to perform access input from the command line```JSconst readline = require(‘readline’).createInterface({input: process.stdin,output: process.stdout,}); readline.question(What's your name?, name =&gt; { console.log(Hi ${name}!); readline.close();});Another example to perform by `inquirer````jsconst inquirer = require('inquirer');const questions = [ { type: 'input', name: 'name', message: \"What's your name?\", },];inquirer.prompt(questions).then(answers =&gt; { console.log(`Hi ${answers.name}!`);});Expose functionality from a Node.js file using exportsWhen you want to import something you useconst library = require('.library')This is what the module.exports API offfered by the module system to allow us to do.You can do so in 2 ways// car.jsconst car = { brand: 'Ford', model: 'Fiesta',};module.exports = car;// index.jsconst car = require('./car')The second way is to add the exported object as a property of exports. This way allows you to export multiple objects, functions or data:const car = { brand: 'Ford', model: 'Fiesta',};exports.car = car;Or directlyexports.car = { brand: 'Ford', model: 'Fiesta',};And in the other file, you’ll use it by referencing a property of your import:const items = require('./car');const { car } = items;Or you can use a destructuring assigementconst { car } = require('./car')Example// car.jsexports.car = { brand: 'Ford', model: 'Fiesta',};module.exports = { brand: 'Tesla', model: 'Model S',};// app.jsconst tesla = require('./car');const ford = require('./car').car;console.log(tesla, ford);This will print { brand: ‘Tesla’, model: ‘Model S’ } undefined since the require function’s return value has been updated to the object that module.exports points to, so the property that exports added can’t be accessed.An introduction to the npm package manager npm yarn pnpmInstall from single packageOften you’ll see more flags added to this command: –save-dev installs and adds the entry to the package.json file devDependencies –no-save installs but does not add the entry to the package.json file dependencies –save-optional installs and adds the entry to the package.json file optionalDependencies –no-optional will prevent optional dependencies from being installedShorthands of the flags can also be used: -S: –save -D: –save-dev -O: –save-optionalTo Update npm updatenpm update &lt;package-name&gt; You can manage version by npm. Install or update package with version by command npm install &lt;package-name&gt;@&lt;version&gt; Running Tasks npm run &lt;task-name&gt; {\"scripts\": { \"watch\": \"webpack --watch --progress --colors --config webpack.conf.js\", \"dev\": \"webpack --progress --colors --config webpack.conf.js\", \"prod\": \"NODE_ENV=production webpack -p --config webpack.conf.js\",}} Where does npm install the packages? a local install a global installBy default, the package is installed in the current file tree, under the node_modules subfolder. A global installation is performed using the -g flag The package.json guide The file structure of package.json is present in Json file copy{\"name\": \"test-project\",\"version\": \"1.0.0\",\"description\": \"A Vue.js project\",\"main\": \"src/main.js\",\"private\": true,\"scripts\": { \"dev\": \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js\", \"start\": \"npm run dev\", \"unit\": \"jest --config test/unit/jest.conf.js --coverage\", \"test\": \"npm run unit\", \"lint\": \"eslint --ext .js,.vue src test/unit\", \"build\": \"node build/build.js\"},\"dependencies\": { \"vue\": \"^2.5.2\"},\"devDependencies\": { \"autoprefixer\": \"^7.1.2\", \"babel-core\": \"^6.22.1\", \"babel-eslint\": \"^8.2.1\", \"babel-helper-vue-jsx-merge-props\": \"^2.0.3\", \"babel-jest\": \"^21.0.2\", \"babel-loader\": \"^7.1.1\", \"babel-plugin-dynamic-import-node\": \"^1.2.0\", \"babel-plugin-syntax-jsx\": \"^6.18.0\", \"babel-plugin-transform-es2015-modules-commonjs\": \"^6.26.0\", \"babel-plugin-transform-runtime\": \"^6.22.0\", \"babel-plugin-transform-vue-jsx\": \"^3.5.0\", \"babel-preset-env\": \"^1.3.2\", \"babel-preset-stage-2\": \"^6.22.0\", \"chalk\": \"^2.0.1\", \"copy-webpack-plugin\": \"^4.0.1\", \"css-loader\": \"^0.28.0\", \"eslint\": \"^4.15.0\", \"eslint-config-airbnb-base\": \"^11.3.0\", \"eslint-friendly-formatter\": \"^3.0.0\", \"eslint-import-resolver-webpack\": \"^0.8.3\", \"eslint-loader\": \"^1.7.1\", \"eslint-plugin-import\": \"^2.7.0\", \"eslint-plugin-vue\": \"^4.0.0\", \"extract-text-webpack-plugin\": \"^3.0.0\", \"file-loader\": \"^1.1.4\", \"friendly-errors-webpack-plugin\": \"^1.6.1\", \"html-webpack-plugin\": \"^2.30.1\", \"jest\": \"^22.0.4\", \"jest-serializer-vue\": \"^0.3.0\", \"node-notifier\": \"^5.1.2\", \"optimize-css-assets-webpack-plugin\": \"^3.2.0\", \"ora\": \"^1.2.0\", \"portfinder\": \"^1.0.13\", \"postcss-import\": \"^11.0.0\", \"postcss-loader\": \"^2.0.8\", \"postcss-url\": \"^7.2.1\", \"rimraf\": \"^2.6.0\", \"semver\": \"^5.3.0\", \"shelljs\": \"^0.7.6\", \"uglifyjs-webpack-plugin\": \"^1.1.1\", \"url-loader\": \"^0.5.8\", \"vue-jest\": \"^1.0.2\", \"vue-loader\": \"^13.3.0\", \"vue-style-loader\": \"^3.0.1\", \"vue-template-compiler\": \"^2.5.2\", \"webpack\": \"^3.6.0\", \"webpack-bundle-analyzer\": \"^2.9.0\", \"webpack-dev-server\": \"^2.9.1\", \"webpack-merge\": \"^4.1.0\"},\"engines\": { \"node\": \"&gt;= 6.0.0\", \"npm\": \"&gt;= 3.0.0\"},\"browserslist\": [\"&gt; 1%\", \"last 2 versions\", \"not ie &lt;= 8\"]} There are lots of things going on here: version indicates the current version name sets the application/package name description is a brief description of the app/package main sets the entry point for the application private if set to true prevents the app/package to be accidentally published on npm scripts defines a set of node scripts you can run dependencies sets a list of npm packages installed as dependencies devDependencies sets a list of npm packages installed as development dependencies engines sets which versions of Node.js this package/app works on browserslist is used to tell which browsers (and their versions) you want to support The package-lock.json file The goal of package-lock.json file is to keep track of the exact version of every package that is installed so that a product is 100% reproducible in the same way even if packages are updated by their maintainers. Find the installed version of an npm package To see the version of all installed npm packages, including their dependencies npm list for example ❯ npm list/Users/joe/dev/node/cowsay└─┬ cowsay@1.3.1├── get-stdin@5.0.1├─┬ optimist@0.6.1│ ├── minimist@0.0.10│ └── wordwrap@0.0.3├─┬ string-width@2.1.1│ ├── is-fullwidth-code-point@2.0.0│ └─┬ strip-ansi@4.0.0│ └── ansi-regex@3.0.0└── strip-eof@1.0.0 To get only your top-level packages (basically, the ones you told npm to install and you listed in the package.json), run npm list –depth=0: Install an older version of an npm package You might also be interested in listing all the previous versions of a package. You can do it with npm view versions:```bash❯ npm view cowsay versions [ ‘1.0.0’, ‘1.0.1’, ‘1.0.2’, ‘1.0.3’, ‘1.1.0’, ‘1.1.1’, ‘1.1.2’, ‘1.1.3’, ‘1.1.4’, ‘1.1.5’, ‘1.1.6’, ‘1.1.7’, ‘1.1.8’, ‘1.1.9’, ‘1.2.0’, ‘1.2.1’, ‘1.3.0’, ‘1.3.1’ ]Install package```bashnpm install &lt;package&gt;@&lt;version&gt;Semantic Versioning using npmAll versions have 3 digits: x.y.z the first digit is the major version the second digit is the minor version the third digit is the patch versionLet’s see those rules in detail: ^: It will only do updates that do not change the leftmost non-zero number i.e there can be changes in minor version or patch version but not in major version. If you write ^13.1.0, when running npm update, it can update to 13.2.0, 13.3.0 even 13.3.1, 13.3.2 and so on, but not to 14.0.0 or above. ~: if you write ~0.13.0 when running npm update it can update to patch releases: 0.13.1 is ok, but 0.14.0 is not. &gt;: you accept any version higher than the one you specify &gt;=: you accept any version equal to or higher than the one you specify &lt;=: you accept any version equal or lower to the one you specify &lt;: you accept any version lower than the one you specify =: you accept that exact version -: you accept a range of versions. Example: 2.1.0 - 2.6.2   : you combine sets. Example: &lt; 2.1   &gt; 2.6 no symbol: you accept only that specific version you specify (1.2.1) latest: you want to use the latest version available Uninstalling npm packages npm uninstall &lt;package-name&gt;npm uninstall -g &lt;package-name&gt; npm global or local packages In your code you can only require local packages: require('package-name') A package should be installed globally when it provides an executable command that you run from the shell (CLI), and it’s reused across projects.Great examples of popular global packages which you might know are npm vue-cli grunt-cli mocha react-native-cli gatsby-cli forever nodemonYou probably have some packages installed globally already on your system. You can see them by running npm list -g --depth 0 npm dependencies and devDependencies When you install an npm package, you are installing it as a dependency.When you install with option --save-dev, you are installing it as a development dependency.development dependencies are intended as development-only packages, that are unneeded in production. So when you run comman npm install in production you should need include flag production such as npm install --production. The npx Node.js Package Runner npm allow you to run that npm command without installing it first. If the command isn’t found, npx will install it into a central cache npx cowsay \"Hello\" Now, this is a funny useless command. Other scenarios include: running the vue CLI tool to create new applications and run them: npx @vue/cli create my-vue-app creating a new React app using create-react-app: npx create-react-app my-react-app Run some code using a different Node.js version npx node@10 -v #v10.18.1npx node@12 -v #v12.14.1 Run arbitrary code snippets directly from a URL npx https://gist.github.com/zkat/4bc19503fe9e9309e2bfaa2c58074d32 Understanding process.nextTick() Every time the event loop takes a full trip, we call it a tick. process.nextTick(() =&gt; {// do something}); The event loop is busy to run the current code. When this operation ends, the JS engine runs all the functions passed to nextTick calls during that operation.It the way we can tell the JS engine to process a function asynchronously, but as soon as possible, not queue it.Calling setTimeout(() =&gt; {}, 0) will execute the function at the end of next tick, much later than when using nextTick() which prioritizes the call and executes it just before the beginning of the next tick. Use nextTick() when you want to make sure that in the next event loop iteration that code is already executed.Understanding setImmediate()Any function passed as the setImmediate() argument is a callback that’s executed in the next iteration of the event loop.How is setImmediate() different from setTimeout(() =&gt; {}, 0) (passing a 0ms timeout), and from process.nextTick() and Promise.then()?A function passed to process.nextTick() is going to be executed on the current iteration of the event loop, after the current operation ends. This means it will always execute before setTimeout and setImmediate.A setTimeout() callback with a 0ms delay is very similar to setImmediate. The execution order will depend on various factors, but they will be both run in the next iteration of the event loop.A process.nextTick callback is added to process.nextTick queue. A Promise.then() callback is added to promise.microtask queue. A setTimeou, setImmediate callback is added to macrotask queueconst baz = () =&gt; console.log('baz');const foo = () =&gt; console.log('foo');const zoo = () =&gt; console.log('zoo');const start = () =&gt; { console.log('start'); setImmediate(baz); new Promise((resolve, reject) =&gt; { resolve('bar'); }).then((resolve) =&gt; { console.log(resolve); process.nextTick(zoo); }); process.nextTick(foo);};start();// start foo bar zoo bazThis code will first call start(), then call foo() in process.nextTick queue. After that, it will handle promises microtask queue, which prints bar and adds zoo() in process.nextTick queue at the same time. Then it will call zoo() which has just been added. In the end, the baz() in macrotask queue is called.The Node.js Event emitterOn the backend side, Node.js offers us the option to build a similar system using event module.This module, inparticular, offers the EventEmitter class, which we’ll use to handle our events.You initializa that usingconst EventEmitter = require('events')const eventEmitter = new EventEmitter();This object exposes, among many others, the on and emit methods. emit is used to trigger an event on is used to add a callback function that’s going to be executed when the event is triggered.For example, let’s create a start event, and as a matter of providing a sample, we react to that by just logging to the console: eventEmitter.on('start', () =&gt; { console.log('started');}); When we run eventEmitter.emit('start'); You can pass argument to the event handler by passing them as additional argument to emit```jseventEmitter.on(‘start’, (start, end) =&gt; {console.log(started from ${start} to ${end});}); eventEmitter.emit(‘start’, 1, 100);The EventEmitter object also exposes several other methods to interact with events, like- once(): add a one-time listener- removeListener() / off(): remove an event listener from an event- removeAllListeners(): remove all listeners for an event.References [docs](https://nodejs.org/api/events.html)## REST API Development with Node.js, Express, and MongoDBTutorial basic to help create a rest api framework with node.js by express and mongodb using JavaScript ES2015. References to [github project](https://github.com/maitraysuthar/rest-api-nodejs-mongodb)### Software Requirements- Node.js 8+- MongoDB 3.6+ (Recommended 4+)### How to install- b1```shgit clone https://github.com/maitraysuthar/rest-api-nodejs-mongodb.git b2 cd rest-api-nodejs-mongodbnpm install b3 Setting up environments You will find a file named .env.example on root directory of project. Create a new file by copying and pasting the file and then renaming it to just .env cp .env.example .env The file .env is already ignored, so you never commit your credentials. Change the values of the file to your environment. Helpful comments added to .env.example file to understand the constants. Project structure .├── app.js├── package.json├── bin│ └── www├── controllers│ ├── AuthController.js│ └── BookController.js├── models│ ├── BookModel.js│ └── UserModel.js├── routes│ ├── api.js│ ├── auth.js│ └── book.js├── middlewares│ ├── jwt.js├── helpers│ ├── apiResponse.js│ ├── constants.js│ ├── mailer.js│ └── utility.js├── test│ ├── testConfig.js│ ├── auth.js│ └── book.js└── public ├── index.html └── stylesheets └── style.css b5: Running API server locally npm run dev Integration Write new model: If you need to add more models to the project just create a new file in /models/ and use them in the controllers. Creating new routes: If you need to add more routes to the project just create a new file in /routes/ and add it in /routes/api.js it will be loaded dynamically. Creating new controllers: If you need to add more controllers to the project just create a new file in /controllers/ and use them in the routes. Running Test Cases Running Eslint npm run lint Prequise npm Express JS Mongo DB Rest API integrate Authentication and CRUDpackage.json example package manage installed {\"name\": \"rest-api-nodejs-mongodb\",\"version\": \"1.0.0\",\"private\": true,\"scripts\": { \"start\": \"node ./bin/www\", \"dev\": \"nodemon ./bin/www\", \"test\": \"nyc _mocha --timeout 10000 --exit --report lcovonly -- -R spec\", \"lint\": \"eslint --fix --config .eslintrc.json \\\"**/*.js\\\"\"},\"dependencies\": { \"bcrypt\": \"^3.0.6\", \"codacy-coverage\": \"^3.4.0\", \"cookie-parser\": \"~1.4.3\", \"cors\": \"^2.8.5\", \"debug\": \"~2.6.9\", \"dotenv\": \"^8.2.0\", \"express\": \"~4.16.0\", \"express-jwt\": \"^5.3.1\", \"express-validator\": \"^6.2.0\", \"jsonwebtoken\": \"^8.5.1\", \"mocha-lcov-reporter\": \"^1.3.0\", \"moment\": \"^2.24.0\", \"mongoose\": \"^5.7.6\", \"morgan\": \"~1.9.0\", \"nodemailer\": \"^6.3.1\"},\"devDependencies\": { \"chai\": \"^4.2.0\", \"chai-http\": \"^4.3.0\", \"eslint\": \"^6.5.1\", \"mocha\": \"^6.2.2\", \"nodemon\": \"^1.19.4\", \"nyc\": \"^14.1.1\"}} Authentication NodeJs package express is popular package help to create server. JWT ```jsconst jwt = require(“express-jwt”);const secret = process.env.JWT_SECRET; const authenticate = jwt({ secret: secret});module.exports = authenticate;The mechanism of authentication when user register or login into server that user will receive a token was generated by server in response to user. To generate token by JWT, nodejs provide a package `express-jwt`.Both `JWT_SECRET` and `JWT_TIMEOUT_DURATION` variable is configure in `.env` file.MONGODB_URL=mongodb://127.0.0.1:27017/rest-api-nodejs-mongodbExample Connection String:-mongodb://127.0.0.1:27017/rest-api-nodejs-mongodbmongodb://[MongodbHost]:[PORT]/[DatabaseName]JWT_SECRET=abcdefghijklmnopqrstuvwxyz1234567890Example Secret:- abcdefghijklmnopqrstuvwxyz1234567890JWT_TIMEOUT_DURATION=”2 hours”You can place duration available here: https://github.com/auth0/node-jsonwebtoken#usageSearch for “expiresIn” option on above link.EMAIL_SMTP_HOST=YourSMTPHostEMAIL_SMTP_PORT=YourSMTPPortEMAIL_SMTP_USERNAME=YourSMTPUsernameEMAIL_SMTP_PASSWORD=YourSMTPPasswordtrue for 465, false for other portsEMAIL_SMTP_SECURE=false#### Routercreate file `auth.js` inside `routes` folder with content:```jsvar express = require(\"express\");const AuthController = require(\"../controllers/AuthController\");var router = express.Router();router.post(\"/register\", AuthController.register);router.post(\"/login\", AuthController.login);router.post(\"/verify-otp\", AuthController.verifyConfirm);router.post(\"/resend-verify-otp\", AuthController.resendConfirmOtp);module.exports = router; Import express module and controller AuthController var express = require(\"express\");const AuthController = require(\"../controllers/AuthController\"); Router apis: register, login, verify-otp, ‘resend-verify-otp’. Such apis was require package nodemailer and variable configure in .env.index servevar express = require(\"express\");var router = express.Router();/* GET home page. */router.get(\"/\", function(req, res) { res.render(\"index\", { title: \"Express\" });});module.exports = router;api routervar express = require(\"express\");var authRouter = require(\"./auth\");var bookRouter = require(\"./book\");var app = express();app.use(\"/auth/\", authRouter);app.use(\"/book/\", bookRouter);module.exports = app;book routervar express = require(\"express\");const BookController = require(\"../controllers/BookController\");var router = express.Router();router.get(\"/\", BookController.bookList);router.get(\"/:id\", BookController.bookDetail);router.post(\"/\", BookController.bookStore);router.put(\"/:id\", BookController.bookUpdate);router.delete(\"/:id\", BookController.bookDelete);module.exports = router;AuthControllerconst UserModel = require(\"../models/UserModel\");const { body,validationResult } = require(\"express-validator\");const { sanitizeBody } = require(\"express-validator\");//helper file to prepare responses.const apiResponse = require(\"../helpers/apiResponse\");const utility = require(\"../helpers/utility\");const bcrypt = require(\"bcrypt\");const jwt = require(\"jsonwebtoken\");const mailer = require(\"../helpers/mailer\");const { constants } = require(\"../helpers/constants\");/** * User registration. * * @param {string} firstName * @param {string} lastName * @param {string} email * @param {string} password * * @returns {Object} */exports.register = [ // Validate fields. body(\"firstName\").isLength({ min: 1 }).trim().withMessage(\"First name must be specified.\") .isAlphanumeric().withMessage(\"First name has non-alphanumeric characters.\"), body(\"lastName\").isLength({ min: 1 }).trim().withMessage(\"Last name must be specified.\") .isAlphanumeric().withMessage(\"Last name has non-alphanumeric characters.\"), body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\").custom((value) =&gt; { return UserModel.findOne({email : value}).then((user) =&gt; { if (user) { return Promise.reject(\"E-mail already in use\"); } }); }), body(\"password\").isLength({ min: 6 }).trim().withMessage(\"Password must be 6 characters or greater.\"), // Sanitize fields. sanitizeBody(\"firstName\").escape(), sanitizeBody(\"lastName\").escape(), sanitizeBody(\"email\").escape(), sanitizeBody(\"password\").escape(), // Process request after validation and sanitization. (req, res) =&gt; { try { // Extract the validation errors from a request. const errors = validationResult(req); if (!errors.isEmpty()) { // Display sanitized values/errors messages. return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { //hash input password bcrypt.hash(req.body.password,10,function(err, hash) { // generate OTP for confirmation let otp = utility.randomNumber(4); // Create User object with escaped and trimmed data var user = new UserModel( { firstName: req.body.firstName, lastName: req.body.lastName, email: req.body.email, password: hash, confirmOTP: otp } ); // Html email body let html = \"&lt;p&gt;Please Confirm your Account.&lt;/p&gt;&lt;p&gt;OTP: \"+otp+\"&lt;/p&gt;\"; // Send confirmation email mailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email }; return apiResponse.successResponseWithData(res,\"Registration Success.\", userData); }); }).catch(err =&gt; { console.log(err); return apiResponse.ErrorResponse(res,err); }) ; }); } } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * User login. * * @param {string} email * @param {string} password * * @returns {Object} */exports.login = [ body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\"), body(\"password\").isLength({ min: 1 }).trim().withMessage(\"Password must be specified.\"), sanitizeBody(\"email\").escape(), sanitizeBody(\"password\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { UserModel.findOne({email : req.body.email}).then(user =&gt; { if (user) { //Compare given password with db's hash. bcrypt.compare(req.body.password,user.password,function (err,same) { if(same){ //Check account confirmation. if(user.isConfirmed){ // Check User's account active or not. if(user.status) { let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email, }; //Prepare JWT token for authentication const jwtPayload = userData; const jwtData = { expiresIn: process.env.JWT_TIMEOUT_DURATION, }; const secret = process.env.JWT_SECRET; //Generated JWT token with Payload and secret. userData.token = jwt.sign(jwtPayload, secret, jwtData); return apiResponse.successResponseWithData(res,\"Login Success.\", userData); }else { return apiResponse.unauthorizedResponse(res, \"Account is not active. Please contact admin.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Account is not confirmed. Please confirm your account.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Email or Password wrong.\"); } }); }else{ return apiResponse.unauthorizedResponse(res, \"Email or Password wrong.\"); } }); } } catch (err) { return apiResponse.ErrorResponse(res, err); } }];/** * Verify Confirm otp. * * @param {string} email * @param {string} otp * * @returns {Object} */exports.verifyConfirm = [ body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\"), body(\"otp\").isLength({ min: 1 }).trim().withMessage(\"OTP must be specified.\"), sanitizeBody(\"email\").escape(), sanitizeBody(\"otp\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { var query = {email : req.body.email}; UserModel.findOne(query).then(user =&gt; { if (user) { //Check already confirm or not. if(!user.isConfirmed){ //Check account confirmation. if(user.confirmOTP == req.body.otp){ //Update user as confirmed UserModel.findOneAndUpdate(query, { isConfirmed: 1, confirmOTP: null }).catch(err =&gt; { return apiResponse.ErrorResponse(res, err); }); return apiResponse.successResponse(res,\"Account confirmed success.\"); }else{ return apiResponse.unauthorizedResponse(res, \"Otp does not match\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Account already confirmed.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Specified email not found.\"); } }); } } catch (err) { return apiResponse.ErrorResponse(res, err); } }];/** * Resend Confirm otp. * * @param {string} email * * @returns {Object} */exports.resendConfirmOtp = [ body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\"), sanitizeBody(\"email\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { var query = {email : req.body.email}; UserModel.findOne(query).then(user =&gt; { if (user) { //Check already confirm or not. if(!user.isConfirmed){ // Generate otp let otp = utility.randomNumber(4); // Html email body let html = \"&lt;p&gt;Please Confirm your Account.&lt;/p&gt;&lt;p&gt;OTP: \"+otp+\"&lt;/p&gt;\"; // Send confirmation email mailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ user.isConfirmed = 0; user.confirmOTP = otp; // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } return apiResponse.successResponse(res,\"Confirm otp sent.\"); }); }); }else{ return apiResponse.unauthorizedResponse(res, \"Account already confirmed.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Specified email not found.\"); } }); } } catch (err) { return apiResponse.ErrorResponse(res, err); } }]; ExpressJs support MVC architechture, In controller have actions provide authen such as register, login, confirmOTP, resendConfirm Model UserModel store and query data from mongoDB. const UserModel = require(\"../models/UserModel\");const { body,validationResult } = require(\"express-validator\");const { sanitizeBody } = require(\"express-validator\");//helper file to prepare responses.const apiResponse = require(\"../helpers/apiResponse\");const utility = require(\"../helpers/utility\");const bcrypt = require(\"bcrypt\");const jwt = require(\"jsonwebtoken\");const mailer = require(\"../helpers/mailer\");const { constants } = require(\"../helpers/constants\"); Validation package express-validator take the body and validation module body(\"firstName\").isLength({ min: 1 }).trim().withMessage(\"First name must be specified.\") .isAlphanumeric().withMessage(\"First name has non-alphanumeric characters.\"), body(\"lastName\").isLength({ min: 1 }).trim().withMessage(\"Last name must be specified.\") .isAlphanumeric().withMessage(\"Last name has non-alphanumeric characters.\"), body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\").custom((value) =&gt; { return UserModel.findOne({email : value}).then((user) =&gt; { if (user) { return Promise.reject(\"E-mail already in use\"); } }); }), body(\"password\").isLength({ min: 6 }).trim().withMessage(\"Password must be 6 characters or greater.\") Each line is element of array impement by export function. Express will run through each element and store variable in memory and same stack. The validation, The request parameter, The process element, so on. (req, res) =&gt; { try { // Extract the validation errors from a request. const errors = validationResult(req); if (!errors.isEmpty()) { // Display sanitized values/errors messages. return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { //hash input password bcrypt.hash(req.body.password,10,function(err, hash) { // generate OTP for confirmation let otp = utility.randomNumber(4); // Create User object with escaped and trimmed data var user = new UserModel( { firstName: req.body.firstName, lastName: req.body.lastName, email: req.body.email, password: hash, confirmOTP: otp } ); // Html email body let html = \"&lt;p&gt;Please Confirm your Account.&lt;/p&gt;&lt;p&gt;OTP: \"+otp+\"&lt;/p&gt;\"; // Send confirmation email mailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email }; return apiResponse.successResponseWithData(res,\"Registration Success.\", userData); }); }).catch(err =&gt; { console.log(err); return apiResponse.ErrorResponse(res,err); }) ; }); } } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } } code inside is same with JS code. After validation, the action register move to process logic register with user attribute save to DB. A mail send to user when sucessful saved. Try catch for unexpected errors: bcrypt module for hash password and this is a function js in ECMASCRIPT 2015 bcrypt.hash(req.body.password,10,function(err, hash) { }).catch(err =&gt; { console.log(err); return apiResponse.ErrorResponse(res,err);}) ; (req, res) is object request and response and parameters of function register. Response send to user // sucessreturn apiResponse.successResponseWithData(res,\"Registration Success.\", userData);// errorreturn apiResponse.ErrorResponse(res,err); About the api resonpose structure in json format and js code, used jsonwebtoken and express-jwt module for generate token and authentication user request.```jsexports.successResponse = function (res, msg) { var data = { status: 1, message: msg }; return res.status(200).json(data);};exports.successResponseWithData = function (res, msg, data) { var resData = { status: 1, message: msg, data: data }; return res.status(200).json(resData);};exports.ErrorResponse = function (res, msg) { var data = { status: 0, message: msg, }; return res.status(500).json(data);};exports.notFoundResponse = function (res, msg) { var data = { status: 0, message: msg, }; return res.status(404).json(data);};exports.validationErrorWithData = function (res, msg, data) { var resData = { status: 0, message: msg, data: data }; return res.status(400).json(resData);};exports.unauthorizedResponse = function (res, msg) { var data = { status: 0, message: msg, }; return res.status(401).json(data);};- Register function is trigger a procedure send email to user register with otp in a email, after email register and confirm otp send a user register save to db and api response register api send to user```jsmailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email }; return apiResponse.successResponseWithData(res,\"Registration Success.\", userData); }); }).catch(err =&gt; { console.log(err); return apiResponse.ErrorResponse(res,err); }) ;Login methodJs codeexports.login = [ body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\"), body(\"password\").isLength({ min: 1 }).trim().withMessage(\"Password must be specified.\"), sanitizeBody(\"email\").escape(), sanitizeBody(\"password\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { UserModel.findOne({email : req.body.email}).then(user =&gt; { if (user) { //Compare given password with db's hash. bcrypt.compare(req.body.password,user.password,function (err,same) { if(same){ //Check account confirmation. if(user.isConfirmed){ // Check User's account active or not. if(user.status) { let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email, }; //Prepare JWT token for authentication const jwtPayload = userData; const jwtData = { expiresIn: process.env.JWT_TIMEOUT_DURATION, }; const secret = process.env.JWT_SECRET; //Generated JWT token with Payload and secret. userData.token = jwt.sign(jwtPayload, secret, jwtData); return apiResponse.successResponseWithData(res,\"Login Success.\", userData); }else { return apiResponse.unauthorizedResponse(res, \"Account is not active. Please contact admin.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Account is not confirmed. Please confirm your account.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Email or Password wrong.\"); } }); }else{ return apiResponse.unauthorizedResponse(res, \"Email or Password wrong.\"); } }); } } catch (err) { return apiResponse.ErrorResponse(res, err); } }]; bcrypt module provide function compare password with user.password. Promise function search in DB and find user exist with email. UserModel.findOne({email : req.body.email}).then(user =&gt; {...});bcrypt.compare(req.body.password,user.password,function (err,same) {}); Check User status and send response with JWT token generate for user: userData.token = jwt.sign(jwtPayload, secret, jwtData); in expired time. if(user.status) { let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email, }; //Prepare JWT token for authentication const jwtPayload = userData; const jwtData = { expiresIn: process.env.JWT_TIMEOUT_DURATION, }; const secret = process.env.JWT_SECRET; //Generated JWT token with Payload and secret. userData.token = jwt.sign(jwtPayload, secret, jwtData); return apiResponse.successResponseWithData(res,\"Login Success.\", userData); }else { return apiResponse.unauthorizedResponse(res, \"Account is not active. Please contact admin.\"); } Verify OTP verify otp from user with database. var query = {email : req.body.email};UserModel.findOne(query).then(user =&gt; { if (user) { //Check already confirm or not. if(!user.isConfirmed){ //Check account confirmation. if(user.confirmOTP == req.body.otp){ //Update user as confirmed UserModel.findOneAndUpdate(query, { isConfirmed: 1, confirmOTP: null }).catch(err =&gt; { return apiResponse.ErrorResponse(res, err); }); return apiResponse.successResponse(res,\"Account confirmed success.\"); }else{ return apiResponse.unauthorizedResponse(res, \"Otp does not match\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Account already confirmed.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Specified email not found.\"); }}); Resend OTP query to get email searching email on db and resend otp to user request resend otp. var query = {email : req.body.email};UserModel.findOne(query).then(user =&gt; { if (user) { //Check already confirm or not. if(!user.isConfirmed){ // Generate otp let otp = utility.randomNumber(4); // Html email body let html = \"&lt;p&gt;Please Confirm your Account.&lt;/p&gt;&lt;p&gt;OTP: \"+otp+\"&lt;/p&gt;\"; // Send confirmation email mailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ user.isConfirmed = 0; user.confirmOTP = otp; // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } return apiResponse.successResponse(res,\"Confirm otp sent.\"); }); }); }else{ return apiResponse.unauthorizedResponse(res, \"Account already confirmed.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Specified email not found.\"); }}); Promise function in request often used in Express to asynchronous request nodejs. Mailer function Mailer to send email is support by nodemailer module. send function was eported```jsconst nodemailer = require(“nodemailer”); // create reusable transporter object using the default SMTP transportlet transporter = nodemailer.createTransport({ host: process.env.EMAIL_SMTP_HOST, port: process.env.EMAIL_SMTP_PORT, //secure: process.env.EMAIL_SMTP_SECURE, // lack of ssl commented this. You can uncomment it. auth: { user: process.env.EMAIL_SMTP_USERNAME, pass: process.env.EMAIL_SMTP_PASSWORD }});exports.send = function (from, to, subject, html){ // send mail with defined transport object // visit https://nodemailer.com/ for more options return transporter.sendMail({ from: from, // sender address e.g. no-reply@xyz.com or “Fred Foo 👻” foo@example.com to: to, // list of receivers e.g. bar@example.com, baz@example.com subject: subject, // Subject line e.g. ‘Hello ✔’ //text: text, // plain text body e.g. Hello world? html: html // html body e.g. ‘Hello world?’ });};### Utily to generate OTPGenerate otp for confirm user request```jsexports.randomNumber = function (length) { var text = \"\"; var possible = \"123456789\"; for (var i = 0; i &lt; length; i++) { var sup = Math.floor(Math.random() * possible.length); text += i &gt; 0 &amp;&amp; sup == i ? \"0\" : possible.charAt(sup); } return Number(text);};Constants dataConfirmEmails from, admin.exports.constants = { admin: { name: \"admin\", email: \"admin@admin.com\" }, confirmEmails: { from : \"no-reply@test-app.com\" }};User modelUser model to query data from monggodbvar mongoose = require(\"mongoose\");var UserSchema = new mongoose.Schema({ firstName: {type: String, required: true}, lastName: {type: String, required: true}, email: {type: String, required: true}, password: {type: String, required: true}, isConfirmed: {type: Boolean, required: true, default: 0}, confirmOTP: {type: String, required:false}, otpTries: {type: Number, required:false, default: 0}, status: {type: Boolean, required: true, default: 1}}, {timestamps: true});// Virtual for user's full nameUserSchema .virtual(\"fullName\") .get(function () { return this.firstName + \" \" + this.lastName; });module.exports = mongoose.model(\"User\", UserSchema);HTTP server handle request and response#!/usr/bin/env node/** * Module dependencies. */var app = require('../app');var debug = require('debug')('rest-api-nodejs-mongodb:server');var http = require('http');/** * Get port from environment and store in Express. */var port = normalizePort(process.env.PORT || '3000');app.set('port', port);/** * Create HTTP server. */var server = http.createServer(app);/** * Listen on provided port, on all network interfaces. */server.listen(port);server.on('error', onError);server.on('listening', onListening);/** * Normalize a port into a number, string, or false. */function normalizePort(val) { var port = parseInt(val, 10); if (isNaN(port)) { // named pipe return val; } if (port &gt;= 0) { // port number return port; } return false;}/** * Event listener for HTTP server \"error\" event. */function onError(error) { if (error.syscall !== 'listen') { throw error; } var bind = typeof port === 'string' ? 'Pipe ' + port : 'Port ' + port; // handle specific listen errors with friendly messages switch (error.code) { case 'EACCES': console.error(bind + ' requires elevated privileges'); process.exit(1); break; case 'EADDRINUSE': console.error(bind + ' is already in use'); process.exit(1); break; default: throw error; }}/** * Event listener for HTTP server \"listening\" event. */function onListening() { var addr = server.address(); var bind = typeof addr === 'string' ? 'pipe ' + addr : 'port ' + addr.port; debug('Listening on ' + bind);}Book rest api handleBookcontroller will handle function as actions: book list, book detail, book store, book update, and book delete.Book controllerconst Book = require(\"../models/BookModel\");const { body,validationResult } = require(\"express-validator\");const { sanitizeBody } = require(\"express-validator\");const apiResponse = require(\"../helpers/apiResponse\");const auth = require(\"../middlewares/jwt\");var mongoose = require(\"mongoose\");mongoose.set(\"useFindAndModify\", false);// Book Schemafunction BookData(data) { this.id = data._id; this.title= data.title; this.description = data.description; this.isbn = data.isbn; this.createdAt = data.createdAt;}/** * Book List. * * @returns {Object} */exports.bookList = [ auth, function (req, res) { try { Book.find({user: req.user._id},\"_id title description isbn createdAt\").then((books)=&gt;{ if(books.length &gt; 0){ return apiResponse.successResponseWithData(res, \"Operation success\", books); }else{ return apiResponse.successResponseWithData(res, \"Operation success\", []); } }); } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * Book Detail. * * @param {string} id * * @returns {Object} */exports.bookDetail = [ auth, function (req, res) { if(!mongoose.Types.ObjectId.isValid(req.params.id)){ return apiResponse.successResponseWithData(res, \"Operation success\", {}); } try { Book.findOne({_id: req.params.id,user: req.user._id},\"_id title description isbn createdAt\").then((book)=&gt;{ if(book !== null){ let bookData = new BookData(book); return apiResponse.successResponseWithData(res, \"Operation success\", bookData); }else{ return apiResponse.successResponseWithData(res, \"Operation success\", {}); } }); } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * Book store. * * @param {string} title * @param {string} description * @param {string} isbn * * @returns {Object} */exports.bookStore = [ auth, body(\"title\", \"Title must not be empty.\").isLength({ min: 1 }).trim(), body(\"description\", \"Description must not be empty.\").isLength({ min: 1 }).trim(), body(\"isbn\", \"ISBN must not be empty\").isLength({ min: 1 }).trim().custom((value,{req}) =&gt; { return Book.findOne({isbn : value,user: req.user._id}).then(book =&gt; { if (book) { return Promise.reject(\"Book already exist with this ISBN no.\"); } }); }), sanitizeBody(\"*\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); var book = new Book( { title: req.body.title, user: req.user, description: req.body.description, isbn: req.body.isbn }); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); } else { //Save book. book.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } let bookData = new BookData(book); return apiResponse.successResponseWithData(res,\"Book add Success.\", bookData); }); } } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * Book update. * * @param {string} title * @param {string} description * @param {string} isbn * * @returns {Object} */exports.bookUpdate = [ auth, body(\"title\", \"Title must not be empty.\").isLength({ min: 1 }).trim(), body(\"description\", \"Description must not be empty.\").isLength({ min: 1 }).trim(), body(\"isbn\", \"ISBN must not be empty\").isLength({ min: 1 }).trim().custom((value,{req}) =&gt; { return Book.findOne({isbn : value,user: req.user._id, _id: { \"$ne\": req.params.id }}).then(book =&gt; { if (book) { return Promise.reject(\"Book already exist with this ISBN no.\"); } }); }), sanitizeBody(\"*\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); var book = new Book( { title: req.body.title, description: req.body.description, isbn: req.body.isbn, _id:req.params.id }); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); } else { if(!mongoose.Types.ObjectId.isValid(req.params.id)){ return apiResponse.validationErrorWithData(res, \"Invalid Error.\", \"Invalid ID\"); }else{ Book.findById(req.params.id, function (err, foundBook) { if (foundBook === null){ return apiResponse.notFoundResponse(res,\"Book not exists with this id\"); }else{ //Check authorized user if(foundBook.user.toString() !== req.user._id){ return apiResponse.unauthorizedResponse(res, \"You are not authorized to do this operation.\"); }else{ //update book. Book.findByIdAndUpdate(req.params.id, book, {},function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } else { let bookData = new BookData(book); return apiResponse.successResponseWithData(res,\"Book update Success.\", bookData); } }); } } }); } } } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * Book Delete. * * @param {string} id * * @returns {Object} */exports.bookDelete = [ auth, function (req, res) { if(!mongoose.Types.ObjectId.isValid(req.params.id)){ return apiResponse.validationErrorWithData(res, \"Invalid Error.\", \"Invalid ID\"); } try { Book.findById(req.params.id, function (err, foundBook) { if(foundBook === null){ return apiResponse.notFoundResponse(res,\"Book not exists with this id\"); }else{ //Check authorized user if(foundBook.user.toString() !== req.user._id){ return apiResponse.unauthorizedResponse(res, \"You are not authorized to do this operation.\"); }else{ //delete book. Book.findByIdAndRemove(req.params.id,function (err) { if (err) { return apiResponse.ErrorResponse(res, err); }else{ return apiResponse.successResponse(res,\"Book delete Success.\"); } }); } } }); } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];Mongodb will manage by mongoose module.set mode useFindAndModify. Create schema// Book Schemafunction BookData(data) { this.id = data._id; this.title= data.title; this.description = data.description; this.isbn = data.isbn; this.createdAt = data.createdAt;}Book controller require authenticate from middleware jwt and found user of request. Implement auth such as a element and export in controller action.Book listexports.bookList = [ auth, function (req, res) { try { Book.find({user: req.user._id},\"_id title description isbn createdAt\").then((books)=&gt;{ if(books.length &gt; 0){ return apiResponse.successResponseWithData(res, \"Operation success\", books); }else{ return apiResponse.successResponseWithData(res, \"Operation success\", []); } }); } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];bookDetailBook.findOne({_id: req.params.id,user: req.user._id},\"_id title description isbn createdAt\").then((book)=&gt;{ if(book !== null){ let bookData = new BookData(book); return apiResponse.successResponseWithData(res, \"Operation success\", bookData); }else{ return apiResponse.successResponseWithData(res, \"Operation success\", {}); }});Used promise Book.findOne({_id: req.params.id,user: req.user._id},\"_id title description isbn createdAt\").then((book)=&gt;{});. id of book is param of request, and user is param of request after authenticated.bookUpdate params santizied input by function sanitizeBody(\"*\").escape() Use mongoose.Types.ObjectId.isValid(req.params.id) to validate ID in request params.bookDelete Promise for find and delete book Book.findByIdAndRemove(req.params.id,function (err) { if (err) { return apiResponse.ErrorResponse(res, err); }else{ return apiResponse.successResponse(res,\"Book delete Success.\"); }}); Book Model ```jsvar mongoose = require(“mongoose”); var Schema = mongoose.Schema;var BookSchema = new Schema({ title: {type: String, required: true}, description: {type: String, required: true}, isbn: {type: String, required: true}, user: { type: Schema.ObjectId, ref: “User”, required: true },}, {timestamps: true});module.exports = mongoose.model(“Book”, BookSchema);### Test- testConfig.js```js//During the automated test the env variable, We will set it to \"test\"process.env.NODE_ENV = \"test\";process.env.MONGODB_URL = \"mongodb://127.0.0.1:27017/rest-api-nodejs-mongodb-test\";//Require the dev-dependencieslet chai = require(\"chai\");let chaiHttp = require(\"chai-http\");let server = require(\"../app\");let should = chai.should();chai.use(chaiHttp);//Export this to use in multiple filesmodule.exports = { chai: chai, server: server, should: should};Import necessary module for test. Add these lines before test code.const { chai, server, should } = require(\"./testConfig\");const UserModel = require(\"../models/UserModel\");const BookModel = require(\"../models/BookModel\"); auth.js describe(\"Auth\", () =&gt; { // Before each test we empty the database before((done) =&gt; { UserModel.deleteMany({}, (err) =&gt; { done(); }); }); // Prepare data for testing const testData = { \"firstName\":\"test\", \"lastName\":\"testing\", \"password\":\"Test@123\", \"email\":\"maitraysuthar@test12345.com\" }; /** Test the /POST route*/ describe(\"/POST Register\", () =&gt; { it(\"It should send validation error for Register\", (done) =&gt; { chai.request(server) .post(\"/api/auth/register\") .send({\"email\": testData.email}) .end((err, res) =&gt; { res.should.have.status(400); done(); }); }); });}); book.jsdescribe(\"Book\", () =&gt; { //Before each test we empty the database before((done) =&gt; { BookModel.deleteMany({}, (err) =&gt; { done(); }); }); // Prepare data for testing const userTestData = { \"password\":\"Test@123\", \"email\":\"maitraysuthar@test12345.com\" }; // Prepare data for testing const testData = { \"title\":\"testing book\", \"description\":\"testing book desc\", \"isbn\":\"3214htrff4\" }; /* * Test the /POST route */ describe(\"/POST Login\", () =&gt; { it(\"it should do user Login for book\", (done) =&gt; { chai.request(server) .post(\"/api/auth/login\") .send({\"email\": userTestData.email,\"password\": userTestData.password}) .end((err, res) =&gt; { res.should.have.status(200); res.body.should.have.property(\"message\").eql(\"Login Success.\"); userTestData.token = res.body.data.token; done(); }); }); });}); app.jsFile to configure app and config variables```JSvar express = require(“express”);var path = require(“path”);var cookieParser = require(“cookie-parser”);var logger = require(“morgan”);require(“dotenv”).config();var indexRouter = require(“./routes/index”);var apiRouter = require(“./routes/api”);var apiResponse = require(“./helpers/apiResponse”);var cors = require(“cors”);// DB connectionvar MONGODB_URL = process.env.MONGODB_URL;var mongoose = require(“mongoose”);mongoose.connect(MONGODB_URL, { useNewUrlParser: true, useUnifiedTopology: true }).then(() =&gt; { //don’t show the log when it is test if(process.env.NODE_ENV !== “test”) { console.log(“Connected to %s”, MONGODB_URL); console.log(“App is running … \\n”); console.log(“Press CTRL + C to stop the process. \\n”); }}) .catch(err =&gt; { console.error(“App starting error:”, err.message); process.exit(1); });var db = mongoose.connection;var app = express();//don’t show the log when it is testif(process.env.NODE_ENV !== “test”) { app.use(logger(“dev”));}app.use(express.json());app.use(express.urlencoded({ extended: false }));app.use(cookieParser());app.use(express.static(path.join(__dirname, “public”)));//To allow cross-origin requestsapp.use(cors());//Route Prefixesapp.use(“/”, indexRouter);app.use(“/api/”, apiRouter);// throw 404 if URL not foundapp.all(“*”, function(req, res) { return apiResponse.notFoundResponse(res, “Page not found”);});app.use((err, req, res) =&gt; { if(err.name == “UnauthorizedError”){ return apiResponse.unauthorizedResponse(res, err.message); }});module.exports = app;- DB configure from monggo DB```js// DB connectionvar MONGODB_URL = process.env.MONGODB_URL;var mongoose = require(\"mongoose\");mongoose.connect(MONGODB_URL, { useNewUrlParser: true, useUnifiedTopology: true }).then(() =&gt; { //don't show the log when it is test if(process.env.NODE_ENV !== \"test\") { console.log(\"Connected to %s\", MONGODB_URL); console.log(\"App is running ... \\n\"); console.log(\"Press CTRL + C to stop the process. \\n\"); }}) .catch(err =&gt; { console.error(\"App starting error:\", err.message); process.exit(1); });var db = mongoose.connection;Build an HTTP Serverconst http = require('http')const port = process.env.PORT || 3000const server = http.createServer((req, res) =&gt; { res.statusCode = 200 res.setHeader('Content-Type', 'text/html') res.end('&lt;h1&gt;Hello, World!&lt;/h1&gt;')})server.listen(port, () =&gt; { console.log(`Server running at port ${port}`)})Let’s analyze it briefly. We include the http module. We use the module to create an HTTP server. Default port is 3000, the listen callback function is called.Whenever a new request is received, the request event is called, providing two object: a request(an http.IncomingMessage object) and a response (an http.ServerResponse object).res.statusCode = 200;res.setHeader('Content-Type', 'text/html');res.end('&lt;h1&gt;Hello, World!&lt;/h1&gt;');Making HTTP requests with Node.jsGET requestconst axios = require('axios');axios .get('https://example.com/todos') .then(res =&gt; { console.log(`statusCode: ${res.status}`); console.log(res); }) .catch(error =&gt; { console.error(error); });const https = require('https');const options = { hostname: 'example.com', port: 443, path: '/todos', method: 'GET',};const req = https.request(options, res =&gt; { console.log(`statusCode: ${res.statusCode}`); res.on('data', d =&gt; { process.stdout.write(d); });});req.on('error', error =&gt; { console.error(error);});req.end();Post requestSimiliar to making GET request, you can use Axios library to perform POST requestconst axios = require('axios');axios .post('https://whatever.com/todos', { todo: 'Buy the milk', }) .then(res =&gt; { console.log(`statusCode: ${res.status}`); console.log(res); }) .catch(error =&gt; { console.error(error); });Or alternative using https module:const https = require('https');const data = JSON.stringify({ todo: 'Buy the milk',});const options = { hostname: 'whatever.com', port: 443, path: '/todos', method: 'POST', headers: { 'Content-Type': 'application/json', 'Content-Length': data.length, },};const req = https.request(options, res =&gt; { console.log(`statusCode: ${res.statusCode}`); res.on('data', d =&gt; { process.stdout.write(d); });});req.on('error', error =&gt; { console.error(error);});req.write(data);req.end();PUT and DELETEPUT and DELETE is same with POST and GET method but method is PUT or DELETE.Get HTTP request body data using Node.jsIf you are using Express, that’s quite simple: use the express.json() middleware which is available is Express v4.16.0 onwards.const axios = require('axios');axios.post('https://whatever.com/todos', { todo: 'Buy the milk',});This is the matching server side codeconst express = require('express');const app = express();app.use( express.urlencoded({ extended: true, }));app.use(express.json());app.post('/todos', (req, res) =&gt; { console.log(req.body.todo);});If you’re not using Express and you want to do this in vanilla Node.js, you need to do a bit more work, of course, as Express abstracts a lot of this for you.const server = http.createServer((req, res) =&gt; { // we can access HTTP headers req.on('data', chunk =&gt; { console.log(`Data chunk available: ${chunk}`); }); req.on('end', () =&gt; { // end of data });});So to access the data, assuming we expect to receive a string, we must concatenate the into a string when listening to the stream data, amd when the stream end, we parse the string to JSON:const server = http.createServer(async (req, res) =&gt; { const buffers = []; for await (const chunk of req) { buffers.push(chunk); } const data = Buffer.concat(buffers).toString(); console.log(JSON.parse(data).todo); // 'Buy the milk' res.end();});Node.js file statsYou call it passing a file path, and once Node.js gets the file details it will call the callback function you pass, with 2 parameters: an error message, and the file stats:const fs = require('fs');fs.stat('/Users/joe/test.text', (err, stats) =&gt; { if (err) { console.error(err); return; } stats.isFile(); stats.isDirectory(); stats.isSymbolicLink(); stats.size;});You can use promise-based fsPromises.stat() method offerd by the fs/promises module if you like:const fs = require('fs/promises');async function example() { try { stats.isFile(); // true stats.isDirectory(); // false stats.isSymbolicLink(); // false stats.size; // 1024000 //= 1MB } catch (err) { console.log(err); }}example();Node.js File PathsYou include this module in your files usingconst path = require('path');const notes = '/users/joe/notes.txt';path.dirname(notes); // /users/joepath.basename(notes); // notes.txtpath.extname(notes); // .txtpath.basename(notes, path.extname(notes)); // notesWorking with pathsconst name = 'joe';path.join('/', 'users', name, 'notes.txt'); // '/users/joe/notes.txt'path.resolve('joe.txt'); // '/Users/joe/joe.txt' if run from my home folderpath.normalize('/users/joe/..//test.txt'); // '/users/test.txt'Neither resolve nor normalize will check if the path exists. They just calculate a path based on the information they got.Check file exists by:const stats = await fs.stat('/Users/joe/test.txt');stats.isFile(); // truestats.isDirectory(); // falsestats.isSymbolicLink(); // falseReading files with Node.jsAll three of fs.readFile(), fs.readFileSync() and fsPromises.readFile() read the full content of the file in memory before returning the data.This means that big files are going to have a major impact on your memory consumption and speed of execution of the program.In this case, a better option is to read the file content using streams. fs.readFile```JSconst fs = require(‘fs’);fs.readFile(‘/Users/joe/test.txt’, ‘utf8’, (err, data) =&gt; { if (err) { console.error(err); return; } console.log(data);});- fs.readFileSync()```JSconst fs = require('fs');try { const data = fs.readFileSync('/Users/joe/test.txt', 'utf8'); console.log(data);} catch (err) { console.error(err);} fsPromises.readFile() and fs/promises module```JSconst fs = require(‘fs/promises’);async function example() { try { const data = await fs.readFile(‘/Users/joe/test.txt’, { encoding: ‘utf8’ }); console.log(data); } catch (err) { console.log(err); }}example();## Writing files with Node.jsThe easiest way to write to files in Node.js is to use the `fs.writeFile()` API.- fs.writeFile```JSconst fs = require('fs');const content = 'Some content!';fs.writeFile('/Users/joe/test.txt', content, err =&gt; { if (err) { console.error(err); } // file written successfully}); fs.writeFileSync()```JSconst fs = require(‘fs’);const content = ‘Some content!’;try { fs.writeFileSync(‘/Users/joe/test.txt’, content); // file written successfully} catch (err) { console.error(err);}- fsPromises.writeFile() method offered by the `fs/promises` module:```JSconst fs = require('fs/promises');async function example() { try { const content = 'Some content!'; await fs.writeFile('/Users/joe/test.txt', content); } catch (err) { console.log(err); }}example();// for modify flagfs.writeFile('/Users/joe/test.txt', content, { flag: 'a+' }, err =&gt; {});The flags you’ll likely use are r+ open the file for reading and writing w+ open the file for reading and writing, positioning the stream at the beginning of the file. The file is created if it does not exist a open the file for writing, positioning the stream at the end of the file. The file is created if it does not exist a+ open the file for reading and writing, positioning the stream at the end of the file. The file is created if it does not exist Append to a file fs.appendFile()```JSconst content = ‘Some content!’;fs.appendFile(‘file.log’, content, err =&gt; { if (err) { console.error(err); } // done!});- fsPromises.appendFile()```JSconst fs = require('fs/promises');async function example() { try { const content = 'Some content!'; await fs.appendFile('/Users/joe/test.txt', content); } catch (err) { console.log(err); }}example();Using streamsAll those methods write the full content to the file before returning the control back to your program (in the async version, this means executing the callback)In this case, a better option is to write the file content using streams.Working with folders in Node.jsThe Node.js fs core module provides many handy methods you can use to work with folders.Check if a folder existsUse fs.access() (and its promise-based fsPromises.access() counterpart) to check if the folder exists and Node.js can access it with its permissions.const fs = require('fs');const folderName = '/Users/joe/test';try { if (!fs.existsSync(folderName)) { fs.mkdirSync(folderName); }} catch (err) { console.error(err);}Read the content of a directoryconst fs = require('fs');const folderPath = '/Users/joe';fs.readdirSync(folderPath);fs.readdirSync(folderPath).map(fileName =&gt; { return path.join(folderPath, fileName);});const isFile = fileName =&gt; { return fs.lstatSync(fileName).isFile();};fs.readdirSync(folderPath) .map(fileName =&gt; { return path.join(folderPath, fileName); }) .filter(isFile);Rename a folderfs.rename() or fs.renameSync() or fsPromises.rename(). The first parameter is the current path, the second the new path:const fs = require('fs');fs.rename('/Users/joe', '/Users/roger', err =&gt; { if (err) { console.error(err); } // done});fs.renameSync() is synchromous verionconst fs = require('fs');try { fs.renameSync('/Users/joe', '/Users/roger');} catch (err) { console.error(err);} fsPromises.rename() is the promise-based version:```JSconst fs = require(‘fs/promises’);async function example() { try { await fs.rename(‘/Users/joe’, ‘/Users/roger’); } catch (err) { console.log(err); }}example();### Remove a folderUse `fs.rmdir()` or `fs.rmdirSync()` or `fsPromises.rmdir()` to remove a folder.```JSconst fs = require('fs');fs.rm(dir, { recursive: true, force: true }, err =&gt; { if (err) { throw err; } console.log(`${dir} is deleted!`);}); You can install and make use of the fs-extra module. It’s a drop-in replacement of the fs module, which provides more features on top of it. npm install fs-extra ```jsconst fs = require(‘fs-extra’); const folder = ‘/Users/joe’;fs.remove(folder, err =&gt; { console.error(err);});// promisefs.remove(folder) .then(() =&gt; { // done }) .catch(err =&gt; { console.error(err); });// async/awaitasync function removeFolder(folder) { try { await fs.remove(folder); // done } catch (err) { console.error(err); }}const folder = ‘/Users/joe’;removeFolder(folder);## The Node.js fs moduleThis is core module. When you require module. You can access to all its method, wich include:- fs.access(): check if the file exists and Node.js can access it with its permissions- fs.appendFile(): append data to a file. If the file does not exist, it's created- fs.chmod(): change the permissions of a file specified by the filename passed. Related: fs.lchmod(), fs.fchmod()- fs.chown(): change the owner and group of a file specified by the filename passed. Related: fs.fchown(), fs.lchown()- fs.close(): close a file descriptor- fs.copyFile(): copies a file- fs.createReadStream(): create a readable file stream- fs.createWriteStream(): create a writable file stream- fs.link(): create a new hard link to a file- fs.mkdir(): create a new folder- fs.mkdtemp(): create a temporary directory- fs.open(): opens the file and returns a file descriptor to allow file manipulation- fs.readdir(): read the contents of a directory- fs.readFile(): read the content of a file. Related: fs.read()- fs.readlink(): read the value of a symbolic link- fs.realpath(): resolve relative file path pointers (., ..) to the full path- fs.rename(): rename a file or folder- fs.rmdir(): remove a folder- fs.stat(): returns the status of the file identified by the filename passed. Related: fs.fstat(), fs.lstat()- fs.symlink(): create a new symbolic link to a file- fs.truncate(): truncate to the specified length the file identified by the filename passed. Related: fs.ftruncate()- fs.unlink(): remove a file or a symbolic link- fs.unwatchFile(): stop watching for changes on a file- fs.utimes(): change the timestamp of the file identified by the filename passed. Related: fs.futimes()- fs.watchFile(): start watching for changes on a file. Related: fs.watch()- fs.writeFile(): write data to a file. Related: fs.write()One peculiar thing about the `fs` module is that all the methods are asynchronous by default, but they can also work synchronously by appending `Sync` such as `fs.renameSync`, `fs.writeSync`.```JS// Example: Read a file and change its content and read// it again using callback-based API.const fs = require('fs');const fileName = '/Users/joe/test.txt';fs.readFile(fileName, 'utf8', (err, data) =&gt; { if (err) { console.log(err); return; } console.log(data); const content = 'Some content!'; fs.writeFile(fileName, content, err2 =&gt; { if (err2) { console.log(err2); return; } console.log('Wrote some content!'); fs.readFile(fileName, 'utf8', (err3, data3) =&gt; { if (err3) { console.log(err3); return; } console.log(data3); }); });}); for antoher example```JS// Example: Read a file and change its content and read// it again using callback-based API.const fs = require(‘fs’);const fileName = ‘/Users/joe/test.txt’;fs.readFile(fileName, ‘utf8’, (err, data) =&gt; { if (err) { console.log(err); return; } console.log(data); const content = ‘Some content!’; fs.writeFile(fileName, content, err2 =&gt; { if (err2) { console.log(err2); return; } console.log(‘Wrote some content!’); fs.readFile(fileName, ‘utf8’, (err3, data3) =&gt; { if (err3) { console.log(err3); return; } console.log(data3); }); });});// Example: Read a file and change its content and read// it again using promise-based API.const fs = require(‘fs/promises’);async function example() { const fileName = ‘/Users/joe/test.txt’; try { const data = await fs.readFile(fileName, ‘utf8’); console.log(data); const content = ‘Some content!’; await fs.writeFile(fileName, content); console.log(‘Wrote some content!’); const newData = await fs.readFile(fileName, ‘utf8’); console.log(newData); } catch (err) { console.log(err); }}example();### The Node.js os moduleThis module provides many functions that you can use to retrieve information from the underlying operating system and the computer the program runs on, and interact with it.```jsconst os = require('os'); os.EOL gives the line delimiter sequence. It’s \\n on Linux and macOS, and \\r\\n on Windows. os.constants.signals tells us all the constants related to handling process signals, like SIGHUP, SIGKILL and so on. os.constants.errno sets the constants for error reporting, like EADDRINUSE, EOVERFLOW and more. os.arch() os.cpus() os.endianness() os.freemen() os.homedir() os.hostname() os.loadavg() os.networkInterfaces() os.platform() os.release() os.tmpdir() os.totalmem() os.type() os.uptime() os.userInfo() The Node.js events module The event module provides us the EventEmitter class, which is key to working with events in Node.js const EventEmitter = require('events');const door = new EventEmitter(); The event listener has these in-built events: newListener when a listener is added removeListener when a listener is removedemitter.addListener()Alias for emitter.on().emitter.emit()Emits an event. It synchronously calls every listener in the order they were registered. door.emit('slam'); // emitting the event \"slam\" emitter.eventNames() door.eventNames(); emitter.getMaxListeners()Get the maximum amount of listeners one can add to an EventEmitter object, which defaults to 10 but can be increased or lowered by using setMaxListeners() door.getMaxListeners(); emitter.listenerCount() door.listenerCount('open'); emitter.listeners() door.listeners('open'); emitter.off()Alias for emitter.removeListener() added in Node.js 10emitter.on()Adds a callback function that’s called when an event is emitted. door.on('open', () =&gt; {console.log('Door was opened');}); emitter.once()Adds a callback function that’s called when an event is emitted for the first time after registering this. This callback is only going to be called once, never again.```JSconst EventEmitter = require(‘events’); const ee = new EventEmitter();ee.once(‘my-event’, () =&gt; { // call callback function once});**emitter.prependListener()**When you add a listender using `on` or `addListener`, It's added last in the queue of listenders, and called last. Using `preendListener`it's added, added, and called, before other listeners.**emitter.prependOnceListener()**When you add a listener using `once`, it's added last in the queue of listeners, and called last. Using `preendOnceListener` it's added, and called, before other listeners.**emitter.removeAllListeners()**Removes all listeners of an EventEmitter object listening to a specific event:```jsdoor.removeAllListeners('open');emitter.removeListener()Remove a specific listener. You can do this by saving the callback function to a variable, when added, so you can reference it later:const doSomething = () =&gt; {};door.on('open', doSomething);door.removeListener('open', doSomething);emitter.setMaxListeners()Sets the maximum amount of listeners one can add to an EventEmitter object, which defaults to 10 but can be increased or lowered.door.setMaxListeners(50);The Node.js http moduleThe HTTP core module is a key module to Node.js networking.It can be included usingconst http = require('http');Propertieshttp.METHODS&gt; require('http').METHODS[ 'ACL', 'BIND', 'CHECKOUT', 'CONNECT', 'COPY', 'DELETE', 'GET', 'HEAD', 'LINK', 'LOCK', 'M-SEARCH', 'MERGE', 'MKACTIVITY', 'MKCALENDAR', 'MKCOL', 'MOVE', 'NOTIFY', 'OPTIONS', 'PATCH', 'POST', 'PROPFIND', 'PROPPATCH', 'PURGE', 'PUT', 'REBIND', 'REPORT', 'SEARCH', 'SUBSCRIBE', 'TRACE', 'UNBIND', 'UNLINK', 'UNLOCK', 'UNSUBSCRIBE' ]http.STATUS_CODESThis property lists all the HTTP status codes and their description:&gt; require('http').STATUS_CODES{ '100': 'Continue', '101': 'Switching Protocols', '102': 'Processing', '200': 'OK', '201': 'Created', '202': 'Accepted', '203': 'Non-Authoritative Information', '204': 'No Content', '205': 'Reset Content', '206': 'Partial Content', '207': 'Multi-Status', '208': 'Already Reported', '226': 'IM Used', '300': 'Multiple Choices', '301': 'Moved Permanently', '302': 'Found', '303': 'See Other', '304': 'Not Modified', '305': 'Use Proxy', '307': 'Temporary Redirect', '308': 'Permanent Redirect', '400': 'Bad Request', '401': 'Unauthorized', '402': 'Payment Required', '403': 'Forbidden', '404': 'Not Found', '405': 'Method Not Allowed', '406': 'Not Acceptable', '407': 'Proxy Authentication Required', '408': 'Request Timeout', '409': 'Conflict', '410': 'Gone', '411': 'Length Required', '412': 'Precondition Failed', '413': 'Payload Too Large', '414': 'URI Too Long', '415': 'Unsupported Media Type', '416': 'Range Not Satisfiable', '417': 'Expectation Failed', '418': \"I'm a teapot\", '421': 'Misdirected Request', '422': 'Unprocessable Entity', '423': 'Locked', '424': 'Failed Dependency', '425': 'Unordered Collection', '426': 'Upgrade Required', '428': 'Precondition Required', '429': 'Too Many Requests', '431': 'Request Header Fields Too Large', '451': 'Unavailable For Legal Reasons', '500': 'Internal Server Error', '501': 'Not Implemented', '502': 'Bad Gateway', '503': 'Service Unavailable', '504': 'Gateway Timeout', '505': 'HTTP Version Not Supported', '506': 'Variant Also Negotiates', '507': 'Insufficient Storage', '508': 'Loop Detected', '509': 'Bandwidth Limit Exceeded', '510': 'Not Extended', '511': 'Network Authentication Required' }http.globalAgentPoints to the global instance of the Agent object, which is an instance of the http.Agent class.It’s used to manage connections persistence and reuse for HTTP clients, and it’s a key component of Node.js HTTP networking.More in the http.Agent class description later on.Methodshttp.createServer()Return a new instance of the http.Server class.const server = http.createServer((req, res) =&gt; { // handle every single request with this callback});http.request()Makes an HTTP request to a server, creating an instance of the http.ClientRequest class.http.get()Similar to http.request(), but automatically sets the HTTP method to GET, and calls req.end() automatically.ClassesThe HTTP module provides 5 classes: http.Agent http.ClientRequest http.Server http.ServerResponse http.IncomingMessage http.Agent Node.js creates a global instance of the http.Agent class to manage connections persistence and reuse for HTTP clients, a key component of Node.js HTTP networking.This object make sure that every request made to a server is queued and a single socket is reused.It also maintains a pool of sockets. This is key for performance reasons. http.ClientRequest An http.ClientRequest object is created when http.request() or http.get() is called.When a response is received, the response event is called with response, with an http.IncomingMessage instance as argument. http.Server This class is commonly instantiated and returned when creating a new server using http.createServer().Once ypu have a server object, you have access to its methods: close() stops the serve from accepting new connections listen() starts the HTTP server and listens for connections http.ServerResponse const server = http.createServer((req, res) =&gt; {// res is an http.ServerResponse object}); These methods are used to interact with HTTP headers: getHeaderNames() get the list of the names of the HTTP headers already set getHeaders() get a copy of the HTTP headers already set setHeader(‘headername’, value) sets an HTTP header value getHeader(‘headername’) gets an HTTP header already set removeHeader(‘headername’) removes an HTTP header already set hasHeader(‘headername’) return true if the response has that header set headersSent() return true if the headers have already been sent to the clientAfter processing the headers you can send them to the client by calling response.writeHead(), which accepts the statusCode as the first parameter, the optional status message, and the headers object.To send data to the client in the response body, you use write(). It will send buffered data to the HTTP response stream.If the headers were not sent yet using response.writeHead(), it will send the headers first, with the status code and message that’s set in the request, which you can edit by setting the statusCode and statusMessage properties values: response.statusCode = 500;response.statusMessage = 'Internal Server Error'; http.IncomingMessage An http.IncomingMessage object is created by: http.Server when listening to the request eventhttp.ClientRequest when listening to the response eventIt can be used to access the response: status using its statusCode and statusMessage methods headers using its headers method or rawHeaders HTTP method using its method method HTTP version using the httpVersion method URL using the url method underlying socket using the socket methodThe data is accessed using streams, since http.IncomingMessage implements the Readable Stream interface. Node.js Buffers What is a buffer? A buffer is an area of memory. Most Javascript developers are much less familiar with this concept compared to programmers using a system programming language (like C, C++, or Go), which interact directly with memory every day.It represents a fixed-size chunk of memory (can’t be resized) allocated outside of the V8 JavaScript engine.You can think of a buffer like an array of integers, which each represent a byte of data.It is implemented by the Node.js Buffer class. Why do we need a buffer? A buffer is an area of memory. Most javascript developers are much less familiar with this concept, compared to programmers using a system programming, which interact directly with memory every day.It represents a fixed-size chunk of memory allocated outside of the V8 javascript engine.It is implemented by the Node.js Buffer class Why do we need a buffer? Buffers were introduced to help developers deal with binary data, in an ecosystem that traditionally only dealt with strings rather than binaries.Buffers in Node.js are not related to the concept of buffering data. That is what happens when a stream processor receives data faster than it can digest. How to create a buffer A buffer is created using the Buffer.from(), Buffer.alloc(), and Buffer.allocUnsafe() methods. const buf = Buffer.from('Hey!');const buf = Buffer.alloc(1024);const buf = Buffer.allocUnsafe(1024); Buffer.from(array) Buffer.from(arrayBuffer[, byteOffset[, length]]) Buffer.from(buffer) Buffer.from(string[, encoding]) Using a buffer Access the content of a buffer const buf = Buffer.from('Hey!');console.log(buf[0]); // 72console.log(buf[1]); // 101console.log(buf[2]); // 121const buf = Buffer.alloc(4);buf.write('Hey!');const buf = Buffer.from('Hey!');buf[1] = 111; // o in UTF-8console.log(buf.toString()); // Hoy!// Slice a bufferconst buf = Buffer.from('Hey!');buf.subarray(0).toString(); // Hey!const slice = buf.subarray(0, 2);console.log(slice.toString()); // Hebuf[1] = 111; // oconsole.log(slice.toString()); // Ho// Copy a bufferconst buf = Buffer.from('Hey!');const bufcopy = Buffer.alloc(4); // allocate 4 bytesbufcopy.set(buf);const buf = Buffer.from('Hey?');const bufcopy = Buffer.from('Moo!');bufcopy.set(buf.subarray(1, 3), 1);console.log(bufcopy.toString()); // 'Mey!' Node.js Streams What are streams Streams are one of the fundamental concepts that power Node.js applications.They are a way to handle reading/writing files, network communications, or any kind of end-to-end information exchange in an efficient way.Streams are not a concept unique to Node.js. They were introduced in the Unix operating system decades ago, and programs can interact with each other passing streams through the pipe operator (|).For example, in the traditional way, when you tell the program to read a file, the file is read into memory, from start to finish, and then you process it.Using streams you read it piece by piece, processing its content without keeping it all in memory.The Node.js stream module provides the foundation upon which all streamng apis are built. All streams are instances of EventEmitter. Why streams Streams basically provide two major advantages over using other data handling methods: Memory efficiency: you don’t need to load large amounts of data in memory before you are able to process it Time efficiency: It takes way less time to start processing data, since you can start processing as soon as you have it, rather than waiting till the whole data payload is avaiable An example of a stream A typical examples is reading files from a disk.Using the Node.js fs module, you can read a file, and serve it over HTTP when a new connection is established to your HTTP server:```JSconst http = require(‘http’);const fs = require(‘fs’); const server = http.createServer(function (req, res) { fs.readFile(${__dirname}/data.txt, (err, data) =&gt; { res.end(data); });});server.listen(3000);- `readFile()` reads the full contents of the file, and invokes the callback function when it's done.- `res.end(data)` in the callback will return the file contents to the HTTP client.If the file is big, the operation will take quite bit of time. Here is the same thing written using streams:```JSconst http = require('http');const fs = require('fs');const server = http.createServer((req, res) =&gt; { const stream = fs.createReadStream(`${__dirname}/data.txt`); stream.pipe(res);});server.listen(3000);Instead of waiting until the file is fully read, we start streaming it to the HTTP client as soon as we have a chunk of data ready to be sent.pipe()The above example uses the line stream.pipe(res): the pipe() method is called on the file stream.What does this code do? It takes the source, and pipes it into a destication.You call it on the source stream, so in this case, the file sream is piped to the HTTP response.The return value of the pipe() method is the destination stream, which is a very convenient thing that let us chain multiple pipe() calls, like this:src.pipe(dest1).pipe(dest2);// ORsrc.pipe(dest1);dest1.pipe(dest2);Streams-powered Node.js APIsDue to their advantages, many Node.js core modules provide native stream handling capabilities, most notably: process.stdin returns a stream connected to stdin process.stdout returns a stream connected to stdoutprocess.stderr returns a stream connected to stderr fs.createReadStream() creates a readable stream to a file fs.createWriteStream() creates a writable stream to a file net.connect() initiates a stream-based connection http.request() returns an instance of the http.ClientRequest class, which is a writable stream zlib.createGzip() compress data using gzip (a compression algorithm) into a stream zlib.createGunzip() decompress a gzip stream. zlib.createDeflate() compress data using deflate (a compression algorithm) into a stream zlib.createInflate() decompress a deflate stream Different types of streams There are four classes of streams: Readable: a stream which could be used for read data from it. In other words, its readonly. Writable: a stream which could be used for write data to it. It is writeonly. Duplex: a stream which can read and write data, basically its a combination of a Readable and Writable stream. Transform: a Duplex stream which reads data, transforms the data, and then writes the transformed data in the desired format. How to create a readable stream const Stream = require('stream');readableStream._read = () =&gt; {};const readableStream = new Stream.Readable({read() {},});readableStream.push('hi!');readableStream.push('ho!'); How to get data from a readable stream ```JSconst Stream = require(‘stream’); const readableStream = new Stream.Readable({ read() {},});const writableStream = new Stream.Writable();writableStream._write = (chunk, encoding, next) =&gt; { console.log(chunk.toString()); next();};readableStream.pipe(writableStream);readableStream.push(‘hi!’);readableStream.push(‘ho!’);You can also consume a readable stream directly, using the `readable` event```JSreadableStream.on('readable', () =&gt; { console.log(readableStream.read());});How to create a writable streamconst Stream = require('stream');const writableStream = new Stream.Writable();writableStream._write = (chunk, encoding, next) =&gt; { console.log(chunk.toString()); next();};process.stdin.pipe(writableStream);How to send data to a writable streamwritableStream.write('hey!\\n');Signaling a writable stream that you ended writingUse the end() method:const Stream = require('stream');const readableStream = new Stream.Readable({ read() {},});const writableStream = new Stream.Writable();writableStream._write = (chunk, encoding, next) =&gt; { console.log(chunk.toString()); next();};readableStream.pipe(writableStream);readableStream.push('hi!');readableStream.push('ho!');readableStream.on('close', () =&gt; writableStream.end());writableStream.on('close', () =&gt; console.log('ended'));readableStream.destroy();How to create a transform streamWe get the Transform stream from the stream module, and we initialize it and implement the transform._transform() method.First create a transform stream object:const { Transform } = require('stream');const transformStream = new Transform();then implementt _transform:transformStream._transform = (chunk, encoding, callback) =&gt; { transformStream.push(chunk.toString().toUpperCase()); callback();};Pipe readable stream:process.stdin.pipe(transformStream).pipe(process.stdout);Node.js, the difference between development and productionNode.js assumes it’s always running in a development environment. You can signal Node.js that you are running in production by settung the NODE_ENV=production environment variable.export NODE_ENV=production# orNODE_ENV=production node app.jsIn Node.js conditional to handle multiple environmentsif (process.env.NODE_ENV === 'development') { // ...}if (process.env.NODE_ENV === 'production') { // ...}if (['production', 'staging'].includes(process.env.NODE_ENV)) { // ...}// ORif (process.env.NODE_ENV === 'development') { app.use(express.errorHandler({ dumpExceptions: true, showStack: true }));}if (process.env.NODE_ENV === 'production') { app.use(express.errorHandler());}Error handling in Node.jsCreating exceptionsuse throw keyword.Error objectsthrow new Error('Ran out of coffee');orclass NotEnoughCoffeeError extends Error { // ...}throw new NotEnoughCoffeeError();Handling exceptionsAn exception handler is a try/catch statement.try { // lines of code} catch (e) {}Catching uncaught exceptionsTo solve this, you listen for the uncaughtException event on the process object:process.on('uncaughtException', err =&gt; { console.error('There was an uncaught error', err); process.exit(1); // mandatory (as per the Node.js docs)});Exceptions with promisesUsing promises you can chain different operations, and handle errors at the end:doSomething1() .then(doSomething2) .then(doSomething3) .catch(err =&gt; console.error(err));How do you know where the error occurred? You dont really know, but you can handle errors in each of function you call(doSomethingX), and inside the error handler throw a new error, that’s going to call outside catch handler:const doSomething1 = () =&gt; { // ... try { // ... } catch (err) { // ... handle it locally throw new Error(err.message); } // ...};To be able to handle errors locally without handling them in the function we call, we can break the chain. You can create a function in each then() and process the exception:doSomething1() .then(() =&gt; { return doSomething2().catch(err =&gt; { // handle error throw err; // break the chain! }); }) .then(() =&gt; { return doSomething3().catch(err =&gt; { // handle error throw err; // break the chain! }); }) .catch(err =&gt; console.error(err));Error handling with async/awaitUsing async/await, you still need to catch errors, and you do it this way:async function someFunction() { try { await someOtherFunction(); } catch (err) { console.error(err.message); }}How to log an object in Node.jsWhen you type console.log() into a JavaScript program that runs in the browser, that is going to create a nice entry in Browser Console:console.log(obj)We don’t have suck luxury when we log something to the console, because that’s going to output the object to the shell if you run the Node.js program manually, or to the log file. You get a string representation of the object.const obj = { name: 'joe', age: 35, person1: { name: 'Tony', age: 50, person2: { name: 'Albert', age: 21, person3: { name: 'Peter', age: 23, }, }, },};console.log(obj);// pretty way to printconsole.log(JSON.stringify(obj, null, 2));require('util').inspect.defaultOptions.depth = null;console.log(obj);If you don’t want to touch any kinds of defaultOptions, a perfect alternative is console.dir.// `depth` tells util.inspect() how many times to recurse while formatting the object, default is 2console.dir(obj, { depth: 10,});// ...or pass `null` to recurse indefinitelyconsole.dir(obj, { depth: null,});// %o tells console.log() to string-format and log obj in its placeconsole.log('%o', obj);Node.js with TypeScriptWhat is TypeScriptTypeScript is a very popular open-source language maintained and developed by Microsoft, it’s loved and used by a lot of software developers around the world.Basically, it’s a superset of javascript that adds new capabilities to the language. Most notable addition are static tupe definitions, something that is not present in plain Javascript. Thanks to types, it’s possible, for example, to declare what king of arguments we are expecting and what is returned really powerful tool and opens new world of possibilities in javascript projects. It makes our code more secure and robust by preventing a lot of bugs before code is even shipped - it catches problems during writing the code and integrates wonderfully with code editors like Visual Studio Code.We can talk about other TypeScript benefits later, let’s see some examples now!ExamplesTake a look at this code snippet and then we can unpack it together:type User = { name: string; age: number;};function isAdult(user: User): boolean { return user.age &gt;= 18;}const justine: User = { name: 'Justine', age: 23,};const isJustineAnAdult: boolean = isAdult(justine);First part with type keyword is responsible for declaring our custom type of objects representing users. Later we utilize this newly created type to create function isAdult that accepts one argument of type User and returns boolean. After this we create justine, our example data that can be used for calling previously defined function. Finally, we create new variable with information whether justine is an adult or not.There are additional things about this example that you should know. Firstly, if we would not comply with declared types, TypeScript would alarm us that something is wrong and prevent misuse. Secondly, not everything must be typed explicitly - TypeScript is very smart and can deduce types for us. For example, variable isJustineAnAdult would be of type boolean even if we didn’t type it explicitly or justine would be valid argument for our function even if we didn’t declare this variable as of User type.Okay, so we have some TypeScript code. Now how do we run it?First thing to do is to install TypeScript in our project:npm i -D typescriptnpx tsc example.tsThis command will result in a new file named example.js that we can run using Node.js. Now when we know how to compile and run TypeScript code let’s see TypeScript bug-preventing capabilities in action!This is how we will modify our code:type User = { name: string; age: number;};function isAdult(user: User): boolean { return user.age &gt;= 18;}const justine: User = { name: 'Justine', age: 'Secret!',};const isJustineAnAdult: string = isAdult(justine, \"I shouldn't be here!\");And this is what TypeScript has to say about this:example.ts:12:3 - error TS2322: Type 'string' is not assignable to type 'number'.12 age: \"Secret!\", ~~~ example.ts:3:3 3 age: number; ~~~ The expected type comes from property 'age' which is declared here on type 'User'example.ts:15:7 - error TS2322: Type 'boolean' is not assignable to type 'string'.15 const isJustineAnAdult: string = isAdult(justine, \"I shouldn't be here!\"); ~~~~~~~~~~~~~~~~example.ts:15:51 - error TS2554: Expected 1 arguments, but got 2.15 const isJustineAnAdult: string = isAdult(justine, \"I shouldn't be here!\"); ~~~~~~~~~~~~~~~~~~~~~~Found 3 errors.More about TypeScriptTypescript offers a whole lot of greate mechanisms like interfaces, classes, ulity types and so on. Also, on bigger projects you can declare your TypeScript compiler configuration in a separate file and granulary adjust how it works, how strict it is and where it stores compiled files for example. You can read more about all this awesome stuff in the offcial TypeScript.docsSome of the other benefits of TypeScript that are worth mentioning are that it can be adopted progressively, it helps making code more readable and understanable and it allows developers to use modern language features while shipping code for older Node.js versions.TypeScript in the Node.js worldTypeScript is well-establised in the Node.js world and used by many companies, open-sources projects tools and frameworks, Some of the notable examples of open-source projects using TypeScript are: NestJS - robust and fully-featured framework that makes creating scalable and well-architected systems easy and pleasant TypeORM - great ORM influenced by other well-known tools from other languages like Hibernate, Doctrine or Entity Framework Prisma - next-generation ORM featuring a declarative data model, generated migrations and fully type-safe database queries RxJS - widely used library for reactive programming AdonisJS - A fully featured web framework with Node.js FoalTs - The Elegant Nodejs FrameworkAnd many, many more great projects… Maybe even your next one! Asynchronous flow control At its core, JavaScript is designed to be non-blocking on the “main” thread, this is where views are rendered. You can imagine the importance of this in the browser. When the main thread becomes blocked it results in the infamous “freezing” that end users dread, and no other events can be dispatched resulting in the loss of data acquisition, for example.This creates some unique constraints that only a functional style of programming can cure. This is where callbacks come in to the picture. However, callbacks can become challenging to handle in more complicated procedures. This often results in “callback hell” where multiple nested functions with callbacks make the code more challenging to read, debug, organize, etc.async1(function (input, result1) { async2(function (result2) { async3(function (result3) { async4(function (result4) { async5(function (output) { // do something with output }); }); }); });});Of course, in real life there would most likely be additional lines of code to handle result1, result2, etc., thus, the length and complexity of this issue usually results in code that looks much more messy than the example above.This is where functions come in to great use. More complex operations are made up of many functions: initiator style / input middleware terminatorThe “initiator style / input” is the first function in the sequence. This function will accept the original input, if any, for the operation. The operation is an executable series of functions, and the original input will primarily be: variables in a global environment direct invocation with or without arguments values obtained by file system or network requestsA middleware function will return another function, and a terminator function will invoke the callback. The following illustrates the flow to network or file system requests. Here the latency is 0 because all these values are available in memory.```JSfunction final(someInput, callback) { callback(${someInput} and terminated by executing callback );}function middleware(someInput, callback) { return final(${someInput} touched by middleware , callback);}function initiate() { const someInput = ‘hello this is a function ‘; middleware(someInput, function (result) { console.log(result); // requires callback to return result });}initiate();### State managementFunctions may or may not be state dependent. State dependency aries when the input or other variable of a function relies on an outside function.In this way there are two primary strategies for state management:1. passing in variables directly to a function, and2. acquiring a variable value from a cache, session, file, database, network, or other outside source.Note, I did not mention global variable. Managing state with global variables is often a sloppy anti-pattern that make it difficult or impossible to guarantee state. Global variables in complex programs should be avoided when possible.### Control flowIf an object is available in memory, iteration is possible, and there will not be a change to control flow:```JSfunction getSong() { let _song = ''; let i = 100; for (i; i &gt; 0; i -= 1) { _song += `${i} beers on the wall, you take one down and pass it around, ${ i - 1 } bottles of beer on the wall\\n`; if (i === 1) { _song += \"Hey let's get some more beer\"; } } return _song;}function singSong(_song) { if (!_song) throw new Error(\"song is '' empty, FEED ME A SONG!\"); console.log(_song);}const song = getSong();// this will worksingSong(song);However, if the data exists outside of memory the iteration will no longer work:function getSong() { let _song = ''; let i = 100; for (i; i &gt; 0; i -= 1) { /* eslint-disable no-loop-func */ setTimeout(function () { _song += `${i} beers on the wall, you take one down and pass it around, ${ i - 1 } bottles of beer on the wall\\n`; if (i === 1) { _song += \"Hey let's get some more beer\"; } }, 0); /* eslint-enable no-loop-func */ } return _song;}function singSong(_song) { if (!_song) throw new Error(\"song is '' empty, FEED ME A SONG!\"); console.log(_song);}const song = getSong('beer');// this will not worksingSong(song);// Uncaught Error: song is '' empty, FEED ME A SONG!You will be able to perform almost all of your operations with the following 3 patterns: In series: functions will be executed in a strict sequential order, this one is most similar to for loops:```JS// operations defined elsewhere and ready to executeconst operations = [ { func: function1, args: args1 }, { func: function2, args: args2 }, { func: function3, args: args3 },];function executeFunctionWithArgs(operation, callback) { // executes function const { args, func } = operation; func(args, callback);}function serialProcedure(operation) { if (!operation) process.exit(0); // finished executeFunctionWithArgs(operation, function (result) { // continue AFTER callback serialProcedure(operations.shift()); });}serialProcedure(operations.shift());2. Full parallel: when ordering is not an issue, such as emailing a list of 1,000,000 email recipients.```JSlet count = 0;let success = 0;const failed = [];const recipients = [ { name: 'Bart', email: 'bart@tld' }, { name: 'Marge', email: 'marge@tld' }, { name: 'Homer', email: 'homer@tld' }, { name: 'Lisa', email: 'lisa@tld' }, { name: 'Maggie', email: 'maggie@tld' },];function dispatch(recipient, callback) { // `sendEmail` is a hypothetical SMTP client sendMail( { subject: 'Dinner tonight', message: 'We have lots of cabbage on the plate. You coming?', smtp: recipient.email, }, callback );}function final(result) { console.log(`Result: ${result.count} attempts \\ &amp; ${result.success} succeeded emails`); if (result.failed.length) console.log(`Failed to send to: \\ \\n${result.failed.join('\\n')}\\n`);}recipients.forEach(function (recipient) { dispatch(recipient, function (err) { if (!err) { success += 1; } else { failed.push(recipient.name); } count += 1; if (count === recipients.length) { final({ count, success, failed, }); } });}); Limited parallel: parallel with limit, such as successfully emailing 1,000,000 recipients from a list of 10E7 users.```JSlet successCount = 0;function final() { console.log(dispatched ${successCount} emails); console.log(‘finished’);}function dispatch(recipient, callback) { // sendEmail is a hypothetical SMTP client sendMail( { subject: ‘Dinner tonight’, message: ‘We have lots of cabbage on the plate. You coming?’, smtp: recipient.email, }, callback );}function sendOneMillionEmailsOnly() { getListOfTenMillionGreatEmails(function (err, bigList) { if (err) throw err;function serial(recipient) { if (!recipient || successCount &gt;= 1000000) return final(); dispatch(recipient, function (_err) { if (!_err) successCount += 1; serial(bigList.shift()); });}serial(bigList.shift()); }); }sendOneMillionEmailsOnly();Each has its own use cases, benefits, and issues you can experiement and read about in more detail. Most importanly, remember to modularize your operations and use callbacks! If you feel any doubt, treat everything as if it were middleware!## Node.js with WebAssemblyWebAssembly is a high-performance assembly-like language that can be compiled from a myriad of languages including C/C++, Rust, and AssemblyScript. As of right now, it is supported by Chrome, Firefox, Safari, Edge, and Node.js!The WebAssembly specification details two file formats, a binary format called a WebAssembly Module with a .wasm extension and corresponding text representation called WebAssembly Text format with a .wat extension.### Key Concepts- Module - A compiled WebAssembly binary, ie a .wasm file.- Memory - A resizable ArrayBuffer.- Table - A resizable typed array of references not stored in Memory.- Instance - An instantiation of a Module with its Memory, Table, and variables.```JSconsole.log(WebAssembly);/*Object [WebAssembly] { compile: [Function: compile], validate: [Function: validate], instantiate: [Function: instantiate]}*/Generating WebAssembly ModulesThere are multiple methods available to generate WebAssembly binary files including: Writing WebAssembly (.wat) by hand and converting to binary format using tools such as wabt Using emscripten with a C/C++ application Using wasm-pack with a Rust application Using AssemblyScript if you prefer a TypeScript-like experience Some of these tools generate not only the binary file, but the JavaScript “glue” code and corresponding HTML files to run in the browser. How to use it Once you have a WebAssembly module, you can use the Node.js WebAssembly object to instantiate it.```JS// Assume add.wasm file exists that contains a single function adding 2 provided argumentsconst fs = require(‘fs’); const wasmBuffer = fs.readFileSync(‘/path/to/add.wasm’);WebAssembly.instantiate(wasmBuffer).then(wasmModule =&gt; { // Exported function live under instance.exports const { add } = wasmModule.instance.exports; const sum = add(5, 6); console.log(sum); // Outputs: 11});```Interacting with the OSWebAssembly modules cannot directly access OS functionality on its own. A third-party tool Wasmtime can be used to access this functionality. Wasmtime utilizes the WASI API to access the OS functionality.Resources General WebAssembly Information MDN Docs Write WebAssembly by hand" }, { "title": "gRPC – An RPC library and framework", "url": "/posts/grpc-an-rpc-library-and-framework/", "categories": "", "tags": "", "date": "2022-05-10 00:00:00 +0700", "snippet": "", "content": "" }, { "title": "How to create ecommerce website use Laravel 9, VueJs, Docker, Supervisord, Brooadcast and Redis Queue (part 1)", "url": "/posts/how-to-create-ecommerce-website-use-laravel-9-vuejs-docker-supervisord-brooadcast-and-redis-queue-part-1/", "categories": "", "tags": "", "date": "2022-05-09 00:00:00 +0700", "snippet": "IntrodutionCreate an E-Commerce website with laravel 9 and VueJs : Dockerize setup all in one API integration. Redis for queue job in concurrency request. Supervisor manage queue job(Laravel ho...", "content": "IntrodutionCreate an E-Commerce website with laravel 9 and VueJs : Dockerize setup all in one API integration. Redis for queue job in concurrency request. Supervisor manage queue job(Laravel horizon). Laravel Broadcast event to observe status of serve and update to client. Deploy website to AWS EC2.Checkout source code on my gitPart 1 InstallDockerize setup all in oneDocker InstallLaravel with VueJs, Redis, Supervisod, and Horizon installRedis InstallPusher InstallComplete Dockerfile and .yml fileFROM php:8.0-fpm # Copy composer.lock and composer.json into the working directoryCOPY composer.lock composer.json /var/www/html/ # Set working directoryWORKDIR /var/www/html/ # Install dependencies for the operating system softwareRUN apt-get update &amp;&amp; apt-get install -y \\ build-essential \\ libpng-dev \\ libjpeg62-turbo-dev \\ libfreetype6-dev \\ locales \\ zip \\ jpegoptim optipng pngquant gifsicle \\ vim \\ libzip-dev \\ unzip \\ git \\ libonig-dev \\ curl \\ build-essential \\ openssl \\ libssl-dev curl \\ supervisor \\ software-properties-common RUN groupadd dev# node installENV NODE_VERSION=15.14.0RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bashENV NVM_DIR=/usr/local/nvm/.nvmRUN mkdir /usr/local/nvmRUN mkdir /usr/local/nodeRUN chown -R root:dev /usr/local/nvmRUN chmod -R 775 /usr/local/nvmRUN chown -R root:dev /usr/local/nodeRUN chmod -R 775 /usr/local/nodeRUN cp -R /root/.nvm/ /usr/local/nvm/ENV NVM_DIR=/usr/local/nvm/.nvmRUN export NVM_DIR=/usr/local/nvm/.nvmRUN . \"/usr/local/nvm/.nvm/nvm.sh\" &amp;&amp; nvm install ${NODE_VERSION}RUN . \"/usr/local/nvm/.nvm/nvm.sh\" &amp;&amp; nvm use v${NODE_VERSION}RUN . \"/usr/local/nvm/.nvm/nvm.sh\" &amp;&amp; nvm alias default v${NODE_VERSION}ENV PATH=\"/usr/local/nvm/.nvm/versions/node/v${NODE_VERSION}/bin/:${PATH}\"RUN node --versionRUN npm --version# Clear cacheRUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* # Install extensions for phpRUN docker-php-ext-install pdo_mysql mbstring zip exif pcntl RUN docker-php-ext-configure gd --with-freetype --with-jpegRUN docker-php-ext-install gdRUN pecl install redis \\ &amp;&amp; docker-php-ext-enable redis # Install composer (php package manager)RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer # Copy existing application directory contents to the working directoryCOPY . /var/www/htmlENV ENABLE_CRONTAB 1ENV ENABLE_HORIZON 1ENTRYPOINT [\"sh\", \"/var/www/html/docker-entrypoint.sh\"]COPY ./supervisor.d/*.* /etc/supervisor/conf.d/RUN composer installRUN composer dump-autoloadRUN chmod 777 install.shRUN [\"/bin/bash\", \"-c\", \"./install.sh\"]# Assign permissions of the working directory to the www-data userRUN chown -R www-data:www-data \\ /var/www/html/storage \\ /var/www/html/bootstrap/cacheRUN chmod -R 777 /var/www/html/storageEXPOSE 9000CMD supervisord -n -c /etc/supervisor/supervisord.conf# Expose port 9000 and start php-fpm server (for FastCGI Process Manager)# CMD [\"php-fpm\"]Service distributed by docker-compose.ymlversion: '2.0'services: #PHP Service app: build: context: . dockerfile: Dockerfile image: cloudsigma.com/php container_name: app restart: unless-stopped tty: true environment: SERVICE_NAME: app SERVICE_TAGS: dev working_dir: /var/www/html/ volumes: - ./:/var/www/html/ - ./php/laravel.ini:/usr/local/etc/php/conf.d/laravel.ini networks: - app-network #Nginx Service webserver: image: nginx:alpine container_name: webserver restart: unless-stopped tty: true ports: - \"80:80\" - \"443:443\" volumes: - ./:/var/www/html/ - ./nginx/conf.d/:/etc/nginx/conf.d/ networks: - app-network #MySQL Service db: image: mysql:5.7.32 container_name: db restart: unless-stopped tty: true ports: - \"3306:3306\" environment: MYSQL_DATABASE: lecommerce MYSQL_ROOT_PASSWORD: 123456 SERVICE_TAGS: dev SERVICE_NAME: mysql volumes: - dbdata:/var/lib/mysql/ - ./mysql/my.cnf:/etc/mysql/my.cnf networks: - app-network redis: image: redis:alpine container_name: redis ports: - \"6379:6379\" volumes: - cache_data:/data networks: - app-network#Docker Networksnetworks: app-network: driver: bridge#Volumesvolumes: dbdata: driver: local cache_data: driver: localdocker-entrypoint.shIntroduction of Docker ENTRYPOINT. Docker entrypoint is a Dockerfile directive or instruction that is used to specify the executable which should run when a container is started from a Docker image. It has two forms, the first one is the ‘exec’ form and the second one is the ‘shell’ form.I was used a PHP-FPM based image, this file was useful when use Ubuntu image or deploy to Linux based system:#!/bin/shset -e# Enable Laravel scheduleif [[ \"${ENABLE_CRONTAB:-0}\" = \"1\" ]]; then echo \"* * * * * php /var/www/html/artisan schedule:run &gt;&gt; /dev/null 2&gt;&amp;1\" &gt;&gt; /etc/crontabs/www-datafiexec \"$@\"Part 2" }, { "title": "SOLID principles, and SOLID in frameworks (part 1)", "url": "/posts/solid-principles-and-solid-in-frameworks/", "categories": "Blogging, Tutorial", "tags": "solid, rails, laravel, PHP, ruby, python", "date": "2022-04-16 00:00:00 +0700", "snippet": "Solid principles contains five best praticles when construct source code by OOP methodology.Single responsible principlesThe single-responsibility principle (SRP) is a computer-programming principl...", "content": "Solid principles contains five best praticles when construct source code by OOP methodology.Single responsible principlesThe single-responsibility principle (SRP) is a computer-programming principle that states that every module, class or function in a computer program should have responsibility over a single part of that program’s functionality, and it should encapsulate that part. All of that module, class or function’s services should be narrowly aligned with that responsibility.E.g: Defime module User manage user, define User class represent User in system, define Role class manage Role in System, define Permission class represent Permission in system, Policy class represent policy in system. Every software component should have one and only one reponsiblity reason to change.cohesion Cohesion is the degree to which the various part of a software components relatedHigher cohesion helps attain better adherence to the SRP.coupling Coupling is defined as level of the inter dependency betweens various of software components.Loose coupling helps attain better adherence to the SRP.class Rectangle { public $width; public $height; public function __construct($width, $height) { $this-&gt;width = $width; $this-&gt;height = $height; }}class Square { public $length; public function __construct($length) { $this-&gt;length = $length; }}class AreaCalculator { protected $shapes; public function __construct($shapes = array()) { $this-&gt;shapes = $shapes; } public function sum() { $area = []; foreach($this-&gt;shapes as $shape) { if($shape instanceof Square) { $area[] = pow($shape-&gt;length, 2); } else if($shape instanceof Rectangle) { $area[] = $shape-&gt;width * $shape-&gt;height; } } return array_sum($area); }}Open closed princliple Software components should be closed for modification, but open for extension Closed for modification: New features getting added to sofrware component, should not have modify code Open for extensiion: A software should be extenable to add a new feature or to add a new behaviour to it.Eg: Define two class of Shape with attributes is closed for moification and the method is open for extendsion, example like we want calculator for area of rectangle.interface Shape{ /** * Calculate the area of the shape. * * @return mixed */ public function getArea();}class Rectangle implements Shape{ private $width; private $height; public function getArea() { return $this-&gt;width * $this-&gt;height; }}class Circle implements Shape{ private $radius; private function getArea() { return pow($this-&gt;radius, 2) * PI; }}/** * Calculate the total area of the shapes. * * @param Illuminate\\Support\\Collection $shapes * @return float */ public function Area($shapes) { $area = 0; $shapes-&gt;each(function ($shape) use (&amp;$area) { $area += $shape-&gt;getArea(); }); return $area; }}Good: Ease of adding new features Lead to minimal cost of developing and testing software requires decoupling, which in turn automatically foallows the SRPCaution: Do not bindly, you will end up with a huge number of class that can complicated your overall design. Make a subjective, rather than an objective decision.The Open/Closed Principle is designed to make you write your code in such a manner that the core functionality is as unambiguous and concise as possibleLiskov subtitution priciple Obejct should be replaceable with their subtypes without affecting the correctness of the programabstract class Bird {\t\tabstract public function fly();}class Pengiue extends Bird {\tpublic function fly() \t{\t\t\treturn false;\t}}class Eagle extends Bird {\tpublic function fly()\t{\t\t\treturn true\t}} break the hierachy Tell, don’t askInterface Segregation principle No client should be forced to depend on methods it does not useTechniques to identify violations of ISP Fat interfaces. Interface with low cohesion. Empty method implentations.&lt;?php// Interface Segregation Principle Violationinterface Workable{ public function canCode(); public function code(); public function test();}class Programmer implements Workable{ public function canCode() { return true; } public function code() { return 'coding'; } public function test() { return 'testing in localhost'; }}class Tester implements Workable{ public function canCode() { return false; } public function code() { throw new Exception('Opps! I can not code'); } public function test() { return 'testing in test server'; }}class ProjectManagement{ public function processCode(Workable $member) { if ($member-&gt;canCode()) { $member-&gt;code(); } }}// Refactoredinterface Codeable{ public function code();}interface Testable{ public function test();}class Programmer implements Codeable, Testable{ public function code() { return 'coding'; } public function test() { return 'testing in localhost'; }}class Tester implements Testable{ public function test() { return 'testing in test server'; }}class ProjectManagement{ public function processCode(Codeable $member) { $member-&gt;code(); }}Dependentcy Injection principle High level module should not depend on low level module, Both should depend on abstrctions.Abstraction should not depend on details. Details should depend on abstraction Without dependency injectionclass GoogleMaps{ public function getCoordinatesFromAddress($address) { // calls Google Maps webservice }}class OpenStreetMap{ public function getCoordinatesFromAddress($address) { // calls OpenStreetMap webservice }}class StoreService{ public function getStoreCoordinates($store) { $geolocationService = new GoogleMaps(); // or $geolocationService = GoogleMaps::getInstance() if you use singletons return $geolocationService-&gt;getCoordinatesFromAddress($store-&gt;getAddress()); }}With dependency injectionclass StoreService { private $geolocationService; public function __construct(GeolocationService $geolocationService) { $this-&gt;geolocationService = $geolocationService; } public function getStoreCoordinates($store) { return $this-&gt;geolocationService-&gt;getCoordinatesFromAddress($store-&gt;getAddress()); }}interface GeolocationService { public function getCoordinatesFromAddress($address);}class GoogleMaps implements GeolocationService { ...class OpenStreetMap implements GeolocationService { ...Inversion of control Inversion of control (IOC) defines the way objects are used, but it does not specify how to create them. IOC defines the relationship between the high-level class and detail class, where the high-level class depends on detail class. High- and low-level classes are related by abstracting a detail class in IOC.&lt;?php//Define ClassAclass ClassA{ public $ClassB; public $ClassC; public function ClassA() { echo \"&lt;h2 style='color:red'&gt; Initialization of ClassA &lt;/h2&gt;\"; } public function method() { $this-&gt;ClassB = new ClassB(); $this-&gt;ClassC = new ClassC(); $this-&gt;ClassB-&gt;method(); $this-&gt;ClassC-&gt;method(); }}//Define ClassBclass ClassB{ public function ClassB() { echo \"&lt;h2 style='color:blue'&gt; Initialization of ClassB &lt;/h2&gt;\"; } public function method() { echo \"&lt;h3 style='color:blue'&gt; The output from ClassB &lt;/h3&gt;\"; }}//Define ClassCclass ClassC{ public function ClassC() { echo \"&lt;h2 style='color:green'&gt; Initialization of ClassC &lt;/h2&gt;\"; } public function method() { echo \"&lt;h3 style='color:green'&gt; The output from ClassC &lt;/h3&gt;\"; }}//Create object of ClassA$object = new ClassA();//Call the method() function$object-&gt;method();?&gt;Totally SOLID principles complement each other, and work together in unison, to achieve the common purpose of well-designed software" }, { "title": "How to build a Shortened URL By Ruby On Rails like Bitly or Twitter", "url": "/posts/shortened-url-by-ruby-on-rails/", "categories": "Framework, Rails", "tags": "rails, api, rspec, shortenURL", "date": "2022-04-16 00:00:00 +0700", "snippet": "How to build a Shortened URL By Ruby On Rails like Bitly or TwitterGive a website shortenedUrl to convert longURL to short URL with alpha lexigraphy encoded/decoded response to User. Give two endp...", "content": "How to build a Shortened URL By Ruby On Rails like Bitly or TwitterGive a website shortenedUrl to convert longURL to short URL with alpha lexigraphy encoded/decoded response to User. Give two endpoint encoded decoded Extra default enpoint is index, and retore lasted encoded url About algorithm used on website. I use Bijective algorithm to make a alpha lexigraphy short url. Give rate limit process to request IP and prevent attack vector. I was use gem rack-attack for this case to handle limit request per IP is 5 request to 2 second. other wise, better to authentic user before they use api. To do this prevent, can implement a mechanism authentic by a system by api key for user indetify and in session. System was give an URL with a unique shortURL to decrease weight in database and open scale up if have mutilple dabase installed. To prepare for scale up: 1. Give a better cache system support Rate limit. 2. With URL was created before, we change to operation select instead create same URL. 3. Given a load ballancing to suppot this load is even better than server serve every single request. 4. Architecture from Monolith or micro service and under control of load balancing give us a good reputation.Install System Ruby version ^2.5~3.0 Rails rack-attack gem Rspec factory_bot_rails fakerTo install run command below:bundleAPI in useThree enpoint with two default enpoint(/api/v1/shortner/): index /api/v1/shortner: Get methodjson response: { \"message\": \"Hello shortner\", \"encoded\": \"q\", \"url\": \"http://google.com\"} encoded /api/v1/shortner/encoded: Post methodJson request: { \"url\": \"http://google.com\"} Json Response { \"url\": \"http://google.com\", \"decoded\": \"b\"} decoded request /api/v1/shortner/decoded: Post methodJson Request { \"encoded\": \"e\"} Json Response { \"url\": \"http://google.com\", \"status\": 200} Last URL saved Last user saved was restore in Database. I was use default by sqlite3. But for better performance and production case need to setup dabase with MYSQL or PostgresSQL.encoded def generate_slug last_id = ShortenUrl.last ? ShortenUrl.last.id : 1 self.slug = Bijective.bijective_encode(last_id) enddecodedurl = ShortenUrl.find_by_slug params[:encoded]url.urlAlgorithm to conver longURL to Shortclass Bijective ALPHABET = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\".split(//) # make your own alphabet using: # (('a'..'z').to_a + ('A'..'Z').to_a + (0..9).to_a).shuffle.join def self.bijective_encode(i) # from http://refactormycode.com/codes/125-base-62-encoding # with only minor modification return ALPHABET[0] if i == 0 s = '' base = ALPHABET.length while i &gt; 0 s &lt;&lt; ALPHABET[i.modulo(base)] i /= base end s.reverse end def self.bijective_decode(s) # based on base2dec() in Tcl translation # at http://rosettacode.org/wiki/Non-decimal_radices/Convert#Ruby i = 0 base = ALPHABET.length s.each_char { |c| i = i * base + ALPHABET.index(c) } i endendTest application with SpecTest case was specified with controller and function endoint. To run test from root of project run command:rspecSumarizeGive a website for serve enpoint to convert long url to short url, you can find source code at my github repository." }, { "title": "Decoupling Ruby on Rails: Delegation vs Dependency Injection", "url": "/posts/decoupling-ruby-on-rails-delegation-vs-dependency-injection/", "categories": "Fullstack, Backend", "tags": "ruby, solid, dependency, rails, oop", "date": "2022-04-16 00:00:00 +0700", "snippet": "Delegation &amp;&amp; Dependency InjectionDelegationRuby’s standard library provides us SimpleDelegator as an easy way to implement the decorator pattern. You pass in your object to the constructor...", "content": "Delegation &amp;&amp; Dependency InjectionDelegationRuby’s standard library provides us SimpleDelegator as an easy way to implement the decorator pattern. You pass in your object to the constructor, and then any method calls to the delegator are forwarded to your object.E.g:class PrawnWrapper &lt; SimpleDelegator def initialize(document: nil) document ||= Prawn::Document.new(...) super(document) endendWe can then update our reports to inherit from this class, and they will still function the same as before, using the default document created in our initializer. The magic happens when we use this in our overview report:class OverviewReport &lt; PrawnWrapper ... def render sales = SaleReport.new(..., document: self) sales.sales_table costs = CostReport.new(..., document: self) costs.costs_pie_chart ... endendwe have essentially made it as if SalesReport is now a subclass of OverviewReport. In our case, this means that all the calls to prawn’s API now go SalesReport -&gt; OverviewReport -&gt; Prawn::Document.How SimpleDelegator Works: deletegation as servicesThe way SimpleDelegator works under the hood is basically to use Ruby’s method_missing functionality to forward method calls to another object.So SimpleDelegator (or a subclass of it) receives a method call. If it implements that method, great; it will execute it just as any other object would. However, it if does not have that method defined, then it will hit method_missing. method_missing will then attempt to call that method on the object given to its constructor.require 'simple_delegator'class Thing def one 'one' end def two 'two' endendclass ThingDecorator &lt; SimpleDelegator def two 'three!' endendThingDecorator.new(Thing.new).one #=&gt; \"one\"ThingDecorator.new(Thing.new).two #=&gt; \"three!\" The key difference: SimpleDelegator takes the object it will delegate to as an argument in its constructor. This means we can pass in different objects at runtime.This is what allows use to redirect the calls to a prawn object in Solution 2 above. If we call a single report, the prawn calls go to a new document created in the constructor. The overview report, however, can change this so that calls to prawn are forwarded to its document.Delegate in Active RecordForward methods or attributes from class to subclassclass Greeter &lt; ActiveRecord::Base def hello 'hello' end def goodbye 'goodbye' endendclass Foo &lt; ActiveRecord::Base belongs_to :greeter delegate :hello, to: :greeterendFoo.new.hello # =&gt; \"hello\"Foo.new.goodbye # =&gt; NoMethodError: undefined method `goodbye' for #&lt;Foo:0x1af30c&gt;Dependency Injection(DI)ConceptIn software engineering, dependency injection is a design pattern in which an object receives other objects that it depends on. A form of inversion of control, dependency injection aims to separate the concerns of constructing objects and using them, leading to loosely coupled programs.[1][2][3] The pattern ensures that an object which wants to use a given service should not have to know how to construct those services. Instead, the receiving object (or ‘client’) is provided with its dependencies by external code (an ‘injector’), which it is not aware of.[4] Dependency injection solves the following problems:[5] How can a class be independent from the creation of the objects it depends on? How can an application, and the objects it uses support different configurations? How can the behavior of a piece of code be changed without editing it directly? Fundamentally, dependency injection consists of passing parameters to a method. As mentioned previously, one common solution to this kind of problem is to refactor the code to use Dependency Injection. That is, rather than having all these reports call methods on self, we will instead pass in our PDF document as an argument.Dependency injection in servicesThis would give us something more like:class CostReport &lt; Prawn::Document... def title(pdf = self) pdf.text \"Cost Report\" pdf.move_down 20 ... endendThis works, but there is some overhead here. For one thing, every single drawing method now has to take the pdf argument, and every single call to prawn now has to go through this pdf argument.Dependency injection has some benefits: it pushes us toward decoupled components in our system and allows us to pass in mocks or stubs to make unit testing easier.However, we are not reaping the rewards of these benefits in our scenario. We are already strongly coupled to the prawn API, so changing to a different PDF library would almost certainly require an entire rewrite of the code.Testing is also not a big concern here, because in our case testing generated PDF reports with automated tests is too cumbersome to be worthwhile.Delegation and Dependency InjectionI want to mention, when Developer need to implement Dependency Injection in Rails is make something double and couple because It’s likely we want to create multiple services different and less couple with base class used While Ruby on Rails is every where we can fine or define an Object and we spent manage effort with each class services.So it less use than delegation.Let think about a Document class want to define a method to render a report. By DI we mush define a method with parameter is Interface represent of class handling services.But in delegate we make a less couple but still maintain connection between two class, by passing a method from other class to use in this class. This behavious is same but one is Rails define and support, one is design pattern so we can choose to use an approach with each use case.Use Case when DI is always better: Declare services with long time developement but need behavious we wanted, So we can boot a class use DI when bootstrap a service so give a best performance while delegate is must require the clarify of other class method and this required a check behavious: input and output before using delegate." }, { "title": "Loop with decrement order in Python, Ruby, PHP.", "url": "/posts/loop-with-decrement-order-in-Python-Ruby-PHP/", "categories": "Fullstack, Python", "tags": "python, ruby, php, loop, for-loop", "date": "2022-03-12 00:00:00 +0700", "snippet": "Loop in decrease modePythonIn Python, decrement is not clearly supported by syntax.By default, the loop is used to iterate over the sequence: a list, a tuple, a dictionary, a set, or a string.For &...", "content": "Loop in decrease modePythonIn Python, decrement is not clearly supported by syntax.By default, the loop is used to iterate over the sequence: a list, a tuple, a dictionary, a set, or a string.For &lt;variable&gt; in range(start index, stop index, step) Variable: A variable is a value that starts from the start index and ends at the stop index in for loop. Start index: It is an optional value. If not passed, it starts from 0. It is the integer value from which the for loop value gets starts iterating. Stop index: It is the integer value from which the for loop value gets stops iterating.Step: It is the optional value. If not passed it increment the value by 1. It is an integer value that defines the increment and decrement of the loop.1. Using Start, Stop Index, and step to Decrement for loop in Python#Start index and stop index valuestartindex = 10stopindex = 0step = -1 #applying for loopfor i in range(startindex, stopindex, step): print(i)2. Using reversed() Function to Decrement for loop in PythonReversed() function is used to loop over a sequence in reverse order#applying reversed() function for i in reversed(range(5)): print('Output : ',i)3. Using whileThis way look isn’t an elegant way, but while was introduction is an infinite loop. So everything is hard to approach you can start to think about using while#applying while and -= operations = 10while s&gt;0: print(\"Output : \",s) s -= 1RubyFirst way:for i in (10).downto(0) puts iendSecond way:(10).downto(0) do |i| puts iendThird way:i=10;until i&lt;0 puts i i-=1endFour way:i = 10while i &gt; 0 i -= 1endPHPElegant way:for($i=10,$j=1;$i&gt;=1;$i--){ echo $i.'&lt;br&gt;'; echo $j.'&lt;br&gt;'; $j++;}" }, { "title": "Repository pattern và repository pattern trong laravel", "url": "/posts/Repository-pattern-va-repository-pattern-trong-laravel/", "categories": "Fullstack, Backend, DesignPattern", "tags": "design-pattern, laravel", "date": "2021-07-31 00:00:00 +0700", "snippet": "Repository patternKhái niệmDesign Pattern là một kỹ thuật trong lập trình hướng đối tượng, nó khá quan trọng và mọi lập trình viên muốn giỏi đều phải biết. Được sử dụng thường xuyên trong các ngôn ...", "content": "Repository patternKhái niệmDesign Pattern là một kỹ thuật trong lập trình hướng đối tượng, nó khá quan trọng và mọi lập trình viên muốn giỏi đều phải biết. Được sử dụng thường xuyên trong các ngôn ngữ OOP. Nó sẽ cung cấp cho bạn các “mẫu thiết kế”, giải pháp để giải quyết các vấn đề chung, thường gặp trong lập trình. Các vấn đề mà bạn gặp phải có thể bạn sẽ tự nghĩ ra cách giải quyết nhưng có thể nó chưa phải là tối ưu. Design Pattern giúp bạn giải quyết vấn đề một cách tối ưu nhất, cung cấp cho bạn các giải pháp trong lập trình OOP.Repository pattern là một trong các design pattern, đóng vai trò như một lớp trung gian kết nối giữa tầng business logic và database layer. Điều này sẽ làm bớt việc viết logic tại controller cũng như ở model. Những lý do ta nên sử dụng mẫu Repository Pattern: Code dễ dàng maintain. Tăng tính bảo mật và rõ ràng cho code. Lỗi ít hơn. tránh việc lặp code.Định nghĩa theo sách Patterns of Enterprise Application Architecture, Martin Fowler A repository performs the tasks of an intermediary between the domain model layers and data mapping, acting in a similar way to a set of domain objects in memory. Client objects declaratively build queries and send them to the repositories for answers. Conceptually, a repository encapsulates a set of objects stored in the database and operations that can be performed on them, providing a way that is closer to the persistence layer. Repositories, also, support the purpose of separating, clearly and in one direction, the dependency between the work domain and the data allocation or mapping.Thay vì việc viết logic vào trong controller, bạn có thể tạo thư mục Repository và tạo file repository trùng với class model. Sử dụng bằng các inject vào trong controller thông qua construct.Laravel repository patternReferencesDesign the infrastructure persistence layerTổng hợp các bài hướng dẫn về Design Pattern" }, { "title": "Phương thức thanh toán Softbank cài đặt bằng ngôn ngữ PHP", "url": "/posts/softbank-payment-japan/", "categories": "Tutorial, Payment", "tags": "php, softbank, au, paypay", "date": "2021-07-18 00:00:00 +0700", "snippet": "Phương thức Softbank payment bằng PHPTôi sẽ bắt đầu với nội dung bài viết luôn, với lý do là có biết bao phương thức payment và nếu bạn làm việc ở thị trường Nhật thì softbank payment là một phương...", "content": "Phương thức Softbank payment bằng PHPTôi sẽ bắt đầu với nội dung bài viết luôn, với lý do là có biết bao phương thức payment và nếu bạn làm việc ở thị trường Nhật thì softbank payment là một phương thức thanh toán khá thông dụng.Liên kết: Địa chỉ softbank: https://www.sbpayment.jp/ Địa chỉ cho lập trình viên: https://developer.sbpayment.jp/ Bài viết tham khảo: https://kipalog.com/posts/Tich-hop-softbank-payment-cho-trang-web-cua-ban Softbank paymentSoftbank payment là một phương thức thanh toán khá thông dụng tại các website thương mại điện tử tại Nhật, nó đáp ứng một số lượng lớn các thẻ thanh toán nội địa cũng như master, visa, ngoài ra alipay v..v… Softbank payment cung cấp 2 cách để thanh toán(link type, API type): Api tích hợp Liên kết tới trang trung gian của softbank payment. \\ Phương thức bảo mật: 3D secure (3Ds), giống như phương thức thanh toán xác thực 2 bướcSoftbank payment cũng có dịch vụ khá giống với stripe: Online payment service Payment service for stores: pot, qr, mobile Partner program Tích hợp softbank payment Đăng ký tài khoản các bạn vào link sau đây để đăng ký tài khoản nhé:https://developer.sbpayment.jp/user_regist/consent Softbank cung cấp môi trường sanbox nếu bạn muốn test hoặc trải nghiệm. Trong bài viết này tôi muốn giới thiệu đến phương thức thanh toán sự dụng liên kết tới bên thứ ba Cài đặt form Mẫu form tương ứng: gồm các thông số kèm order chi tiết. Thông tin form, order order.pay_method = \"\";order.merchant_id = \"30132\";order.service_id = \"001\"; Thông tin merchant credit：クレジットカード決済credit3d：クレジットカード決済（本人認証サービス（3D セキュア））unionpay：銀聯ネット決済webcvs：Web コンビニ決済payeasy：Pay-easy 決済banktransfer：総合振込決済cyberedy：楽天 Edy 決済（楽天 Edy）mobileedy：楽天 Edy 決済（モバイル楽天 Edy）suica：モバイル Suica 決済webmoney：WebMoney 決済netcash：Net Cash 決済bitcash：BitCash 決済prepaid：JCB PREMO 決済docomo：ドコモ払いauone：au かんたん決済softbank：S!まとめて支払いyahoowallet：Yahoo!ウォレット決済yahoowalletdg：Yahoo!ウォレット決済（デジコン版）rakuten：楽天ペイ（オンライン決済）recruit：リクルートかんたん支払いalipay：Alipay 国際決済paypal：Paypal 決済netmile：ネットマイル決済mysoftbank：ソ フトバ ンク まとめて支払い（Ａ）softbank2：ソフトバンクまとめて支払い（Ｂ）saisonpoint：永久不滅ポイントlinepay：LINE Paytpoint：T ポイントプログラム（オンライン決済）applepay：Apple Paynppostpay：NP 後払い Mình chọn phương thức paypay , bạn có thể nhập chực tiếp paypay không cần nhập mã id của các phương thức.Bạn vào trang paypay và đăng ký tài khoản developer tại đây https://www.paypal.com \\ Tôi có lấy mẫu từ trang developer:function f_submit() { var order = new Order(); order.pay_method = \"1234\"; order.merchant_id = \"30132\"; order.service_id = \"001\"; order.cust_code = \"Merchant_TestUser_999999\"; order.sps_cust_no = \"1234\"; order.sps_payment_no = \"1234\"; order.order_id = \"a1a1be0a082ac31eedbd126c8b0b0d5a\"; order.item_id = \"T_0003\"; order.pay_item_id = \"\"; order.item_name = \"テスト商品\"; order.tax = \"\"; order.amount = \"1\"; order.pay_type = \"0\"; order.auto_charge_type = \"\"; order.service_type = \"0\"; order.div_settele = \"\"; order.last_charge_month = \"\"; order.camp_type = \"\"; order.tracking_id = \"\"; order.terminal_type = \"0\"; order.success_url = \"http://stbfep.sps-system.com/MerchantPaySuccess.jsp\"; order.cancel_url = \"http://stbfep.sps-system.com/MerchantPayCancel.jsp\"; order.error_url = \"http://stbfep.sps-system.com/MerchantPayError.jsp\"; order.pagecon_url = \"http://stbfep.sps-system.com/MerchantPayResultRecieveSuccess.jsp\"; order.free1 = \"\"; order.free2 = \"\"; order.free3 = \"\"; order.free_csv_input = \"LAST_NAME=鈴木,FIRST_NAME=太郎,LAST_NAME_KANA=スズキ,FIRST_NAME_KANA=タロウ,FIRST_ZIP=210,SECOND_ZIP=0001,ADD1=岐阜県,ADD2=あああ市あああ町,ADD3=,TEL=12345679801,MAIL=aaaa@bb.jp,ITEM_NAME=TEST ITEM\"; order.request_date = \"20210718190350\"; order.limit_second = \"\"; order.hashkey = \"c48e0e2c7d04f0954594f14c7801bd430ca6263e\"; var orderDetail = new OrderDetail(); orderDetail.dtl_rowno = \"1\"; orderDetail.dtl_item_id = \"dtlItem_1\"; orderDetail.dtl_item_name = \"明細商品名1\"; orderDetail.dtl_item_count = \"1\"; orderDetail.dtl_tax = \"1\"; orderDetail.dtl_amount = \"1\"; orderDetail.dtl_free1 = \"\"; orderDetail.dtl_free2 = \"\"; orderDetail.dtl_free3 = \"\"; order.orderDetail.push(orderDetail); // フリーCSV order.free_csv = base64.encode(order.free_csv_input, 1); //チェックサム order.sps_hashcode = Sha1.hash( order.toString() ); feppost(order);}// オブジェクト定義[OrderDetail]function OrderDetail(){ this.toString = function() { var result = this.dtl_rowno + this.dtl_item_id + this.dtl_item_name + this.dtl_item_count + this.dtl_tax + this.dtl_amount + this.dtl_free1 + this.dtl_free2 + this.dtl_free3; return result; }}// オブジェクト定義[Order]function Order(){ this.orderDetail = new Array(); this.toString = function() { var resultOrderDetail = \"\"; for (i = 0; i &lt; this.orderDetail.length; i++) { resultOrderDetail = resultOrderDetail + this.orderDetail[i].toString(); } var result = this.pay_method + this.merchant_id + this.service_id + this.cust_code + this.sps_cust_no + this.sps_payment_no + this.order_id + this.item_id + this.pay_item_id + this.item_name + this.tax + this.amount + this.pay_type + this.auto_charge_type + this.service_type + this.div_settele + this.last_charge_month + this.camp_type + this.tracking_id + this.terminal_type + this.success_url + this.cancel_url + this.error_url + this.pagecon_url + this.free1 + this.free2 + this.free3 + this.free_csv + resultOrderDetail + this.request_date + this.limit_second + this.hashkey; return result; };}// 日時の取得function getYYYYMMDDHHMMSS(){ var now = new Date(); return now.getFullYear() + zeroPadding(now.getMonth() + 1) + zeroPadding(now.getDate()) + zeroPadding(now.getHours()) + zeroPadding(now.getMinutes()) + zeroPadding(now.getSeconds());}function zeroPadding(num) { if (num &lt; 10) { num = \"0\" + num; }\treturn num + \"\";}function feppost(order) { var connectUrl = \"https://stbfep.sps-system.com/f01/FepBuyInfoReceive.do\"; var form = $('&lt;form&gt;&lt;/form&gt;',{action:connectUrl,target:'receiver',method:'POST'}).hide(); var body = $('body'); body.append(form); form.append($('&lt;input&gt;',{type:'hidden',name:'pay_method' ,value:order.pay_method })); form.append($('&lt;input&gt;',{type:'hidden',name:'merchant_id' ,value:order.merchant_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'service_id' ,value:order.service_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'cust_code' ,value:order.cust_code })); form.append($('&lt;input&gt;',{type:'hidden',name:'sps_cust_no' ,value:order.sps_cust_no })); form.append($('&lt;input&gt;',{type:'hidden',name:'sps_payment_no' ,value:order.sps_payment_no })); form.append($('&lt;input&gt;',{type:'hidden',name:'order_id' ,value:order.order_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'item_id' ,value:order.item_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'pay_item_id' ,value:order.pay_item_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'item_name' ,value:order.item_name })); form.append($('&lt;input&gt;',{type:'hidden',name:'tax' ,value:order.tax })); form.append($('&lt;input&gt;',{type:'hidden',name:'amount' ,value:order.amount })); form.append($('&lt;input&gt;',{type:'hidden',name:'pay_type' ,value:order.pay_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'auto_charge_type' ,value:order.auto_charge_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'service_type' ,value:order.service_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'div_settele' ,value:order.div_settele })); form.append($('&lt;input&gt;',{type:'hidden',name:'last_charge_month' ,value:order.last_charge_month })); form.append($('&lt;input&gt;',{type:'hidden',name:'camp_type' ,value:order.camp_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'tracking_id' ,value:order.tracking_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'terminal_type' ,value:order.terminal_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'success_url' ,value:order.success_url })); form.append($('&lt;input&gt;',{type:'hidden',name:'cancel_url' ,value:order.cancel_url })); form.append($('&lt;input&gt;',{type:'hidden',name:'error_url' ,value:order.error_url })); form.append($('&lt;input&gt;',{type:'hidden',name:'pagecon_url' ,value:order.pagecon_url })); form.append($('&lt;input&gt;',{type:'hidden',name:'free1' ,value:order.free1 })); form.append($('&lt;input&gt;',{type:'hidden',name:'free2' ,value:order.free2 })); form.append($('&lt;input&gt;',{type:'hidden',name:'free3' ,value:order.free3 })); form.append($('&lt;input&gt;',{type:'hidden',name:'free_csv' ,value:order.free_csv })); form.append($('&lt;input&gt;',{type:'hidden',name:'request_date' ,value:order.request_date })); form.append($('&lt;input&gt;',{type:'hidden',name:'limit_second' ,value:order.limit_second })); form.append($('&lt;input&gt;',{type:'hidden',name:'hashkey' ,value:order.hashkey })); form.append($('&lt;input&gt;',{type:'hidden',name:'sps_hashcode' ,value:order.sps_hashcode })); for (i = 0; i &lt; order.orderDetail.length; i++) { form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_rowno' ,value:order.orderDetail[i].dtl_rowno })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_item_id' ,value:order.orderDetail[i].dtl_item_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_item_name' ,value:order.orderDetail[i].dtl_item_name })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_item_count' ,value:order.orderDetail[i].dtl_item_count })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_tax' ,value:order.orderDetail[i].dtl_tax })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_amount' ,value:order.orderDetail[i].dtl_amount })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_free1' ,value:order.orderDetail[i].dtl_free1 })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_free2' ,value:order.orderDetail[i].dtl_free2 })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_free3' ,value:order.orderDetail[i].dtl_free3 })); } form.submit();}bước 3: Ajax call back $.ajax({ method: 'POST', data: { free_csv: order.free_csv_input }, url: urlGetFreeCsv }).done((encrypt) =&gt; { //チェックサム order.free_csv = encrypt; $.ajax({ method: 'POST', data: { text: order.toString() }, url: urlgenHashKey }).done((sps_hashcode) =&gt; { order.sps_hashcode = sps_hashcode; feppost(order); }); });3D-Secure callbackVì sử dụng 3Ds nên phía ngân hàng bắt buộc mình phải mã hoá một số nội dung liên quan đên thông tin tài khoản ( one time token ) bắt mình phải mã hoá dữ liệu theo thuật toán 3DES như sau:bước 1: Tạo spsHashcode public function generateSpsHashcode() { $data = $this-&gt;request-&gt;data; if (isset($data['text']) &amp;&amp; $data['text']) { $text = $data['text']; echo $this-&gt;generateSpsHashcode($text); } die; }private function generateSpsHashcode($string) { $hashKey = Configure::read('Purchase.hashKey'); return sha1($string.$hashKey); }bước 2: tạo free csvpublic function generateFreeCsv() { $data = $this-&gt;request-&gt;data; if (isset($data['free_csv']) &amp;&amp; $data['free_csv']) { $text = $data['free_csv']; echo $this-&gt;encrypt3DES($text);die; } die; }private function encrypt3DES($text) { $text = $this-&gt;padding3DES($text); $cipher = 'DES-EDE3-CBC'; $secretKey = Configure::read('Purchase.secretKey'); $iv = Configure::read('Purchase.iv'); if (in_array($cipher, openssl_get_cipher_methods())) { $ciphertext = openssl_encrypt($text, $cipher, $secretKey, $options=0, $iv); } return $ciphertext; }private function padding3DES($text) { $mod = strlen($text) % 8; if ($mod === 0) { return $text; } $padding = ''; for ($i = 0; $i &lt; 8 - $mod; $i++) { $padding .= ' '; } return $text.$padding; }Callback URL code Success Error Cancel Pagecon callback: Đây là url quan trọng để khi bạn xác thực thành công thông tin thẻ, ngân hàng sẽ request đến url này của bạn báo rằng KH có thể thanh toán được, ở đây mình phải tiến hành xử lí cho tài khoản đã thanh toán tuỳ vào nghiệp vụ của trang web, sau đó trả về OK nếu thành công hoặc NG nếu thất bại.Kết LuậnĐối với phương thức thanh toán bằng softbank, tôi cũng chỉ khái quát được nội dung và cách kết nối. Phần xử lý callback có lẽ mình sẽ viết kỹ hơn vào bài sau nếu có thời gian.Chú thích 3D-secure: is a protocol designed to be an additional security layer for online credit and debit card transactions. The name refers to the “three domains” which interact using the protocol: the merchant/acquirer domain, the issuer domain, and the interoperability domain." }, { "title": "Trình soạn thảo cho jekyll", "url": "/posts/jekyll-admin-simple-but-power-editor-for-jekyll/", "categories": "Blogging, Tutorial", "tags": "jekyll, rails, bugs", "date": "2021-07-18 00:00:00 +0700", "snippet": "Jekyll admin editorJekyll admin được cung cấp bới Opensource, với hệ thống support khá khiêm tốn hiện tại jekyll-admin đang dừng lại version 0.11.0 nhưng nếu chỉ là khiếm tốn thì tôi sẽ không giới ...", "content": "Jekyll admin editorJekyll admin được cung cấp bới Opensource, với hệ thống support khá khiêm tốn hiện tại jekyll-admin đang dừng lại version 0.11.0 nhưng nếu chỉ là khiếm tốn thì tôi sẽ không giới thiệu với các bạn. Nếu bạn đã sử dụng jekyll, một loại blog cá nhân được viết bằng markdown, tôi cũng đã giới thiệu trong một số bài viết trước đó.Jekyll admin là gì ?như tiêu đề của bài viết :heheƯu điểm Đơn giản Dễ sử dụng Ngắn gọn markdown support Configuration support Trình quản lý jekyll khá ổn .. mà vẫn thiếu thiếu, chả hiểu tại sao Hoàn thiện tốt với jekyll Cung cấp thẻ tags và thẻ categories(dùng làm menus) Giao diện khoa họcNhược điểmDo là một trang web quản lý nội dung bài việt của jekyll, nó không thể tránh khỏi những nhược điểm:Đơn giản Nhiều lỗi được báo không chính xác(còn tôi thì đoán chính xác là do ông nào try catch rồi vứt cái locale.yml …)Phiên bản hiện nay của jekyll-admin là 0.11.0 cũng đã lâu rồi so với jekyll, và với mỗi lần bạn update jekyll có thể sẽ gặp lỗi. \\Trong bài viết, tôi sẽ đề cập tới một lỗi ruby 2.4.0 jekyll 4.2.0 jekyll-admin 0.11.0Lỗi jekyll admin server cannot init configurationbạn fix thế này nhé, bước 1: chọn 1 editor, chọn vào thư mục .rvm hoặc rbenv nếu rbenv là trình quản lý ruby, chọn vào thư mục gems/lib/jekell-admin/server.rb bước 2: function configuration bạn chịnh sửa nội dung như sau: # Computed configuration, with updates and defaults # Returns an instance of Jekyll::Configuration def configuration @configuration ||= site.config.merge(overrides) if @configuration.is_a?(Hash) @configuration = @configuration.each_with_object(Jekyll::Configuration.new) { |(k, v), hsh| hsh[k] = v } end end bước 3: nếu bạn bảo sao tự dưng lại lỗi, ừ thì do phiên bản ruby ^^!" }, { "title": "Rails dynamic notifications by ajax", "url": "/posts/rails-dynamic-notifications/", "categories": "Fullstack, Rails", "tags": "rails, javascript, notification, ajax", "date": "2021-04-09 00:00:00 +0700", "snippet": "Controller def get_data_notification per_page = 5 page = params[:page].present? ? params[:page].to_i : 1 all_notiff = NotificationServices::GetList.new(current_account).run() @total_no...", "content": "Controller def get_data_notification per_page = 5 page = params[:page].present? ? params[:page].to_i : 1 all_notiff = NotificationServices::GetList.new(current_account).run() @total_notification = all_notiff.count @count_unread = all_notiff.where(mark_readed: false).count @notifications = all_notiff.page(page).per(per_page) @current_page_notification = page if page.present? @next_page_notification = @total_notification &gt; page*per_page ? page + 1 : nil else @next_page_notification = @total_notification &gt; per_page ? 2 : nil end end def index per_page = 5 page = params[:page].present? ? params[:page].to_i : 1 all_notiff = NotificationServices::GetList.new(current_account).run() @total_notification = all_notiff.count @count_unread = all_notiff.where(mark_readed: false).count @notifications = all_notiff.page(page).per(per_page) @current_page_notification = page if page.present? @next_page_notification = @total_notification &gt; page*per_page ? page + 1 : nil else @next_page_notification = @total_notification &gt; per_page ? 2 : nil end @page = page endERB page notification erb&lt;% if notifications.present? %&gt; &lt;div id=\"itemNotification\"&gt; &lt;% notifications.each.with_index do |nt, index| %&gt; &lt;% if index &gt; 0 || current_page_notification &gt; 1 %&gt; &lt;hr class=\"m-0\"&gt; &lt;% end %&gt; &lt;%= link_to mark_unread_pages_notification_path(nt), class: \"dropdown-item d-flex #{ 'unread-message' unless nt.mark_readed }\", style:\"position: relative;\" do %&gt; &lt;div class=\"mr-2\"&gt; &lt;div class=\"icon-circle text-center text-uppercase text-white bg-primary bottom-2px\" style=\"height: 30px; width: 30px;\"&gt; &lt;span class=\"position-relative font-size-12px\"&gt;&lt;%= parse_letter_name(nt.owner_first_name, nt.owner_last_name) if nt.owner_id.present? %&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;p class=\"font-size-16px font-weight-bold mb-1\"&gt;&lt;%= notification_title(nt.view_type, nt.team_name, nt.owner_fullname) %&gt;&lt;/p&gt; &lt;div class=\"clearfix\"&gt;&lt;/div&gt; &lt;p class=\"font-size-16px text-gray-500 mb-3\"&gt;\"&lt;%= truncate(notification_description(nt, nt.view_type, nt.team_name, nt.objective_name).html_safe, length: 40) %&gt;\"&lt;/p&gt; &lt;div class=\"font-size-16px text-gray-500\"&gt;&lt;%= parse_ago_time(nt.created_at, current_account.timezone) %&gt;&lt;/div&gt; &lt;/div&gt; &lt;% unless nt.mark_readed %&gt; &lt;div class=\"dot-unread-message\"&gt;&lt;/div&gt; &lt;% end %&gt; &lt;% end %&gt; &lt;% end %&gt; &lt;/div&gt;&lt;% elsif !defined?(@page) %&gt; &lt;div class=\"dropdown-item font-size-16px text-gray-500\"&gt; No notification &lt;/div&gt;&lt;% end %&gt;&lt;script&gt;&lt;/script&gt;1.1 pagination erb&lt;% if next_page_notification.present? %&gt;&lt;div id=\"itemPagination\" class=\"d-none\"style=\"cursor: pointer;\" onclick=\"getMoreNotification('&lt;% next_page_notification %&gt;')\" data-total=\"&lt;%= @total_notification %&gt;\"&gt; &lt;form&gt; &lt;hr class=\"m-0\"&gt; &lt;div class=\"clearfix\"&gt;&lt;/div&gt; &lt;div class=\"padding-y-10px text-center\"&gt; &lt;p class=\"m-0 has-spinner\" id=\"loadmore_btn\"&gt;&lt;/p&gt; &lt;/div&gt; &lt;/form&gt;&lt;/div&gt;&lt;script type=\"text/javascript\"&gt; var nextpage = &lt;%= @next_page_notification %&gt;; $(\"#dataNotification\").on(\"scroll\", function() { var total = &lt;%= @total_notification %&gt;; if ((nextpage - 1) * 5 &gt; total) return false; if($(this).scrollTop() + $(this).innerHeight() &gt;= $(this)[0].scrollHeight) { $(\"#itemPagination\").removeClass(\"d-none\"); var btn = $(\"#loadmore_btn\"); $(\"#loadmore_btn\").buttonLoader('start'); $.ajax({ type: 'GET', url: \"/pages/notifications\", data: { page: nextpage }, success: function(data) { $(\"#loadmore_btn\").buttonLoader('stop'); nextpage ++; } }); } }); (function($) { $.fn.buttonLoader = function(action) { var self = $(\"#loadmore_btn\"); if (action == 'start') { if ($(self).attr(\"disabled\") == \"disabled\") { } //disable buttons when loading state $('.has-spinner').attr(\"disabled\", \"disabled\"); $(self).attr('data-btn-text', $(self).text()); //binding spinner element to button and changing button text $(self).html('&lt;span class=\"spinner\"&gt;&lt;i class=\"fa fa-2x fa-spinner fa-spin\"&gt;&lt;/i&gt;&lt;/span&gt;'); $(self).addClass('active'); } //stop loading animation if (action == 'stop') { var self = $(\"#loadmore_btn\"); $(self).html(''); $(self).removeClass('active'); //enable buttons after finish loading $('.has-spinner').removeAttr(\"disabled\"); } } })(jQuery);&lt;/script&gt;&lt;% end %&gt;CSS.spinner { display: inline-block; opacity: 0; width: 0; -webkit-transition: opacity 0.25s, width 0.25s; -moz-transition: opacity 0.25s, width 0.25s; -o-transition: opacity 0.25s, width 0.25s; transition: opacity 0.25s, width 0.25s;}.has-spinner.active { cursor: progress;}.has-spinner.active .spinner { opacity: 1; width: auto;}.has-spinner.btn.active .spinner { min-width: 20px;}JS javascript for pagination $(\"#dataNotification\").append(\"&lt;%= j render 'layouts/shared/pages/item_notifications', notifications: @notifications, current_page_notification: @current_page_notification %&gt;\");$(\"#dataPagination\").empty().append(\"&lt;%= j render 'layouts/shared/pages/pagination_notifications', next_page_notification: @next_page_notification %&gt;\"); load js&lt;script type=\"text/javascript\"&gt; var nextpage = &lt;%= @next_page_notification %&gt;; $(\"#dataNotification\").on(\"scroll\", function() { var total = &lt;%= @total_notification %&gt;; if ((nextpage - 1) * 5 &gt; total) return false; if($(this).scrollTop() + $(this).innerHeight() &gt;= $(this)[0].scrollHeight) { $(\"#itemPagination\").removeClass(\"d-none\"); var btn = $(\"#loadmore_btn\"); $(\"#loadmore_btn\").buttonLoader('start'); $.ajax({ type: 'GET', url: \"/pages/notifications\", data: { page: nextpage }, success: function(data) { $(\"#loadmore_btn\").buttonLoader('stop'); nextpage ++; } }); } }); (function($) { $.fn.buttonLoader = function(action) { var self = $(\"#loadmore_btn\"); if (action == 'start') { if ($(self).attr(\"disabled\") == \"disabled\") { } //disable buttons when loading state $('.has-spinner').attr(\"disabled\", \"disabled\"); $(self).attr('data-btn-text', $(self).text()); //binding spinner element to button and changing button text $(self).html('&lt;span class=\"spinner\"&gt;&lt;i class=\"fa fa-2x fa-spinner fa-spin\"&gt;&lt;/i&gt;&lt;/span&gt;'); $(self).addClass('active'); } //stop loading animation if (action == 'stop') { var self = $(\"#loadmore_btn\"); $(self).html(''); $(self).removeClass('active'); //enable buttons after finish loading $('.has-spinner').removeAttr(\"disabled\"); } } })(jQuery);&lt;/script&gt;" }, { "title": "Add highlight input filed", "url": "/posts/add-highlight-input-filed/", "categories": "Fullstack, Stylesheets", "tags": "input, bootstrap, blue-highline", "date": "2021-04-08 00:00:00 +0700", "snippet": "textarea:focus,input[type=\"text\"]:focus,input[type=\"password\"]:focus,input[type=\"datetime\"]:focus,input[type=\"datetime-local\"]:focus,input[type=\"date\"]:focus,input[type=\"month\"]:focus,input[type=\"t...", "content": "textarea:focus,input[type=\"text\"]:focus,input[type=\"password\"]:focus,input[type=\"datetime\"]:focus,input[type=\"datetime-local\"]:focus,input[type=\"date\"]:focus,input[type=\"month\"]:focus,input[type=\"time\"]:focus,input[type=\"week\"]:focus,input[type=\"number\"]:focus,input[type=\"email\"]:focus,input[type=\"url\"]:focus,input[type=\"search\"]:focus,input[type=\"tel\"]:focus,input[type=\"color\"]:focus,.uneditable-input:focus { border-color: rgba(44, 130, 201, 1); box-shadow: 0 1px 1px rgba(0, 0, 0, 0.075) inset, 0 0 8px rgba(44, 130, 201, 1); outline: 0 none;}" }, { "title": "Rails validation form", "url": "/posts/rails-validation-form/", "categories": "Fullstack, Rails, Validation", "tags": "rails, html, javascript", "date": "2021-03-28 23:29:00 +0700", "snippet": "By HTML5 and Javascripteg, a submit formUsing setCustomValidity function by default js and oninput=\"checkPasscode(); , required on input field.&lt;form&gt;\t&lt;label for=\"passcode\"&gt;Enter Passco...", "content": "By HTML5 and Javascripteg, a submit formUsing setCustomValidity function by default js and oninput=\"checkPasscode(); , required on input field.&lt;form&gt;\t&lt;label for=\"passcode\"&gt;Enter Passcode:&lt;/label&gt;\t&lt;input id=\"passcode\" \t\t type=\"password\" \t\t placeholder=\"Your passcode\" \t\t oninput=\"checkPasscode();\"\t\t required/&gt;\t&lt;button type=\"submit\"&gt;Submit&lt;/button&gt;&lt;/form&gt;function checkPasscode() {\tvar passcode_input = document.querySelector(\"#passcode\");\t\tif (passcode_input.value != \"Ivy\") {\t\tpasscode_input.setCustomValidity(\"Wrong. It's 'Ivy'.\");\t} else {\t\tpasscode_input.setCustomValidity(\"\"); // be sure to leave this empty!\t\talert(\"Correct!\");\t}}" }, { "title": "Things after install ubuntu 20.04 for developer", "url": "/posts/things-after-install-ubuntu-2004-for-developer/", "categories": "Operation, Ubuntu", "tags": "ubuntu, ubuntu20.04, setup", "date": "2021-03-27 00:00:00 +0700", "snippet": "First stepHere is download file of the install scripFor a quick setup, I recommend that you create a file called install.sh and run the following command every time you install:chmod a+x ./install...", "content": "First stepHere is download file of the install scripFor a quick setup, I recommend that you create a file called install.sh and run the following command every time you install:chmod a+x ./install.sh./install.shFirst step we should update repository ubuntu:sudo apt-get updatesudo apt-get upgrade -yUbuntu packages for developersudo apt-get install build-essential -ysudo apt-get install ubuntu-restricted-extras -yAdd Ubuntu repositorysudo apt-add-repository universe #universe packagessudo add-apt-repository ppa:ondrej/php #php ppasudo add-apt-repository ppa:gerardpuig/ppa # ubuntu cleanersudo add-apt-repository ppa:otto-kesselgulasch/gimp # gimpsudo add-apt-repository ppa:openshot.developers/ppa # openshotqtsudo add-apt-repository -y ppa:teejee2008/ppa #Ubuntu package and toolssudo apt-get install build-essential -ysudo apt-get install ubuntu-restricted-extras -ysudo apt-get install gnome-tweak-tool -ysudo apt-get install synaptic -ywget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.debsudo apt install ./google-chrome-stable_current_amd64.debsudo snap install vlc -ysudo snap install skype -ysudo apt-get install ubuntu-cleaner -ysudo apt-get install simplescreenrecorder -ysudo apt-get install gimp -ysudo apt-get install cool-retro-term -ysudo apt-get install openshot-qt -ysudo apt-get install gnome-tweaks -ysudo apt-get install chrome-gnome-shell -ysudo apt-get install firefox-gnome-shell -ysudo apt-get install rar unrar p7zip-full p7zip-rar -ysudo apt-get install timeshift -ysudo apt-get install ibus-unikey -yibus restartJava installsudo apt-get install openjdk-11-jdk -yLAMP installsudo apt install apache2 -ysudo ufw allow in \"Apache\"sudo apt-get install -y mysql-server mysql-client libmysqlclient-dev libpq-devsudo apt install php libapache2-mod-php php-mysql -ysudo apt-get install libgdbm-dev libncurses5-dev automake libtool bison libffi-devInstall Composer and PHP7.4curl -sS https://getcomposer.org/installer -o composer-setup.phpphp -r \"if (hash_file('SHA384', 'composer-setup.php') === '$HASH') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\"sudo php composer-setup.php --install-dir=/usr/local/bin --filename=composersudo apt-get install -y zip unzip software-properties-commonsudo apt-get install -y php7.4 php7.4-gd php7.4-mbstring php7.4-xml php-zip php7.4-mysqlGit and Configuresudo apt-get install gitgit config --global color.ui truegit config --global user.name \"\"git config --global user.email \"\"ssh-keygen -t rsa -b 4096 -C \"\"MysqlConfigure mysql passwordsudo mysql_secure_installationRuby On Railsgpg --keyserver hkp://pool.sks-keyservers.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDBcurl -sSL https://get.rvm.io | bash -s stablesource ~/.rvm/scripts/rvmsruby -vrvm install 3.0.0rvm use 3.0.0gem install bundlerPythonsudo apt-get install -y python2curl https://bootstrap.pypa.io/get-pip.py --output get-pip.pysudo python2 get-pip.pypip3 install scrapysudo apt-get install -y python3-pipNodejssudo apt install npm -ycurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.3/install.sh | bashnvm list-remotenvm install v14.0.0nvm use v14.0.0npm install --global yarnDocker# Update ap-certificatessudo apt-get updatesudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-releasecurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgecho \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get updatesudo apt-get remove docker docker-engine docker.io containerd runcsudo apt-get install -y docker-ce docker-ce-cli containerd.iosudo apt-get install -y py-pip python3-dev libffi-dev openssl-dev gcc libc-dev rust cargo makesudo usermod -aG docker ${USER}Install Docker Composesudo curl -L \"https://github.com/docker/compose/releases/download/1.28.6/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composesudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-composesource ~/.bashrcDocker Swampsudo apt install -y apt-transport-https ca-certificates curl software-properties-commoncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"sudo apt-get updateapt-cache policy docker-cesudo apt install docker-ceAfter install docker swarm, first step is configure nodesudo vim /etc/hostsAdd following host192.168.1.10\tmanager192.168.1.11\tworker-01192.168.1.12\tworker-02Create Docker Swarm Clustersudo docker swarm init --advertise-addr 192.168.1.10Swarm initialized: current node (fsuaqqpihi2eabmmq8gldzhpv) is now a manager.To add a worker to this swarm, run the following command:sudo docker swarm join --token SWMTKN-1-018kvdektwa74z8fajb5c1u6jyz6qfk4ood8u4qotw7go9jj0p-cfpnh7omy86xcgoh45vau2kaj 192.168.1.10:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.Postgresqlsudo apt install postgresql postgresql-contrib -ysudo curl https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo apt-key addsudo sh -c 'echo \"deb https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/$(lsb_release -cs) pgadmin4 main\" &gt; /etc/apt/sources.list.d/pgadmin4.list &amp;&amp; apt update'sudo apt install pgadmin4 -yConfigure PostgresqlBy default, Postgres uses a concept called “roles” to handle authentication and authorization. These are, in some ways, similar to regular Unix-style users.The installation procedure created a user account called postgres.sudo -i -u postgrespsqlTo exit use this command\\q Create User createuser --interactive Another way if you like linux command```bashsudo -u postgres createuser –interactive –pwpromptOutputEnter name of role to add: sammyShall the new role be a superuser? (y/n) y Create DB createdb sammy Linux Command sudo -u postgres createdb sammy Sync with LinuxTo log in with ident based authentication, you’ll need a Linux user with the same name as your Postgres role and database. sudo adduser sammy Now you can login by this command: sudo -i -u sammypsql Or by that way sudo -u sammy psql full login: psql -h myhost -d mydb -U myuser After tolig you can switch DB by psql -d postgresTo check about connaction: \\conninfoHow To Install PostgreSQL on Ubuntu 20.04 [Quickstart] Service sudo systemctl stop postgresqlsudo systemctl disable postgresql MongoDBwget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -sudo apt-get install gnupgwget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.listsudo apt-get updatesudo apt-get install -y mongodb-orgsudo systemctl disable mongodJekyll#jekyllsudo snap install ruby --classicecho '# Install Ruby Gems to ~/gems' &gt;&gt; ~/.bashrcecho 'export GEM_HOME=\"$HOME/gems\"' &gt;&gt; ~/.bashrcecho 'export PATH=\"$HOME/gems/bin:$PATH\"' &gt;&gt; ~/.bashrcgem install jekyll bundlerTerminal ToolZSHsudo apt-get install zsh -ysh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"echo '# Install Ruby Gems to ~/gems' &gt;&gt; ~/.zshrcecho 'export GEM_HOME=\"$HOME/gems\"' &gt;&gt; ~/.zshrcecho 'export PATH=\"$HOME/gems/bin:$PATH\"' &gt;&gt; ~/.zshrcsudo apt-get install vim -ysudo apt-get install konsole terminator -ychsh -s $(which zsh)Fishsudo apt-get install fish fish-common ondir zsh-syntax-highlightingSoftwareVideo Edit - PEEKsudo add-apt-repository ppa:peek-developers/stablesudo apt updatesudo apt install peekQA tester - screen record and capturesudo apt install flameshot kazam -yGit, PHP storm, gitkraken, postman, utube, hollywordsudo apt install git-flow git-cola -ysudo snap install gitkraken --classicsudo snap install postmansudo snap install phpstorm --classicsudo snap install utubesudo apt-get install hollywoodgnome weathersudo add-apt-repository ppa:gnome-shell-extensionssudo apt-get updatesudo apt-get install gnome-shell-extension-weathersudo dpkg -i mysql-workbench-community-dbgsym_8.0.23-1ubuntu20.04_amd64.debClean APT and configure source list rechecksudo apt-get autocleansudo apt-get autoremovesoftware-properties-gtk" }, { "title": "Cài đặt môi trường Ruby on Rails trên ubuntu 20.04", "url": "/posts/cai-dat-moi-truong-ruby-on-rails-tren-ubuntu/", "categories": "Framework, Rails", "tags": "rails, git, postgresql, mysql", "date": "2021-03-22 00:00:00 +0700", "snippet": "Cài đặt môi trường Ruby on Rails trên ubuntuCài đặt truyền thống RubyĐể cài đặt môi trường code ruby trên ubuntu rất đơn giản vì mặc định ubuntu đã cài đặt sẵn ruby theo từng phiên bản của ubuntu.r...", "content": "Cài đặt môi trường Ruby on Rails trên ubuntuCài đặt truyền thống RubyĐể cài đặt môi trường code ruby trên ubuntu rất đơn giản vì mặc định ubuntu đã cài đặt sẵn ruby theo từng phiên bản của ubuntu.ruby -vTuy nhiên đa số phiên bản ruby này là ruby core để chạy các ứng dụng và chương trình của linux nên không thuận tiện cho chúng ta. Cài đặt lại ruby tại trangsudo apt install curlcurl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.listsudo apt-get updatesudo apt-get install git-core zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev nodejs yarnCài đặt qua công cụ quản lý RubyViệc sử dụng công cụ quản lý rails giúp ta dễ dàng chuyển đổi qua các phiên bản rails theo từng dự án cài đặt tương ứng với một phiên bản ruby cần sử dụng,Thông thường file .ruby-version sẽ cho biết phiên bản ruby và nếu cài đặt công cụ quản lý ruby sẽ tự động cài đặt như ta mong muốn.1. rbenvcài đặt thông qua câu lệnh rbenvcdgit clone https://github.com/rbenv/rbenv.git ~/.rbenvecho 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bashrcecho 'eval \"$(rbenv init -)\"' &gt;&gt; ~/.bashrcexec $SHELLgit clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-buildecho 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' &gt;&gt; ~/.bashrcexec $SHELLrbenv install 3.0.0rbenv global 3.0.0ruby -v2. rvmrvm cũng tương tự nhưng khác câu lệnh với rbenvsudo apt-get install libgdbm-dev libncurses5-dev automake libtool bison libffi-devgpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDBcurl -sSL https://get.rvm.io | bash -s stablesource ~/.rvm/scripts/rvmrvm install 3.0.0rvm use 3.0.0 --defaultruby -vCài đặt Railsgem install rails -v 6.1.1Đối với rbenv, cần rehash lạirbenv rehashKiểm tra lại rails vớirails -vcài đặt bundlergem install bundlerCài đặt DatabasePostgressqlbước 1: cài đặt thư việnsudo apt install postgresql-11 libpq-devbước 2: cài đặt passwordsudo -u postgres createuser hardworkenglishlearner -s# If you would like to set a password for the user, you can do the followingsudo -u postgres psqlpostgres=# \\password 123456Mysqlsudo apt-get install mysql-server mysql-client libmysqlclient-devCài đặt Gitsudo apt-get install gitCấu hình gitgit config --global color.ui truegit config --global user.name \"YOUR NAME\"git config --global user.email \"YOUR@EMAIL.com\"ssh-keygen -t rsa -b 4096 -C \"YOUR@EMAIL.com\"Cầu hình git với ssh-keyssh-keygen -t ed25519 -C \"your_email@example.com\"cat ~/.ssh/id_rsa.pubssh -T git@github.com # test connection Kiểm tra lạiTạo một project với rails#### If you want to use SQLite (not recommended)rails new myapp#### If you want to use MySQLrails new myapp -d mysql#### If you want to use Postgres# Note that this will expect a postgres user with the same username# as your app, you may need to edit config/database.yml to match the# user you created earlierrails new myapp -d postgresql# Move into the application directorycd myapp# If you setup MySQL or Postgres with a username/password, modify the# config/database.yml file to contain the username/password that you specified# Create the databaserake db:createrails server" }, { "title": "Bug thần thánh permission denied @ rb_sysopen", "url": "/posts/bug-rails-gem-server-sysopen/", "categories": "Framework, Rails, Bug", "tags": "rails, bug, rvm", "date": "2021-03-22 00:00:00 +0700", "snippet": "Bug rack/server.rb:362:in initialize: Permission denied @ rb_sysopen (Errno::EACCES)phiên bản sử dụng là rvmTry first update gem ownersudo chown -R userowner:yourgroup ~/.rvmUpdate permissionrvmsu...", "content": "Bug rack/server.rb:362:in initialize: Permission denied @ rb_sysopen (Errno::EACCES)phiên bản sử dụng là rvmTry first update gem ownersudo chown -R userowner:yourgroup ~/.rvmUpdate permissionrvmsudo rvm get stable --auto-dotfilesrvm fix-permissions systemupdate user in group rvmrvm group add rvm $USERÌf not work try update Gemgem update gem cleanup" }, { "title": "Tạo Rails API với Grape", "url": "/posts/Tao-rails-api-voi-grape/", "categories": "Framework, Rails, api, grape", "tags": "rails, api, grape", "date": "2021-03-22 00:00:00 +0700", "snippet": "Trong việc module hóa cũng như tạo cấu trúc REST API bạn có thể tự tạo cho mình một framework riêng hoặc sử dụng một framework đã có và đang được phát triển và ưa dùng là grape. tham khảo tại đâyGr...", "content": "Trong việc module hóa cũng như tạo cấu trúc REST API bạn có thể tự tạo cho mình một framework riêng hoặc sử dụng một framework đã có và đang được phát triển và ưa dùng là grape. tham khảo tại đâyGrape api là gì ?Grape is a REST-like API framework for Ruby. It's designed to run on Rack or complement existing web application frameworks such as Rails and Sinatra by providing a simple DSL to easily develop RESTful APIs. It has built-in support for common conventions, including multiple formats, subdomain/prefix restriction, content negotiation, versioning and much more. api frame work Restful Apis Rack, Sinatra support built-in supportNói gọn lại là Grape là một framework tích hợp và ăn nhập với các server như rack, sinsatra để tạo ra hệ thống Restful APIs. Ưu điểm là nó rất nhanh hơn các rails-api thông thường.Trong bài này tôi sẽ giới thiệu và demo một ứng dụng sử dụng grape api.Với mong muốn đi sâu hơn vào grape và ứng dụng của grape với rails nên tôi sẽ có thể viết nối tiếp một số bài nữa.Cài đặtTrong gemfilegem 'grape'Run bundle : bundle updateController examplemodule Twitter class API &lt; Grape::API version 'v1', using: :header, vendor: 'twitter' format :json prefix :api helpers do def current_user @current_user ||= User.authorize!(env) end def authenticate! error!('401 Unauthorized', 401) unless current_user end end resource :statuses do desc 'Return a public timeline.' get :public_timeline do Status.limit(20) end desc 'Return a personal timeline.' get :home_timeline do authenticate! current_user.statuses.limit(20) end desc 'Return a status.' params do requires :id, type: Integer, desc: 'Status ID.' end route_param :id do get do Status.find(params[:id]) end end desc 'Create a status.' params do requires :status, type: String, desc: 'Your status.' end post do authenticate! Status.create!({ user: current_user, text: params[:status] }) end desc 'Update a status.' params do requires :id, type: String, desc: 'Status ID.' requires :status, type: String, desc: 'Your status.' end put ':id' do authenticate! current_user.statuses.find(params[:id]).update({ user: current_user, text: params[:status] }) end desc 'Delete a status.' params do requires :id, type: String, desc: 'Status ID.' end delete ':id' do authenticate! current_user.statuses.find(params[:id]).destroy end end endendCấu trúc thư mục sơ cấp app |––controllers |––api mount một file base.rb module API class Base &lt; Grape::API mount API::V1::Base endend cấu trúc thư mục app |––controllers |––api |––base.rb Khái niệm cơ bảnmountmount: là cơ chế nói cho rails biết là grape đang tạo ra số lượng api tương ứng với fileTwitter::API.compile!Đối với rails để mount một file và tạo ra routes ta cần làm như sau: rails &lt; 5.2sửa lại file application.rb config.paths.add File.join('app', 'api'), glob: File.join('**', '*.rb')config.autoload_paths += Dir[Rails.root.join('app', 'api', '*')] rails thêm vào file config/routes.rb mount Twitter::API =&gt; '/' rails 6.0 sửa file config/initializers/inflections.rb ActiveSupport::Inflector.inflections(:en) do |inflect|inflect.acronym 'API'end Trong việc mount là một cơ chế kỳ ảo của grape đảm bảo việc nested và mở rộng module hóa cũng quản lý version. versionTừ version 1 bạn có thể phát triển lên version 2, 3. Grape dễ dàng quản lý điều đó dựa trên các câu lệnh cùng thư mụcapp |––controllers |––api |––base.rb |––v1 |––base.rbCác khái niệm cơ bản:There are four strategies in which clients can reach your API’s endpoints: :path, :header, :accept_version_header and :param. The default strategy is :path. pathversion 'v1', using: :path use with curl: curl http://localhost:9292/v1/statuses/public_timeline header version 'v1', using: :header, vendor: 'twitter' use with curl: curl -H Accept:application/vnd.twitter-v1+json http://localhost:9292/statuses/public_timeline accept_version_header version 'v1', using: :accept_version_header example: curl -H \"Accept-Version:v1\" http://localhost:9292/statuses/public_timeline param version 'v1', using: :param, parameter: 'v' ex: curl http://localhost:9292/statuses/public_timeline?v=v1 version, mount, nestedGiả sử bạn cần mở rộng module v1 bằng việc khải báo thêm các api.bước 1: Tạo folder v1app |––controllers |––api |––base.rb |––v1bước 2: với mỗi version tạo một base.rb riêng:app |––controllers |––api |––base.rb |––v1 |––base.rbbước 3: Trong file base khai báo mount thư mục đồng cấpmodule API module V1 class Base &lt; Grape::API mount V1::Users # mount API::V1::AnotherResource end endendbước 4: khai báo api tương ứng user.rbapp |––controllers |––api |––base.rb |––v1 |––base.rb |––users.rbapi end pointNhư ở trên khi bạn khai báo 1 api đồng cấp, đó cũng là api end point.Đối với một api end point:module API module V1 class Users &lt; Grape::API include API::V1::Defaults resource :users do desc \"Return all users\" get \"\", root: :users do User.all end desc \"Return a user\" params do requires :id, type: String, desc: \"ID of the user\" end get \":id\", root: \"user\" do User.where(id: params[:user_id]).first end end end endend resource: khai báo router desc: mô tả get \"\", root: :users do khai báo url api lấy toàn bộ userex: http://localhost:3000/api/v1/userstool: POSTMANAction, HelperMixin, format dữ liệu, các hàm dùng chungđược khai báo tại file defaults.rb, ex: app/controllers/api/v1/defaults.rb khai báo định dạng output như json các hàm như authencation. module API module V1 module Defaults extend ActiveSupport::Concern included do prefix \"api\" version \"v1\", using: :path default_format :json format :json formatter :json, Grape::Formatter::ActiveModelSerializers helpers do def permitted_params @permitted_params ||= declared(params, include_missing: false) end def logger Rails.logger end end # check authentice_user def authenticate_user! uid = request.headers[\"Uid\"] token = request.headers[\"Access-Token\"] @current_user = User.find_by(uid: uid) unless @current_user &amp;&amp; @current_user.valid_token?(token) api_error!(\"You need to log in to use the app.\", \"failure\", 401, {}) end end\t\t # Hàm hiển thị errors message khi lỗi def api_error!(message, error_code, status, header) error!({message: message, code: error_code}, status, header) end # # Hàm raise errors message khi lỗi def api_error_log(message) @logger ||= Logger.new(ProjectLogger.log_path(\"project_api\")) @logger.info(\"=============#{Time.zone.now.to_s}==================\\n\") @logger.info(\"#{message}\\n\") end rescue_from ActiveRecord::RecordNotFound do |e| error_response(message: e.message, status: 404) end rescue_from ActiveRecord::RecordInvalid do |e| error_response(message: e.message, status: 422) end rescue_from Grape::Exceptions::ValidationErrors do |e| error_response(message: e.message, status: 400) end end endendend Action callbackGrape vấn support đầy đủ các hàm call back như:\t1. before\t2. before_validation\t3. validations\t4. after_validation\t5. the API call\t6. after\t7. finallyExample:class MyAPI &lt; Grape::API get '/' do \"root - #{@blah}\" end namespace :foo do before do @blah = 'blah' end get '/' do \"root - foo - #{@blah}\" end namespace :bar do get '/' do \"root - foo - bar - #{@blah}\" end end endendkết quảGET / # 'root - 'GET /foo # 'root - foo - blah'GET /foo/bar # 'root - foo - bar - blah'example versionclass Test &lt; Grape::API resource :foo do version 'v1', :using =&gt; :path do before do @output ||= 'v1-' end get '/' do @output += 'hello' end end version 'v2', :using =&gt; :path do before do @output ||= 'v2-' end get '/' do @output += 'hello' end end endendkết quảGET /foo/v1 # 'v1-hello'GET /foo/v2 # 'v2-hello'RspecViết rồi kiểm thử làm sao ?-&gt; viết rspec cho api ta có thể dùng gem airborne.install:gem install airbornecấu trúc thư mụcspec |––api |––v1 |––users_specnội dung file test:require \"rails_helper\"require \"airborne\"describe \"API::V1::Users\" do after(:all){I18n.locale = :ja} describe \"POST api/v1/users\" do let!(:user) do FactoryGirl.create :user, id: 1, email: \"test@gmail.com\", first_name: \"James\", last_name: \"Bond\", provider: \"email\" end context \"when user update successfully\" do let(:api_response){FactoryGirl.build(:api_update_user_success_response).deep_symbolize_keys} before do post(\"/api/v1/users\", {first_name: \"hitorri\"}, {\"Accept-Language\": \"en\", \"App-Version\": \"knt/1.0\", \"Uid\": user.uid, \"Access-Token\": user.access_token}) end it{expect_json(api_response)} end end endCORSusing gem rack-cors, cấu hình tại file config.rurequire 'rack/cors'use Rack::Cors do allow do origins '*' resource '*', headers: :any, methods: :get endendrun Twitter::APISerializingCông việc của chúng ta là convert mảng sang JSON. Để làm được việc này chúng ta cài thêm gem grape-active_model_serializersmodule API module V1 module Defaults extend ActiveSupport::Concern included do prefix \"api\" version \"v1\", using: :path default_format :json format :json formatter :json, Grape::Formatter::ActiveModelSerializers ...Viết file serializersCreate a directory, serializers, in the top level of your app. Create a graduate_serializer.rb file in that directory. Here is where our serializer will live.app/serializers/graduate_serializer.rbclass GraduateSerializer &lt; ActiveModel::Serializer attributes :id, :first_name, :last_name, :cohort, :current_job, :bio, :news, :website, :picture, :created_at, :updated_atendGrape Gem additionGrape Swagger Add gem ‘grape-swagger’ to your Gemfile and bundle install. Add grape-swagger documentation to the root or base class of your API.tại file app/controllers/api/v1/base.rb thêm đoạn sau: require \"grape-swagger\" ... add_swagger_documentation( api_version: \"v1\", hide_documentation_path: true, mount_path: \"/api/v1/swagger_doc\", hide_format: true ) end Add the documentation endpoint to your routes.tại file config/routes.rb: mount GrapeSwaggerRails::Engine, at: \"/documentation\" vào url: http://localhost/documentation bạn sẽ redirect tới: http://localhost:3000/api/v1/swagger_doc Grape EntityCó thể tham khảo thêm tại đâyGrape LoggerCó thể tham khảo tại đâyGrape Swagger representableCó thể tham khảo thêm tại đâyTài liệu tham khảo https://viblo.asia/p/xay-dung-api-voi-grape-bWrZne1vKxw https://www.thegreatcodeadventure.com/making-a-rails-api-with-grap/ https://github.com/ruby-grape/grape#what-is-grape" }, { "title": "Tạo Blog đơn giản với Github và Jekyll", "url": "/posts/tao-blog-don-gian-voi-jekyll/", "categories": "Blogging, Tutorial, Github", "tags": "github, jekyll", "date": "2021-03-20 00:00:00 +0700", "snippet": "Xây dựng Blog cá nhân với Github và JekyllĐầu tiên nếu đối với mỗi ai trong chúng ta có niềm yêu thích muốn xây dựng cho mình một blog, blog đó là nơi để chia sẻ những kinh nghiệm đời sống và công ...", "content": "Xây dựng Blog cá nhân với Github và JekyllĐầu tiên nếu đối với mỗi ai trong chúng ta có niềm yêu thích muốn xây dựng cho mình một blog, blog đó là nơi để chia sẻ những kinh nghiệm đời sống và công việc cũng có thể là một kênh thông tin để truyền tải thông điệp và lưu giữ nhưng bài học của bạn trong hành trình cuộc sống không đơn giản này.Tôi 30 tuổi và đang là một lập trình viên, tôi chưa gia đình, trong cuộc sống bộn bề và những suy nghĩ còn chưa đến đâu khi nhìn về tương lai và quá khứ đã qua một thứ luôn thúc đẩy tôi đó là tạo 1 blog để chia sẻ và viết lại nhưng dòng code, những dòng tâm sự dài dòng mà phải cô đọng.Chuẩn bị về kiến thức blog đã nàoĐiều bạn cần chuẩn bị cho một blog là một chiếc bàn phím với tối thiểu 44 phím cần thiết cho việc viết lách, một cái đầu vui vẻ sau những phút thảnh thơi giữa cuộc sống bộn bề của bạn.Về thứ hai, do bây giờ để không dài dòng tôi có thể giới thiệu bạn tới một số trang blog miễn phí trên mạng như Wordpress một trang mã nguồn mở cung cấp rất nhiều công cụ quản lý và giao diện đồ sộ, điều tôi không thích là nó quá rườm rà và khá chậm. Tumblr một trang mạng xã hội nhưng vẫn cho phép bạn tạo ra nhưng bài blog có chủ đề và đi kèm với nó là mạng lưới chia sẻ của Tumblr, chắc bạn sẽ phải ngặc nhiên khi có không ít nhưng cây viết Việt trên nền tảng mạng xã hội Tumblr. Bloger Một trang bloger được cung cấp với công cụ quản lý cùng với kho theme rộng rãi, một domain tuy không nhanh nhưng băng thông không giới hạn đủ để bạn có thể thoải mái sáng tạo. Facebook là một trang mạng xã hội, chúng ta có thể chia sẻ rất nhiều chủ đề bài viết, các bạn đừng nghĩ tôi đang nói đến một trang cá nhân vì chuyện đó quá đơn giản mà ở đây đẻ tạo một blog bạn cần tạo một tài khoản facebook sau đó tạo pages facebook, hiện tại có rất nhiều trang facebook theo từng chủ đề như chính trị, giải trí, truyền thông và có thể blog đơn giản của những tay viết nghiệp dư cho đến chuyên nghiệp. WIX Một trang web đang có lượng user đăng ký khá lớn cung cấp domain miễn phí. Quora Một mạng chia sẻ xã hội và những câu hỏi cùng giải đáp rất ý nghĩa Medium Medium là một sân chơi lớn hơn và cũng là một sân chơi đáng giá nơi bạn có thể chia sẻ các bài viết và ý tướng của mình, ở đây bạn có thể nhận được một khoản nhuận bút nếu bài viết của bạn hay và được rating cao. Ghost là một trang tạo blog content chuyên nghiệp và hấp dẫn từ kho giao diện cũng như là cung cấp nhưng công cụ quản lý điều duy nhất tôi không thích là bạn sẽ phải trả phí cho muộn public domain lên trên trang web Đây là cách truyền thống, bạn tạo ra website rồi update lên một hosting bất kỳ, có rất nhiều framework hỗ trợ bạn như wordpress, drupal, cakePHP, Yii, Laravel. Hosting bạn có thể tìm đến rất nhiều nhà cung cấp domain tại việt nam cũng như ở singapore với giá rẻ, bạn có thể phải trả phí cho 1 - 2 năm, giá khoảng 2.7 ~ 3 triệu vnđ. Github Github là công cụ quản lý source code mã nguồn mở đang được ưa dùng nhiều nhất hiện nay, chắc bạn cũng từng vài lần qua download nhưng mã nguồn mở hoặc upload các project của bản thân qua đây cũng là lời giới thiệu với mọi người biết bạn là một lập trình viên như nào về tư tưởng cũng như sự đam mê với lập trình. Tôi cũng trong số các bạn thích github và vẫn thường dạo quanh các repository để tìm hiểu có thể chỉ là một plugin js, css hoặc một dự án về docker cho đến các trang web lớn với các mục địch khác nhau đều được phổ biến tại trang web quản lý này. Đối với mỗi người dùng github bạn chỉ có thể tạo ra một website duy nhất để chia sẻ các dự án và kinh nghiệm của mình, sẽ hợp lý hơn là bạn viết nhưng bài giới thiệu và chủ đề bạn quan tâm đối với mỗi repositories hoặc giả sử bạn là một lập trình viên yêu thích lập trình cần có một trang web hiện đại nhanh chóng được tự cấu hình và lập trình riêng cho nó thi bạn nên chọn lựa github pages. Ở bài viết này tôi sẽ giới thiệu bạn tạo github pages với jekyll một trang sinh mã nguồn statics.Đăng ký Github pagesBước 1: Tạo repository dưới dạng usernameweb.github.io repository 1 tài khoản chỉ tạo được 1 blog.Bước 2: Trong tab setting chọn branchBước 3: Chọn theme nếu bạn dùng kho theme miễn phí của githubBước 4: Clone source code về và init projectgit clone git-ssh-linkcd git-ssh-linkTrong thư mục này bạn có thể tạo ra một trang giao diện tùy ý, github pages support js và javascript nhưng không support php hoặc các ngôn ngữ script khác như nodejs, rails, python.Bước 5: Khởi tạo jekyll như trang blog quản lýTừ bước này bạn sẽ có thể hình dung về trang blog của mình sẽ như thế nào rồi đấy.có thể vào bài này để làm tiếp bước này nhé.Bước 6: Trang quản lý jekyll Tham khảo github của nhà phát triển. Add the following to your site’s Gemfile: ruby gem 'jekyll-admin', group: :jekyll_plugins Run bundle instalBước 7: Cài đặt theme cho jekyll(tùy bạn nhé!)các bạn vào Đây để tham khảo kho theme của jekyll.Bước tiếp theo ? Làm sao để viết blog !Để viết một blog hay và có nội dung không phải khó nhưng cũng là một chuyện liên quan đến lối văn chương và kiến thức của từng người. Tôi không biết phải viết thế nào :) , những bài viết của tôi vẫn sẽ cung cấp cho các bạn những điều mà tôi muốn ghi lại tại đây và chia sẻ. Geting started Writing a new post Text and typography Tutorial" }, { "title": "Enable Google Page Views", "url": "/posts/enable-google-pv/", "categories": "Blogging, Tutorial", "tags": "google analytics, pageviews", "date": "2021-01-04 06:32:00 +0700", "snippet": "This post is to enable Page Views on the Chirpy theme based blog that you just built. This requires technical knowledge and it’s recommended to keep the google_analytics.pv.* empty unless you have ...", "content": "This post is to enable Page Views on the Chirpy theme based blog that you just built. This requires technical knowledge and it’s recommended to keep the google_analytics.pv.* empty unless you have a good reason. If your website has low traffic, the page views count would discourage you to write more blogs. With that said, let’s start with the setup.Set up Google AnalyticsCreate GA account and propertyFirst, you need to set up your account on Google analytics. While you create your account, you must create your first Property as well. Head to https://analytics.google.com/ and click on Start Measuring Enter your desired Account Name and choose the desired checkboxes Enter your desired Property Name. This is the name of the tracker project that appears on your Google Analytics dashboard Enter the required information About your business Hit Create and accept any license popup to set up your Google Analytics account and create your propertyCreate Data StreamWith your property created, you now need to set up Data Stream to track your blog traffic. After you signup, the prompt should automatically take you to create your first Data Stream. If not, follow these steps: Go to Admin on the left column Select the desired property from the drop-down on the second column Click on Data Streams Add a stream and click on Web Enter your blog’s URLIt should look like this:Now, click on the new data stream and grab the Measurement ID. It should look something like G-V6XXXXXXXX. Copy this to your _config.yml file:google_analytics: id: 'G-V6XXXXXXX' # fill in your Google Analytics ID # Google Analytics pageviews report settings pv: proxy_endpoint: # fill in the Google Analytics superProxy endpoint of Google App Engine cache_path: # the local PV cache data, friendly to visitors from GFW regionWhen you push these changes to your blog, you should start seeing the traffic on your Google Analytics. Play around with the Google Analytics dashboard to get familiar with the options available as it takes like 5 mins to pick up your changes. You should now be able to monitor your traffic in real time.Setup Page ViewsThere is a detailed tutorial available to set up Google Analytics superProxy. But, if you are interested to just quickly get your Chirpy-based blog display page views, follow along. These steps were tested on a Linux machine. If you are running Windows, you can use the Git bash terminal to run Unix-like commands.Setup Google App Engine Visit https://console.cloud.google.com/appengine Click on Create Application Click on Create Project Enter the name and choose the data center close to you Select Python language and Standard environment Enable billing account. Yeah, you have to link your credit card. But, you won’t be billed unless you exceed your free quota. For a simple blog, the free quota is more than sufficient. Go to your App Engine dashboard on your browser and select API &amp; Services from the left navigation menu Click on Enable APIs and Services button on the top Enable the following APIs: Google Analytics API On the left, Click on OAuth Consent Screen and accept Configure Consent Screen. Select External since your blog is probably hosted for the public. Click on Publish under Publishing Status Click on Credentials on the left and create a new OAuth Client IDs credential. Make sure to add an entry under Authorized redirect URIs that matches: https://&lt;project-id&gt;.&lt;region&gt;.r.appspot.com/admin/auth Note down the Your Client ID and Your Client Secret. You’ll need this in the next section. Download and install the cloud SDK for your platform: https://cloud.google.com/sdk/docs/quickstart Run the following commands: [root@bc96abf71ef8 /]# gcloud init~snip~Go to the following link in your browser: https://accounts.google.com/o/oauth2/auth?response_type=code&amp;client_id=XYZ.apps.googleusercontent.com&amp;redirect_uri=ABCDEFGEnter verification code: &lt;VERIFICATION CODE THAT YOU GET AFTER YOU VISIT AND AUTHENTICATE FROM THE ABOVE LINK&gt;You are logged in as: [blah_blah@gmail.com].Pick cloud project to use:[1] chirpy-test-300716[2] Create a new projectPlease enter numeric choice or text value (must exactly match listitem): 1[root@bc96abf71ef8 /]# gcloud info# Your selected project info should be displayed here Setup Google Analytics superProxy Clone the Google Analytics superProxy project on Github: https://github.com/googleanalytics/google-analytics-super-proxy to your local. Remove the first 2 lines in the src/app.yaml file: - application: your-project-id- version: 1 In src/config.py, add the OAUTH_CLIENT_ID and OAUTH_CLIENT_SECRET that you gathered from your App Engine Dashboard. Enter any random key for XSRF_KEY, your config.py should look similar to this #!/usr/bin/python2.7__author__ = 'pete.frisella@gmail.com (Pete Frisella)'# OAuth 2.0 Client SettingsAUTH_CONFIG = { 'OAUTH_CLIENT_ID': 'YOUR_CLIENT_ID', 'OAUTH_CLIENT_SECRET': 'YOUR_CLIENT_SECRET', 'OAUTH_REDIRECT_URI': '%s%s' % ( 'https://chirpy-test-XXXXXX.ue.r.appspot.com', '/admin/auth' )}# XSRF SettingsXSRF_KEY = 'OnceUponATimeThereLivedALegend' You can configure a custom domain instead of https://PROJECT_ID.REGION_ID.r.appspot.com.But, for the sake of keeping it simple, we will be using the Google provided default URL. From inside the src/ directory, deploy the app [root@bc96abf71ef8 src]# gcloud app deployServices to deploy:descriptor: [/tmp/google-analytics-super-proxy/src/app.yaml]source: [/tmp/google-analytics-super-proxy/src]target project: [chirpy-test-XXXX]target service: [default]target version: [VESRION_NUM]target url: [https://chirpy-test-XXXX.ue.r.appspot.com]Do you want to continue (Y/n)? YBeginning deployment of service [default]...╔════════════════════════════════════════════════════════════╗╠═ Uploading 1 file to Google Cloud Storage ═╣╚════════════════════════════════════════════════════════════╝File upload done.Updating service [default]...done.Setting traffic split for service [default]...done.Deployed service [default] to [https://chirpy-test-XXXX.ue.r.appspot.com]You can stream logs from the command line by running:$ gcloud app logs tail -s defaultTo view your application in the web browser run:$ gcloud app browse Visit the deployed service. Add a /admin to the end of the URL. Click on Authorize Users and make sure to add yourself as a managed user. If you get any errors, please Google it. The errors are self-explanatory and should be easy to fix. If everything went good, you’ll get this screen:Create Google Analytics QueryHead to https://PROJECT_ID.REGION_ID.r.appspot.com/admin and create a query after verifying the account. GA Core Reporting API query request can be created in Query Explorer.The query parameters are as follows: start-date: fill in the first day of blog posting end-date: fill in today (this is a parameter supported by GA Report, which means that it will always end according to the current query date) metrics: select ga:pageviews dimensions: select ga:pagePathIn order to reduce the returned results and reduce the network bandwidth, we add custom filtering rules 1: filters: fill in ga:pagePath=~^/posts/.*/$;ga:pagePath!@=. Among them, ; means using logical AND to concatenate two rules. If the site.baseurl is specified, change the first filtering rule to ga:pagePath=~^/BASE_URL/posts/.*/$, where BASE_URL is the value of site.baseurl. After Run Query, copy the generated contents of API Query URI at the bottom of the page and fill in the Encoded URI for the query of SuperProxy on GAE.After the query is saved on GAE, a Public Endpoint (public access address) will be generated, and we will get the query result in JSON format when accessing it. Finally, click Enable Endpoint in Public Request Endpoint to make the query effective, and click Start Scheduling in Scheduling to start the scheduled task.Configure Chirpy to Display Page ViewOnce all the hard part is done, it is very easy to enable the Page View on Chirpy theme. Your superProxy dashboard should look something like below and you can grab the required values.Update the _config.yml file of Chirpy project with the values from your dashboard, to look similar to the following:google_analytics: id: 'G-V6XXXXXXX' # fill in your Google Analytics ID pv: proxy_endpoint: 'https://PROJECT_ID.REGION_ID.r.appspot.com/query?id=&lt;ID FROM SUPER PROXY&gt;' cache_path: # the local PV cache data, friendly to visitors from GFW regionNow, you should see the Page View enabled on your blog.Reference Google Analytics Core Reporting API: Filters &#8617; " }, { "title": "Customize the Favicon", "url": "/posts/customize-the-favicon/", "categories": "Blogging, Tutorial", "tags": "favicon", "date": "2019-08-10 23:34:00 +0700", "snippet": "The favicons of Chirpy are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons...", "content": "The favicons of Chirpy are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons.Generate the faviconPrepare a square image (PNG, JPG, or SVG) with a size of 512x512 or more, and then go to the online tool Real Favicon Generator and click the button Select your Favicon image to upload your image file.In the next step, the webpage will show all usage scenarios. You can keep the default options, scroll to the bottom of the page, and click the button Generate your Favicons and HTML code to generate the favicon.Download &amp; ReplaceDownload the generated package, unzip and delete the following two from the extracted files: browserconfig.xml site.webmanifestAnd then copy the remaining image files (.PNG and .ICO) to cover the original files in the directory assets/img/favicons/ of your Jekyll site. If your Jekyll site doesn’t have this directory yet, just create one.The following table will help you understand the changes to the favicon files: File(s) From Online Tool From Chirpy *.PNG ✓ ✗ *.ICO ✓ ✗ ✓ means keep, ✗ means delete.The next time you build the site, the favicon will be replaced with a customized edition." }, { "title": "Getting Started", "url": "/posts/getting-started/", "categories": "Blogging, Tutorial", "tags": "getting started", "date": "2019-08-09 19:55:00 +0700", "snippet": "PrerequisitesFollow the instructions in the Jekyll Docs to complete the installation of the basic environment. Git also needs to be installed.InstallationCreating a New SiteThere are two ways to cr...", "content": "PrerequisitesFollow the instructions in the Jekyll Docs to complete the installation of the basic environment. Git also needs to be installed.InstallationCreating a New SiteThere are two ways to create a new repository for this theme: Using the Chirpy Starter - Easy to upgrade, isolates irrelevant project files so you can focus on writing. GitHub Fork - Convenient for custom development, but difficult to upgrade. Unless you are familiar with Jekyll and are determined to tweak or contribute to this project, this approach is not recommended.Option 1. Using the Chirpy StarterSign in to GitHub and browse to Chirpy Starter, click the button Use this template &gt; Create a new repository, and name the new repository USERNAME.github.io, where USERNAME represents your GitHub username.Option 2. GitHub ForkSign in to GitHub to fork Chirpy, and then rename it to USERNAME.github.io (USERNAME means your username).Next, clone your site to local machine. In order to build JavaScript files later, we need to install Node.js, and then run the tool:$ bash tools/init If you don’t want to deploy your site on GitHub Pages, append option --no-gh at the end of the above command.The above command will: Check out the code to the latest tag (to ensure the stability of your site: as the code for the default branch is under development). Remove non-essential sample files and take care of GitHub-related files. Build JavaScript files and export to assets/js/dist/, then make them tracked by Git. Automatically create a new commit to save the changes above.Installing DependenciesBefore running local server for the first time, go to the root directory of your site and run:$ bundleUsageConfigurationUpdate the variables of _config.yml as needed. Some of them are typical options: url avatar timezone langSocial Contact OptionsSocial contact options are displayed at the bottom of the sidebar. You can turn on/off the specified contacts in file _data/contact.yml.Customizing StylesheetIf you need to customize the stylesheet, copy the theme’s assets/css/jekyll-theme-chirpy.scss to the same path on your Jekyll site, and then add the custom style at the end of it.Starting with version 6.2.0, if you want to overwrite the SASS variables defined in _sass/addon/variables.scss, copy the main sass file _sass/main.scss into the _sass directory in your site’s source, then create a new file _sass/variables-hook.scss and assign new value.Customing Static AssetsStatic assets configuration was introduced in version 5.1.0. The CDN of the static assets is defined by file _data/origin/cors.yml, and you can replace some of them according to the network conditions in the region where your website is published.Also, if you’d like to self-host the static assets, please refer to the chirpy-static-assets.Running Local ServerYou may want to preview the site contents before publishing, so just run it by:$ bundle exec jekyll sAfter a few seconds, the local service will be published at 127.0.0.1:4000.DeploymentBefore the deployment begins, check out the file _config.yml and make sure the url is configured correctly. Furthermore, if you prefer the project site and don’t use a custom domain, or you want to visit your website with a base URL on a web server other than GitHub Pages, remember to change the baseurl to your project name that starts with a slash, e.g, /project-name.Now you can choose ONE of the following methods to deploy your Jekyll site.Deploy by Using GitHub ActionsThere are a few things to get ready for. If you’re on the GitHub Free plan, keep your site repository public. If you have committed Gemfile.lock to the repository, and your local machine is not running Linux, go to the root of your site and update the platform list of the lock-file: $ bundle lock --add-platform x86_64-linux Next, configure the Pages service. Browse to your repository on GitHub. Select the tab Settings, then click Pages in the left navigation bar. Then, in the Source section (under Build and deployment), select GitHub Actions from the dropdown menu. Push any commits to GitHub to trigger the Actions workflow. In the Actions tab of your repository, you should see the workflow Build and Deploy running. Once the build is complete and successful, the site will be deployed automatically. At this point, you can go to the URL indicated by GitHub to access your site.Manually Build and DeployOn self-hosted servers, you cannot enjoy the convenience of GitHub Actions. Therefore, you should build the site on your local machine and then upload the site files to the server.Go to the root of the source project, and build your site as follows:$ JEKYLL_ENV=production bundle exec jekyll bUnless you specified the output path, the generated site files will be placed in folder _site of the project’s root directory. Now you should upload those files to the target server. jekyll-theme-chirpy/master " }, { "title": "Writing a New Post", "url": "/posts/write-a-new-post/", "categories": "Blogging, Tutorial", "tags": "writing", "date": "2019-08-08 13:10:00 +0700", "snippet": "This tutorial will guide you how to write a post in the Chirpy template, and it’s worth reading even if you’ve used Jekyll before, as many features require specific variables to be set.Naming and P...", "content": "This tutorial will guide you how to write a post in the Chirpy template, and it’s worth reading even if you’ve used Jekyll before, as many features require specific variables to be set.Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _posts of the root directory. Please note that the EXTENSION must be one of md and markdown. If you want to save time of creating files, please consider using the plugin Jekyll-Compose to accomplish this.Front MatterBasically, you need to fill the Front Matter as below at the top of the post:---title: TITLEdate: YYYY-MM-DD HH:MM:SS +/-TTTTcategories: [TOP_CATEGORIE, SUB_CATEGORIE]tags: [TAG] # TAG names should always be lowercase--- The posts’ layout has been set to post by default, so there is no need to add the variable layout in the Front Matter block.Timezone of DateIn order to accurately record the release date of a post, you should not only set up the timezone of _config.yml but also provide the post’s timezone in variable date of its Front Matter block. Format: +/-TTTT, e.g. +0800.Categories and TagsThe categories of each post are designed to contain up to two elements, and the number of elements in tags can be zero to infinity. For instance:---categories: [Animal, Insect]tags: [bee]---Author InformationThe author information of the post usually does not need to be filled in the Front Matter , they will be obtained from variables social.name and the first entry of social.links of the configuration file by default. But you can also override it as follows:Adding author information in _data/authors.yml (If your website doesn’t have this file, don’t hesitate to create one).&lt;author_id&gt;: name: &lt;full name&gt; twitter: &lt;twitter_of_author&gt; url: &lt;homepage_of_author&gt;And then use author to specify a single entry or authors to specify multiple entries:---author: &lt;author_id&gt; # for single entry# orauthors: [&lt;author1_id&gt;, &lt;author2_id&gt;] # for multiple entries---Having said that, the key author can also identify multiple entries. The benefit of reading the author information from the file _data/authors.yml is that the page will have the meta tag twitter:creator, which enriches the Twitter Cards and is good for SEO.Table of ContentsBy default, the Table of Contents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to _config.yml and set the value of variable toc to false. If you want to turn off TOC for a specific post, add the following to the post’s Front Matter:---toc: false---CommentsThe global switch of comments is defined by variable comments.active in the file _config.yml. After selecting a comment system for this variable, comments will be turned on for all posts.If you want to close the comment for a specific post, add the following to the Front Matter of the post:---comments: false---MathematicsFor website performance reasons, the mathematical feature won’t be loaded by default. But it can be enabled by:---math: true---After enabling the mathematical feature, you can add math equations with the following syntax: Block math should be added with $$ math $$ with mandatory blank lines before and after $$ Inserting equation numbering should be added with $$\\begin{equation} math \\end{equation}$$ Referencing equation numbering should be done with \\label{eq:label_name} in the equation block and \\eqref{eq:label_name} inline with text (see example below) Inline math (in lines) should be added with $$ math $$ without any blank line before or after $$ Inline math (in lists) should be added with \\$$ math $$&lt;!-- Block math, keep all blank lines --&gt;$$LaTeX_math_expression$$&lt;!-- Equation numbering, keep all blank lines --&gt;$$\\begin{equation} LaTeX_math_expression \\label{eq:label_name}\\end{equation}$$Can be referenced as \\eqref{eq:label_name}.&lt;!-- Inline math in lines, NO blank lines --&gt;\"Lorem ipsum dolor sit amet, $$ LaTeX_math_expression $$ consectetur adipiscing elit.\"&lt;!-- Inline math in lists, escape the first `$` --&gt;1. \\$$ LaTeX_math_expression $$2. \\$$ LaTeX_math_expression $$3. \\$$ LaTeX_math_expression $$MermaidMermaid is a great diagram generation tool. To enable it on your post, add the following to the YAML block:---mermaid: true---Then you can use it like other markdown languages: surround the graph code with ```mermaid and ```.ImagesCaptionAdd italics to the next line of an image, then it will become the caption and appear at the bottom of the image:![img-description](/path/to/image)_Image Caption_SizeIn order to prevent the page content layout from shifting when the image is loaded, we should set the width and height for each image.![Desktop View](/assets/img/sample/mockup.png){: width=\"700\" height=\"400\" } For an SVG, you have to at least specify its width, otherwise it won’t be rendered.Starting from Chirpy v5.0.0, height and width support abbreviations (height → h, width → w). The following example has the same effect as the above:![Desktop View](/assets/img/sample/mockup.png){: w=\"700\" h=\"400\" }PositionBy default, the image is centered, but you can specify the position by using one of the classes normal, left, and right. Once the position is specified, the image caption should not be added. Normal position Image will be left aligned in below sample: ![Desktop View](/assets/img/sample/mockup.png){: .normal } Float to the left ![Desktop View](/assets/img/sample/mockup.png){: .left } Float to the right ![Desktop View](/assets/img/sample/mockup.png){: .right } Dark/Light modeYou can make images follow theme preferences in dark/light mode. This requires you to prepare two images, one for dark mode and one for light mode, and then assign them a specific class (dark or light):![Light mode only](/path/to/light-mode.png){: .light }![Dark mode only](/path/to/dark-mode.png){: .dark }ShadowThe screenshots of the program window can be considered to show the shadow effect:![Desktop View](/assets/img/sample/mockup.png){: .shadow }CDN URLIf you host the images on the CDN, you can save the time of repeatedly writing the CDN URL by assigning the variable img_cdn of _config.yml file:img_cdn: https://cdn.comOnce img_cdn is assigned, the CDN URL will be added to the path of all images (images of site avatar and posts) starting with /.For instance, when using images:![The flower](/path/to/flower.png)The parsing result will automatically add the CDN prefix https://cdn.com before the image path:&lt;img src=\"https://cdn.com/path/to/flower.png\" alt=\"The flower\" /&gt;Image PathWhen a post contains many images, it will be a time-consuming task to repeatedly define the path of the images. To solve this, we can define this path in the YAML block of the post:---img_path: /img/path/---And then, the image source of Markdown can write the file name directly:![The flower](flower.png)The output will be:&lt;img src=\"/img/path/flower.png\" alt=\"The flower\" /&gt;Preview ImageIf you want to add an image at the top of the post, please provide an image with a resolution of 1200 x 630. Please note that if the image aspect ratio does not meet 1.91 : 1, the image will be scaled and cropped.Knowing these prerequisites, you can start setting the image’s attribute:---image: path: /path/to/image alt: image alternative text---Note that the img_path can also be passed to the preview image, that is, when it has been set, the attribute path only needs the image file name.For simple use, you can also just use image to define the path.---image: /path/to/image---LQIPFor preview images:---image: lqip: /path/to/lqip-file # or base64 URI--- You can observe LQIP in the preview image of post Text and Typography.For normal images:![Image description](/path/to/image){: lqip=\"/path/to/lqip-file\" }Pinned PostsYou can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:---pin: true---PromptsThere are several types of prompts: tip, info, warning, and danger. They can be generated by adding the class prompt-{type} to the blockquote. For example, define a prompt of type info as follows:&gt; Example line for prompt.{: .prompt-info }SyntaxInline Code`inline code part`Filepath Hightlight`/path/to/a/file.extend`{: .filepath}Code BlockMarkdown symbols ``` can easily create a code block as follows:```This is a plaintext code snippet.```Specifying LanguageUsing ```{language} you will get a code block with syntax highlight:```yamlkey: value``` The Jekyll tag {% highlight %} is not compatible with this theme.Line NumberBy default, all languages except plaintext, console, and terminal will display line numbers. When you want to hide the line number of a code block, add the class nolineno to it:```shellecho 'No more line numbers!'```{: .nolineno }Specifying the FilenameYou may have noticed that the code language will be displayed at the top of the code block. If you want to replace it with the file name, you can add the attribute file to achieve this:```shell# content```{: file=\"path/to/file\" }Liquid CodesIf you want to display the Liquid snippet, surround the liquid code with {% raw %} and {% endraw %}:{% raw %}```liquid{% if product.title contains 'Pack' %} This product's title contains the word Pack.{% endif %}```{% endraw %}Or adding render_with_liquid: false (Requires Jekyll 4.0 or higher) to the post’s YAML block.VideosYou can embed a video with the following syntax:{% include embed/{Platform}.html id='{ID}' %}Where Platform is the lowercase of the platform name, and ID is the video ID.The following table shows how to get the two parameters we need in a given video URL, and you can also know the currently supported video platforms. Video URL Platform ID https://www.youtube.com/watch?v=H-B46URT4mg youtube H-B46URT4mg https://www.twitch.tv/videos/1634779211 twitch 1634779211 https://www.bilibili.com/video/BV1Q44y1B7Wf bilibili BV1Q44y1B7Wf Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Text and Typography", "url": "/posts/text-and-typography/", "categories": "Blogging, Demo", "tags": "typography", "date": "2019-08-08 10:33:00 +0700", "snippet": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.HeadingsH1 - headingH2 - headingH3 - headingH...", "content": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.HeadingsH1 - headingH2 - headingH3 - headingH4 - headingParagraphQuisque egestas convallis ipsum, ut sollicitudin risus tincidunt a. Maecenas interdum malesuada egestas. Duis consectetur porta risus, sit amet vulputate urna facilisis ac. Phasellus semper dui non purus ultrices sodales. Aliquam ante lorem, ornare a feugiat ac, finibus nec mauris. Vivamus ut tristique nisi. Sed vel Hardwork vulputate, efficitur risus non, posuere mi. Nullam tincidunt bibendum rutrum. Proin commodo ornare sapien. Vivamus interdum diam sed sapien blandit, sit amet aliquam risus mattis. Nullam arcu turpis, mollis quis laoreet at, placerat id nibh. Suspendisse venenatis eros eros.ListsOrdered list Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph ToDo list Job Step 1 Step 2 Step 3 Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line shows the block quote.Prompts An example showing the tip type prompt. An example showing the info type prompt. An example showing the warning type prompt. An example showing the danger type prompt.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Links127.0.0.1:4000FootnoteClick the hook will locate the footnote1, and here is another footnote2.Inline codeThis is an example of Inline Code.FilepathHere is the /path/to/the/file.extend.Code blocksCommonThis is a common code snippet, without syntax highlight and line number.Specific Languageif [ $? -ne 0 ]; then echo \"The command was not successful.\"; #do the needful / exitfi;Specific filename@import \"colors/light-typography\", \"colors/dark-typography\";MathematicsThe mathematics powered by MathJax:\\[\\begin{equation} \\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6} \\label{eq:series}\\end{equation}\\]We can reference the equation as \\eqref{eq:series}.When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dImagesDefault (with caption)Full screen width and center alignmentLeft alignedFloat to leftPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis Hardwork magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Float to rightPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis Hardwork magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Dark/Light mode &amp; ShadowThe image below will toggle dark/light mode based on theme preference, notice it has shadows.VideoReverse Footnote The footnote source &#8617; The 2nd footnote source &#8617; " } ]
